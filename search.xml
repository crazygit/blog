<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Elasticsearch快速入门]]></title>
    <url>%2F2018%2F04%2F11%2Felasticsearch-getting-started%2F</url>
    <content type="text"><![CDATA[Elasticsearch快速入门 当前使用版本: 6.2 基本概念 Near Realtime (NRT) 及时搜索，创建文档的索引后无需等待，立刻就可以开始搜索 Cluster 集群，一个集群可以包含多个节点。需要注意的是集群名字很重要，在同一个网络环境中，节点会自动加入相同集群名的集群。 Node 节点，每个节点都有自己的名字，它作为集群的一个单元，拥有数据存储和提供搜索能力等功能。 Index 索引(Index)是一系列文档(Document)的集合, 索引名全部必须小写，一个集群里可以有任意多个索引。 Type 从Elasticsearch 6.0.0 开始已经废弃不再使用 Document 文档，存储的最小单元 Shards &amp; Replicas 安装 这里主要介绍使用docker的安装的方式，主要有三种格式的镜像 123456# 基本版本(默认版本)，包含X-pack的基本特性和免费证书$ docker pull docker.elastic.co/elasticsearch/elasticsearch:6.2.3# platinum版本，包含X-pack的全部特性和30天试用证书$ docker pull docker.elastic.co/elasticsearch/elasticsearch-platinum:6.2.3# oss版本，不包含X-pack，只有开源的Elasticsearch$ docker pull docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.3 这里主要使用oss版本 1234567891011121314151617181920212223# 安装$ docker pull docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.3# 启动服务$ docker run -p 9200:9200 -p 9300:9300 -e "cluster.name=mycluster" -e "node.name=my_node" docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.3# 检查启动的服务$ curl localhost:9200&#123; "name" : "my_node", "cluster_name" : "mycluster", "cluster_uuid" : "e4xwigyJQp-Er6ZJeIg1wg", "version" : &#123; "number" : "6.2.3", "build_hash" : "c59ff00", "build_date" : "2018-03-13T10:06:29.741383Z", "build_snapshot" : false, "lucene_version" : "7.2.1", "minimum_wire_compatibility_version" : "5.6.0", "minimum_index_compatibility_version" : "5.0.0" &#125;, "tagline" : "You Know, for Search"&#125; 探索集群 集群健康状态检查 123$ curl "localhost:9200/_cat/health?v"epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent1523427762 06:22:42 mycluster green 1 1 0 0 0 0 0 0 - 100.0% 节点信息检查 123$ curl "localhost:9200/_cat/nodes?v"ip heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name172.17.0.2 11 91 2 0.00 0.03 0.04 mdi * my_node 列出所有索引信息 12$ curl "localhost:9200/_cat/indices?v"health status index uuid pri rep docs.count docs.deleted store.size pri.store.size 创建索引 创建名为customer的索引 12345678910$ curl -X PUT "localhost:9200/customer?pretty"&#123; "acknowledged" : true, "shards_acknowledged" : true, "index" : "customer"&#125;$ curl "localhost:9200/_cat/indices?v"health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open customer ltwryTbDSUy2mKATPJ2adA 5 1 0 0 1.1kb 1.1kb 创建文档 在customer索引里，创建ID为1的文档。 注意: 加入索引不存在，elasticearch会自动创建索引。 123456789101112131415161718192021222324252627282930313233$ curl -XPUT 'localhost:9200/customer/_doc/1?pretty' -H 'Content-Type: application/json' -d'&#123; "name": "John Doe"&#125;'&#123; "_index" : "customer", "_type" : "_doc", "_id" : "1", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 0, "_primary_term" : 1&#125;# 查看创建的结果$ curl 'localhost:9200/customer/_doc/1?pretty'&#123; "_index" : "customer", "_type" : "_doc", "_id" : "1", "_version" : 2, "found" : true, "_source" : &#123; "name" : "John Doe" &#125;&#125; 删除索引 1234$ curl -X DELETE 'localhost:9200/customer?pretty'&#123; "acknowledged" : true&#125; 修改数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# 创建一个文档$ curl -XPUT 'localhost:9200/customer/_doc/1?pretty' -H 'Content-Type: application/json' -d'&#123; "name": "John Doe"&#125;'&#123; "_index" : "customer", "_type" : "_doc", "_id" : "1", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 0, "_primary_term" : 1&#125;# 修改文档$ curl -XPUT 'localhost:9200/customer/_doc/1?pretty' -H 'Content-Type: application/json' -d'&#123; "name": "xxxxx"&#125;'&#123; "_index" : "customer", "_type" : "_doc", "_id" : "1", "_version" : 2, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 1&#125;## 通过POST方法，在创建文档时可以不用指定文档的id， elasticsearch会自动生成一个$ curl -XPOST 'localhost:9200/customer/_doc?pretty' -H 'Content-Type: application/json' -d'&#123; "name": "Jane Doe"&#125;'&#123; "_index" : "customer", "_type" : "_doc", "_id" : "Umx0s2IBV3QZ-G3Zdj0c", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 0, "_primary_term" : 1&#125; 更新文档 1234567891011121314151617181920$ curl -XPOST 'localhost:9200/customer/_doc/1/_update?pretty' -H 'Content-Type: application/json' -d'&#123; "doc": &#123; "name": "Jane Doe" &#125;&#125;'&#123; "_index" : "customer", "_type" : "_doc", "_id" : "1", "_version" : 3, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 2, "_primary_term" : 1&#125; 删除文档 12345678910111213141516$ curl -XDELETE 'localhost:9200/customer/_doc/2?pretty'&#123; "_index" : "customer", "_type" : "_doc", "_id" : "2", "_version" : 1, "result" : "not_found", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 0, "_primary_term" : 1&#125; 批量处理 同时创建两个文档 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647$ curl -XPOST 'localhost:9200/customer/_doc/_bulk?pretty' -H 'Content-Type: application/json' -d'&#123;"index":&#123;"_id":"1"&#125;&#125;&#123;"name": "John Doe" &#125;&#123;"index":&#123;"_id":"2"&#125;&#125;&#123;"name": "Jane Doe" &#125;'&#123; "took" : 14, "errors" : false, "items" : [ &#123; "index" : &#123; "_index" : "customer", "_type" : "_doc", "_id" : "1", "_version" : 4, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 3, "_primary_term" : 1, "status" : 200 &#125; &#125;, &#123; "index" : &#123; "_index" : "customer", "_type" : "_doc", "_id" : "2", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 1, "status" : 201 &#125; &#125; ]&#125; 更新和删除文档 12345678910111213141516171819202122232425262728293031323334353637383940414243444546$ curl -XPOST 'localhost:9200/customer/_doc/_bulk?pretty' -H 'Content-Type: application/json' -d'&#123;"update":&#123;"_id":"1"&#125;&#125;&#123;"doc": &#123; "name": "John Doe becomes Jane Doe" &#125; &#125;&#123;"delete":&#123;"_id":"2"&#125;&#125;'&#123; "took" : 20, "errors" : false, "items" : [ &#123; "update" : &#123; "_index" : "customer", "_type" : "_doc", "_id" : "1", "_version" : 5, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 4, "_primary_term" : 1, "status" : 200 &#125; &#125;, &#123; "delete" : &#123; "_index" : "customer", "_type" : "_doc", "_id" : "2", "_version" : 2, "result" : "deleted", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 2, "_primary_term" : 1, "status" : 200 &#125; &#125; ]&#125; 当批量操作时，如果某个操作失败，后面的操作仍然会继续执行，并且根据执行顺序，依次返回每个操作的执行状态。 探索数据 在了解了基本操作之后，让我们来多添加一些数据，做一些数据分析的工作。 下载测试用的数据集 1$ wget "https://github.com/elastic/elasticsearch/blob/master/docs/src/test/resources/accounts.json?raw=true" -O accounts.json 导入数据 12345$ curl -H "Content-Type: application/json" -XPOST "localhost:9200/bank/_doc/_bulk?pretty&amp;refresh" --data-binary "@accounts.json"$ curl "localhost:9200/_cat/indices?v"health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open bank EnHJEWvLRv2tZLoI7Z_lXw 5 1 1000 0 474.7kb 474.7kb 查询数据 查询数据，主要有两种方式： REST request URI REST request body 后者跟前者比，可以执行更复杂的操作，查询体也是json格式，便于阅读。前者主要是使用起来更加方便。 比如: 使用request URI的格式 1$ curl "http://localhost:9200/bank/_search?q=*&amp;sort=account_number:asc&amp;pretty" 使用request body的格式 12345678$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "sort": [ &#123; "account_number": "asc" &#125; ]&#125;' 查询语法 Elasticsearch提供了一个类似json格式的查询语法，叫做Query DSL。 限定返回查询结果的个数 123456$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "size": 1&#125;' 分页效果 1234567$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "from": 10, "size": 10&#125;' 排序 123456$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "sort": &#123; "balance": &#123; "order": "desc" &#125; &#125;&#125;' 执行查询 默认情况下，查询返回的_source会包含所有的字段，我们也可以限制返回某些我们关心的字段 只返回account_number和balance 123456$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;, "_source": ["account_number", "balance"]&#125;' match_all 返回所有结果，没有查询条件限制 12345$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123;&#125; &#125;,&#125;' match 限定查询条件，返回account_number为20的结果 12345$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match": &#123; "account_number": 20 &#125; &#125;&#125;' 查询address里包含mill的结果（不区分大小写） 12345$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match": &#123; "address": "mill" &#125; &#125;&#125;' 查询address里包含mill或lane的记录（不区分大小写) 12345$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match": &#123; "address": "mill lane" &#125; &#125;&#125;' match_phrase 查询address里包含mill和lane的记录（不区分大小写) 12345$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_phrase": &#123; "address": "mill lane" &#125; &#125;&#125;' bool must 返回address包含mill和lane的结果，与上面的match_phrase效果一样 123456789101112$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "bool": &#123; "must": [ &#123; "match": &#123; "address": "mill" &#125; &#125;, &#123; "match": &#123; "address": "lane" &#125; &#125; ] &#125; &#125;&#125;' bool should 返回address包含mill或lane的结果，与{&quot;match&quot;: { &quot;address&quot;: &quot;mill lane&quot;}效果一样 123456789101112$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "bool": &#123; "should": [ &#123; "match": &#123; "address": "mill" &#125; &#125;, &#123; "match": &#123; "address": "lane" &#125; &#125; ] &#125; &#125;&#125;' bool must_not 返回address包即不包含mill，也不包含lane的结果 123456789101112$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "bool": &#123; "must_not": [ &#123; "match": &#123; "address": "mill" &#125; &#125;, &#123; "match": &#123; "address": "lane" &#125; &#125; ] &#125; &#125;&#125;' 组合 bool的多个条件还可以相互组合 返回age=40并且state!=ID的结果 1234567891011121314$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "bool": &#123; "must": [ &#123; "match": &#123; "age": "40" &#125; &#125; ], "must_not": [ &#123; "match": &#123; "state": "ID" &#125; &#125; ] &#125; &#125;&#125;' 过滤(Filter) 过滤出20000&lt;balance&lt;30000的结果 1234567891011121314151617$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "query": &#123; "bool": &#123; "must": &#123; "match_all": &#123;&#125; &#125;, "filter": &#123; "range": &#123; "balance": &#123; "gte": 20000, "lte": 30000 &#125; &#125; &#125; &#125; &#125;&#125;' 聚合(Aggregations) 设置size=0是因为我们只关心聚合后的结果 123456789101112$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "size": 0, "aggs": &#123; "group_by_state": &#123; "terms": &#123; "field": "state.keyword" &#125; &#125; &#125;&#125;' 上面的语句等价于 1SELECT state, COUNT(*) FROM bank GROUP BY state ORDER BY COUNT(*) DESC 聚合嵌套 12345678910111213141516171819$ curl -XGET 'localhost:9200/bank/_search?pretty' -H 'Content-Type: application/json' -d'&#123; "size": 0, "aggs": &#123; "group_by_state": &#123; "terms": &#123; "field": "state.keyword" &#125;, "aggs": &#123; "average_balance": &#123; "avg": &#123; "field": "balance" &#125; &#125; &#125; &#125; &#125;&#125;' Eleasticsearch安装及配置 官方文档详细介绍了各种安装Elasticsearch的方法，以及如何配置Elasticsearch。另外， 还介绍了将Elasticsearch迁移到生产环境时，需要关注的配置项等。 docker-compose.yml 使用docker compose, 可以很方便的在本机创建一个关于elastic stack小集群， 对于开发和测试非常方便。 使用下面的docker-compose.yml文件，通过docker compose命令，可以在本地直接运行elasticsearch和kibana服务，kibana里开发工具的Console可以很方便的和elasticsearch交互。 1234567891011121314151617181920212223242526272829303132333435363738394041version: "3"services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch-oss:6.2.3 container_name: elasticsearch ports: - "9200:9200" - "9300:9300" environment: - cluster.name=docker-cluster - bootstrap.memory_lock=true - http.host=0.0.0.0 - transport.host=127.0.0.1 - "ES_JAVA_OPTS=-Xms512m -Xmx512m" ulimits: memlock: soft: -1 hard: -1 volumes: - es_data:/usr/share/elasticsearch/data networks: - esnet kibana: image: docker.elastic.co/kibana/kibana-oss:6.2.3 container_name: kibana environment: SERVER_NAME: kibana-server ELASTICSEARCH_URL: http://elasticsearch:9200 networks: - esnet depends_on: - elasticsearch ports: - "5601:5601"volumes: es_data: driver: localnetworks: esnet: 关于集成更多elastic stack服务，可以参考: https://github.com/elastic/stack-docker/blob/master/docker-compose.yml kibana Console运行效果如下：]]></content>
      <categories>
        <category>elastic</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工银亚洲开户攻略]]></title>
    <url>%2F2018%2F03%2F09%2Ficbcasia%2F</url>
    <content type="text"><![CDATA[开户必读 申请之前，建议阅读 见证开立工银亚洲账户常见问题 开立工银亚洲账户常见问题 工银亚洲账户分综合账户和理财账户的两种，它们之间的区别，以及支持的币种，费用等问题，里面都有答案了。 准备工作 下面内容摘录自上面的文件 国内开户 您无须前往香港，即可通过指定的境内工行网点，见证签署相关文件，代理开立工银亚洲的「理财金账户」或「综合账户」。在所有资料齐全有效的情况下，最快将于一周内完成代理开户手续。 适用对象：年满18周岁的境内居民 办理开户见证所需文件: 境内居民本人有效身份证件 港澳通行证或护照（有效签注三个月以上） 住址证明：最近三个月内由政府机构或公共事务机构发出的列有客户姓名及地址资料的账单或者信函。如有效身份证上地址可显示客户当前的居住地址，则居住住址证明文件可不用提供。 另外，在前往境内工行网点办理正式见证手续之前，您可以预先登录工银亚洲门户网站，通过网上在线填单预先填写申请表格。 网上在线开户请点击：在线申请及查询 有关工银亚洲开户见证服务详情及常见问题请进入工行网站首页 &gt; 个人业务 &gt; 个人金融 &gt; 个人服务 &gt; 跨境金融 &gt; 代理开户见证业务 &gt; 工银亚洲（香港）开户见证服务，即可查询。 境内代理工银亚洲开户见证服务网点名单: 工银亚洲开户见证业务受理网点。 香港柜面开户 您也可以选择亲临工银亚洲位于香港的分行柜面办理开户手续。开户所需文件同开户见证服务。 工银亚洲网点地址查询：请登录工银亚洲网站http://www.icbcasia.com, 在网页最底部“联系我们”，点击“分行网络”，即可查询到工银亚洲的所有网点信息。 我的申请过程 开户网点选择 首先需要说明的是: 并不是所有的工商银行网点都可以办理工银亚洲开户业务。可以下载表格工银亚洲开户见证业务受理网点, 从中选择一个自己比较方便去的网点办理。 线上申请 首先在线申请, 提交表格后。便可以去开户网点进行办理。 PS: 我线上申请后，到了开户网点发现在线申请的表格根本不能用，必须按照网点提供的模板来填写。所以如果可以的话，最好去开户网点先问一下。 网点办理 准备好如下证件: 境内居民本人有效身份证件 港澳通行证或护照（有效签注三个月以上） 住址证明：最近三个月内由政府机构或公共事务机构发出的列有客户姓名及地址资料的账单或者信函。如有效身份证上地址可显示客户当前的居住地址，则居住住址证明文件可不用提供。(注意: 后面审核通过后，工行亚洲银行卡的银行卡就会邮寄到这个地址，所以最好确保填写的地址可以收到信件) 去网点填写申请表后，会拿到一个密码器和三个密码函，密码函里分别是网上银行，电话理财，借记卡的初始密码。 等待资料审核 审核的过程还是比较严格的，我的申请表格被打回来一次，原因就是因为填写的住址地址和身份证上写的有一点小区别。导致我又重新去网点重新提交了一次申请表。 电话确认 提交申请表，大概过了一周的样子，就收到工行打来的电话确认，会问一些开户时提交的资料问题: 如证件号，手机号，地址等。电话确认过后，又是等待资料审核。 密码器激活 电话确认过后，又过了一周多的时间。手机收到了工银亚洲提示开户成功的短信，并有一个激活码, 这激活码是用于激活之前领到的密码器的。 账户激活 需要从国内同名的工行卡汇款至工银亚洲账户，才能激活账号。激活主要分两步骤： 购汇 使用工商银行APP，搜索“结售汇”功能。可以根据当前的汇率，选择购买美元或港币。 境外汇款 通过境外转账业务，将购买的美元或港币，转至工银亚洲账户。 激活步骤完成。 借记卡激活 借记卡是在收到短信激活码后才寄出的，查询借记卡寄送进度: 可拨打95588 转7-7-2-1查询借记卡寄件编号，再按以下任一种方式查询: 中国邮政邮件查询网址：http://yjcx.chinapost.com.cn/ 中国邮政查询电话：11185 按8 选2 进入人工查询 短信查询：编辑短信内容“YTCX+约投挂号信函邮件号码（含字母‘NE’）”发送到11185，11185会以短信的形式告知客户当前的邮寄状态。 收到借记卡后，根据在网点拿到的密码函里的初始密码，在境内工行ATM机办理一笔查询/取款交易即可启用卡片。 最后，祝大家顺利办到工银亚洲卡。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>工银亚洲</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《A Programmer's Guide to Data Mining》读书笔记一]]></title>
    <url>%2F2018%2F02%2F13%2Fa-programmers-guid-to-data-mining-note-two%2F</url>
    <content type="text"><![CDATA[参考: A Programmer’s Guide to Data Mining 基于用户的协同过滤和基于物品的协同过滤算法区别: 基于用户的协同过滤是通过计算用户之间的距离找出相似用户，并将他们评价过的物品推荐给目标用户。而基于物品的协同过滤则是找出最相似的物品，再结合用户的评价来给出推荐结果。 基于用户的协同过滤又称为内存模型，因为我们需要将所有的评价数据都保存在内存中来进行推荐。 基于物品的协同过滤也称为基于模型的协同过滤，因为我们不需要保存所有的评价数据，而是通过构建一个物品相似度模型来做推荐。 常见的基于物品的协同过滤算法 修正的余弦相似度(Adjusted Cosine Similarity) Slope One 算法 修正的余弦相似度算法 计算公式 \[ sim(i, j) = \frac{\sum\limits_{u \in U}(R_{u,i} - \overline{R}_u)(R_{u,j} - \overline{R}_u)}{\sqrt{\sum\limits_{u \in U}(R_{u,i} - \overline{R}_u)^2}\sqrt{\sum\limits_{u \in U}(R_{u,j} - \overline{R}_u)^2}} \] \(U\)表示同时评价过物品\(i\)和\(j\)的用户的集合 上式表示用户u对物品i的评价值减去用户u对所有物品的评价均值，从而得到修正后的评分。 \(sim(i,j)\)表示物品i和j的相似度，分子表示将同时评价过物品\(i\)和\(j\)的用户的修正评分相乘并求和，分母则是对所有的物品的修正评分做一些汇总处理。 Slope One算法 分类 根据物品特征进行分类 特征值的选取很重要 标准化 当物品的特征数值尺度不一致时，就必须进行标准化。利用标准分数(z-score)进行标准化 这种标准化方式的问题在于它会受异常值的影响。 修正的标准分 将标准分公式中的均值改为中位数，将标准差改为绝对偏差。]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker常用命令]]></title>
    <url>%2F2018%2F02%2F12%2Fdocker-commands%2F</url>
    <content type="text"><![CDATA[Install docker on mac 1$ brew cask install docker Get docker info 12$ docker --versionDocker version 17.12.0-ce, build c97c6d6 12$ docker-compose --versiondocker-compose version 1.18.0, build 8dd22a9 12$ docker-machine --versiondocker-machine version 0.13.0, build 9ba6da9 123456789101112131415161718$ docker versionClient: Version: 17.12.0-ce API version: 1.35 Go version: go1.9.2 Git commit: c97c6d6 Built: Wed Dec 27 20:03:51 2017 OS/Arch: darwin/amd64Server: Engine: Version: 17.12.0-ce API version: 1.35 (minimum version 1.12) Go version: go1.9.2 Git commit: c97c6d6 Built: Wed Dec 27 20:12:29 2017 OS/Arch: linux/amd64 Experimental: true Basic commands 12345678910111213141516171819# List Docker CLI commandsdockerdocker container --help# Display Docker version and infodocker --versiondocker versiondocker info# Excecute Docker imagedocker run hello-world# List Docker imagesdocker image ls# List Docker containers (running, all, all in quiet mode)docker container lsdocker container ls -alldocker container ls -a -q Docker image and container command 12345678910111213141516docker build -t friendlyhello . # Create image using this directory's Dockerfiledocker run -p 4000:80 friendlyhello # Run "friendlyname" mapping port 4000 to 80docker run -d -p 4000:80 friendlyhello # Same thing, but in detached modedocker container ls # List all running containersdocker container ls -a # List all containers, even those not runningdocker container stop &lt;hash&gt; # Gracefully stop the specified containerdocker container kill &lt;hash&gt; # Force shutdown of the specified containerdocker container rm &lt;hash&gt; # Remove specified container from this machinedocker container rm $(docker container ls -a -q) # Remove all containersdocker image ls -a # List all images on this machinedocker image rm &lt;image id&gt; # Remove specified image from this machinedocker image rm $(docker image ls -a -q) # Remove all images from this machinedocker login # Log in this CLI session using your Docker credentialsdocker tag &lt;image&gt; username/repository:tag # Tag &lt;image&gt; for upload to registrydocker push username/repository:tag # Upload tagged image to registrydocker run username/repository:tag # Run image from a registry Docker stack commands 12345678docker stack ls # List stacks or appsdocker stack deploy -c &lt;composefile&gt; &lt;appname&gt; # Run the specified Compose filedocker service ls # List running services associated with an appdocker service ps &lt;service&gt; # List tasks associated with an appdocker inspect &lt;task or container&gt; # Inspect task or containerdocker container ls -q # List container IDsdocker stack rm &lt;appname&gt; # Tear down an applicationdocker swarm leave --force # Take down a single node swarm from the manager Docker-machine commands 123456789101112131415161718192021docker-machine create --driver virtualbox myvm1 # Create a VM (Mac, Win7, Linux)docker-machine create -d hyperv --hyperv-virtual-switch "myswitch" myvm1 # Win10docker-machine env myvm1 # View basic information about your nodedocker-machine ssh myvm1 "docker node ls" # List the nodes in your swarmdocker-machine ssh myvm1 "docker node inspect &lt;node ID&gt;" # Inspect a nodedocker-machine ssh myvm1 "docker swarm join-token -q worker" # View join tokendocker-machine ssh myvm1 # Open an SSH session with the VM; type "exit" to enddocker node ls # View nodes in swarm (while logged on to manager)docker-machine ssh myvm2 "docker swarm leave" # Make the worker leave the swarmdocker-machine ssh myvm1 "docker swarm leave -f" # Make master leave, kill swarmdocker-machine ls # list VMs, asterisk shows which VM this shell is talking todocker-machine start myvm1 # Start a VM that is currently not runningdocker-machine env myvm1 # show environment variables and command for myvm1eval $(docker-machine env myvm1) # Mac command to connect shell to myvm1&amp; "C:\Program Files\Docker\Docker\Resources\bin\docker-machine.exe" env myvm1 | Invoke-Expression # Windows command to connect shell to myvm1docker stack deploy -c &lt;file&gt; &lt;app&gt; # Deploy an app; command shell must be set to talk to manager (myvm1), uses local Compose filedocker-machine scp docker-compose.yml myvm1:~ # Copy file to node's home dir (only required if you use ssh to connect to manager and deploy the app)docker-machine ssh myvm1 "docker stack deploy -c &lt;file&gt; &lt;app&gt;" # Deploy an app using ssh (you must have first copied the Compose file to myvm1)eval $(docker-machine env -u) # Disconnect shell from VMs, use native dockerdocker-machine stop $(docker-machine ls -q) # Stop all running VMsdocker-machine rm $(docker-machine ls -q) # Delete all VMs and their disk images 清理命令 To clear containers: 1docker rm -f $(docker ps -a -q) To clear images: 1docker rmi -f $(docker images -a -q) To clear dangling images: 1docker rmi -f $(docker images -f "dangling=true" -q) To clear volumes: 1docker volume rm $(docker volume ls -q) To clear networks: 1docker network rm $(docker network ls | tail -n+2 | awk '&#123;if($2 !~ /bridge|none|host/)&#123; print $1 &#125;&#125;') 查看启动失败的服务信息 1docker service ps --no-trunc &#123;serviceName&#125;]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker简单入门]]></title>
    <url>%2F2018%2F02%2F12%2Fdocker-get-started%2F</url>
    <content type="text"><![CDATA[Docker安装 根据自己的操作系统，参考官方文档完成安装 测试是否安装成功，运行第一个容器 123456789$ docker run hello-worldUnable to find image 'hello-world:latest' locallylatest: Pulling from library/hello-worldca4f61b1923c: Pull completeDigest: sha256:66ef312bbac49c39a89aa9bcc3cb4f3c9e7de3788c944158df3ee0176d32b751Status: Downloaded newer image for hello-world:latestHello from Docker!... 创建镜像 准备创建镜像需要的文件 创建一个空的目录docker-demo，在里面放入三个文件 Dockerfile 1234567891011121314151617181920# Use an official Python runtime as a parent imageFROM python:2.7-slim# Set the working directory to /appWORKDIR /app# Copy the current directory contents into the container at /appADD . /app# Install any needed packages specified in requirements.txtRUN pip install --trusted-host pypi.python.org -r requirements.txt# Make port 80 available to the world outside this containerEXPOSE 80# Define environment variableENV NAME World# Run app.py when the container launchesCMD [&quot;python&quot;, &quot;app.py&quot;] app.py 123456789101112131415161718192021222324252627from flask import Flaskfrom redis import Redis, RedisErrorimport osimport socket# Connect to Redisredis = Redis(host="redis", db=0, socket_connect_timeout=2, socket_timeout=2)app = Flask(__name__)@app.route("/")def hello(): try: visits = redis.incr("counter") except RedisError: visits = "&lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;" html = "&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;" \ "&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;" \ "&lt;b&gt;Visits:&lt;/b&gt; &#123;visits&#125;" return html.format(name=os.getenv("NAME", "world"), hostname=socket.gethostname(), visits=visits)if __name__ == "__main__": app.run(host='0.0.0.0', port=80) requirements.txt 12FlaskRedis 创建好的效果如下: 12$ lsDockerfile app.py requirements.txt 创建镜像 123456$ docker build -t friendlyhello .$ docker imagesREPOSITORY TAG IMAGE IDfriendlyhello latest 326387cea398 运行容器 1$ docker run -p 4000:80 friendlyhello 访问查看效果 分享镜像 由于Dockerhub国内访问速度特别慢，这里用阿里云代替 阿里云Docker web控制台 容器Hub 登录 1234$ docker login registry.cn-hangzhou.aliyuncs.comUsername: your_usernamePassword: your_passwordLogin Succeeded 给镜像打标签 1$ docker tag friendlyhello registry.cn-hangzhou.aliyuncs.com/crazygit/get-started:part2 发布镜像 1$ docker push registry.cn-hangzhou.aliyuncs.com/crazygit/get-started:part2 镜像加速 默认从docker hub上面下载镜像实在太慢了，可以选择使用国内的镜像，比如下面两家: 阿里云 DaoCloud Docker 中国官方镜像加速 以使用阿里云为例，创建开发者账号，获取加速器链接，我的加速器链接链接是 https://b06kc77a.mirror.aliyuncs.com 中国官方镜像加速方法: 1docker pull registry.docker-cn.com/myname/myrepo:mytag]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Fast Data Processing with Spark 2（Third Edition)》读书笔记目录]]></title>
    <url>%2F2018%2F01%2F26%2Ffast-data-processing-with-spark2-note-index%2F</url>
    <content type="text"><![CDATA[目录 《FastDataProcessingwithSpark2读书笔记一》 快速了解Spark的安装，Spark Shell使用和SparkSession对象 《FastDataProcessingwithSpark2读书笔记二》 Spark中加载和保存数据, 以及Spark的一些概念和Spark SQL查询 《FastDataProcessingwithSpark2读书笔记三》 DataSets/DataFrame的常用接口和函数 《FastDataProcessingwithSpark2读书笔记四》 机器学习相关和GraphX（待完善）]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《高效的秘密》读书文摘]]></title>
    <url>%2F2018%2F01%2F26%2Fsmarter-faster-better%2F</url>
    <content type="text"><![CDATA[激发动力 和周围环境的人打好基础，他们会在合适的时机给予帮助，适当地运用小聪明。 &gt; 罗伯特总爱说，为了做生意，他跑遍了墨西哥湾沿岸，任何一条小路和陋巷都留下过他的车辙。公司逐渐发展起来，罗伯特的豪爽也出了名，他总是邀请新奥尔良、亚特兰大等大城市的客户到各色酒吧，尽兴方归。到了次日早晨，罗伯特会趁那些人还处于宿醉状态，让他们心甘情愿地签下几百万美元的订单。酒吧里，调酒师总是不失默契地往罗伯特的杯子里倒苏打水，而给他的大客户们奉上鸡尾酒。 生活没有动力时，还是要学会给自己灌鸡汤。 &gt; 懂得自我激励的人比同辈人的收入更高，幸福感更强，对自己的家庭、工作和生活的满意度也更高。 自己感觉能掌握一切时，能减少忧虑。 生活中有时也是这样，当我们以为掌控了一切的时候，其实有可能在“绕路”。 当我们有能力掌控局面时，大脑会变得兴奋。就像你在高速公路上绕远，但仍然感觉不错，因为一切皆在你的掌控之中。 自己平时给自己定制目标，很多时候都坚持不下来，原因是忘了制定目标的根本意义，最终却把完成目标当成目标了。 &gt; 把任务转化成有意义的决定，动力就会随之产生。 要么做出决定，证明自己仍然掌控人生，要么抱着等待死亡的心态，两者的区别就在于此。 能够让人产生动力的方法有两个：一是确信局面在自己的掌控中；二是给自己的行动赋予更大的意义。 团队合作 为团队成员带来归属感，并鼓励他们抓住机会。 善于观察，提高社交敏感性 &gt; 迈克尔斯对自己的社交敏感性颇感骄傲，以此作为炫耀的资本，他还期待演员和编剧效仿他。在《周六夜现场》开播早期，当看到疲惫不堪的编剧在办公室哭泣时，他会走过去安慰几句。他经常在演员排练和对台词时，悄悄地把演员拉到一边，询问他们的个人生活状况 两个悲哀的事情: 现在工作上开会我们好像都成了吉姆，不知道说什么。 许多领导也习惯了我们变成了吉姆。 ‘吉姆，你一直没有说话，这个问题你怎么看？’，这看似细枝末节，但能对团队产生巨大的影响。 团队怎样合作要比团队由哪些人组成重要得多 优秀团队的成功具有普遍性。团队合作的关键在于，让每一个成员都拥有发言权，而是否参与投票或者做决定其实并不重要，工作量的多少或者是否在一起办公也不会对团队表现产生影响，团队成员是否拥有发言权和社交敏感性才是最重要的 以下5项团队规范是最关键的: 团队成员认为他们的工作是重要的。 团队成员感觉他们的工作对自己是有意义的。 团队有明确的目标和分工。 团队成员知道他们可以彼此信赖。 最重要的是，团队要有心理安全感。 保持专注 认知隧道”（cognitive tunneling）的精神障碍。当自动化系统突然关闭时，我们被迫启动自己的注意力，大脑从放松状态进入惊慌状态，就有可能产生这种障碍。 懂得如何分配自己的注意力，习惯性地构建有力的心智模型的人往往能获得更高的收入和取得更好的成绩 要想成为真正高效的人，我们必须懂得掌控自己的注意力；必须构建心智模型，从而获得主动权。在开车上班的路上，要求自己想象一整天的工作情况。开会或者用餐的时候，向自己描述所见到的事物及其意义。主动寻找听众，向别人讲述，甚至挑战他们。而且，要让自己习惯于预测接下来会发生什么。如果你是一位家长，请想一想孩子们在晚餐时会说些什么。这样你就能注意到孩子漏说了什么，或者说错了什么，从而给予他们帮助 设定目标 “认知闭合需求”（the need for cognitive closure）的动机，心理学家将其定义为：“针对一个问题做出的明确判断——无论什么判断——相对于混乱和不确定而言，任何明确的判断都更好 设定目标时必须考虑SMART原则 &gt; 目标必须是 &gt; &gt; - 具体的（Specific） &gt; - 可衡量的（Measurable） &gt; - 可达成的（Achievable） &gt; - 现实的（Realistic） &gt; - 有完成期限的（Timeline） &gt; &gt; 换言之，计划必须切实可行，目标必须能够实现。 与简单的目标或者模糊、抽象的目标（比如‘做到最好’）相比，具体的高目标更有可能实现。 在完成目标时，应该避免出现选择简单任务，给虚假的快感这种境况 &gt; 设定SMART目标的人倾向于选择最简单的任务，沉迷于完成项目的快感，并且一旦设定目标，就会考虑优先顺序。“你会形成这样一种思维定式：在每一项待办事项旁边打钩远比思考这项任务是否有意义更重要。 延展目标 &gt; 从制度上鼓励员工设定大胆的目标。在推进项目时，每个高管和部门除了要设定具体的、可达成的、有完成期限的目标以外，还应该设定一个延展目标（stretch goal）—管理者无法描述（至少在初期）应该如何实现的目标 之所以要同时设定延展目标和SMART目标，是因为冒险本身令人恐惧。面对延展目标，我们往往不知从何处着手。那么，为了避免延展目标仅停留在愿望层面，我们需要通过严谨的思考把看似遥不可及的目标转变成一系列现实的短期目标。知道如何设定SMART目标的人往往习惯于把目标细化成一个个可操作的任务。因此，当他们遇到看似超出自己能力范围的目标时，知道应该做些什么。把延展目标和SMART目标结合在一起，能够帮助我们把不太可能实现的目标变为可能。 自己也经常犯这样的错误 &gt; 有时喜欢写下可立即完成的待办事项，这让我感觉很好’时，这其实是使用待办事项的错误方式，因为他们在用待办事项调节自己的情绪，而不是为了提高效率。 在完成目标的路上，我们也要时长思考目标的意义，避免走错路 &gt; 有时候，只设定延展目标和SMART目标是不够的。除了树立远大的志向和制订脚踏实地的计划，我们需要跳出日常工作的范畴，想一想我们正在追求的目标是不是有意义。我们始终都需要思考。 掌握力 每个人都愿意去寻找做好事带来的成就感，这种感觉有时候甚至高于物质奖励 谁都不会为了失败而工作，如果你给他们成功的机会，他们就愿意尝试 把决策权交给能解决问题的人 如果员工们连犯错的机会都没有，这将是企业最大的损失 做决定 如果你无法判断哪一个变量更重要，就用等量加权的方法 培养概率思维需要我们质疑自己的假设，并习惯接受事物的不确定性。要想更好地预测未来，更好地做出决定，我们就必须知道自己希望发生之事和有可能或不可能发生之事的区别。 贝叶斯法则的核心理念是：即使只有少量数据，我们仍然能够通过提出假设并根据对事物的观察来修正这些假设，从而达到预测未来的目的。 和失败的人在一起时，要思考失败的原因 最优秀的企业家能够很清醒地意识到只和成功人士交谈的危险 自己在生活中试图把控和掌握一切，常常也因为一些未知和不确定性焦虑，学会接受人生的不确定性！ &gt; 必须接受人生的不确定性，我用这种方法赶走了焦虑。我们能做的就是学会如何做出最好的决定，并相信假以时日好运必然会降临。 创新 如果某种想法已经在头脑中根深蒂固，就会把其他想法彻底排除，也就没有其他可能的选择。所以，有时候激发创造力最好的方式就是加入适当的干扰，留出足够的空间让阳光照射进来。 如果你也想成为创意经纪人，提高自己的创新效率，以下三点会对你有所帮助: 要学会关注你的个人经历和你对事物的感受，这有助于你分清什么是陈词滥调，什么是真知灼见。 要明白，在创新过程中，你的压力和恐惧并不意味着一切都完结了。相反，我们要保持头脑的灵活性，从而产生一些新想法。创造性绝望至关重要，焦虑能够推动我们以全新的视角审视过去的想法。摆脱混乱局面的方法是，思考你的所知，重新审视你认为行之有效的惯用手段，用它们解决新的问题。创作过程中经历的痛苦是值得的。 要记住，创造性突破虽然令人欣慰，但也“屏蔽”了我们更多的选择，所以一定要和已经取得的成果保持适度距离。在没有自我批判也没有压力的情况下，某个想法会迅速排挤掉其他想法。要想实现这个关键的距离，必须重新审视已经取得的成就，从完全不同的角度进行思考，改变团队的内部结构，或者授权给一个新人。在这个过程中，干扰是必不可少的，我们可以大胆推翻之前的成果来保持思维的活跃度，关键是把干扰控制在合理的水平上 事实上，以上三点可以归结为：创新过程中可以出现差错，也可以推翻重来。这一点很重要，它意味着任何人都具有创造性，都能成为创意经纪人。我们有各自的人生体验，会遭遇各种干扰和压力，这是成为创意经纪人的必经之路，只要我们能够欣然接受创造性绝望和改变，并尝试从不同的角度审视过去的想法。 获取并应用数据 数据越来越充足，却无法有效地利用数据，这种现象被称为“信息盲”（information blindness） 将数据转变为有效的信息 分类 让信息变得有意义 书中核心观点使用指南 如何激发动力 &gt; - 证明选择权掌握在你自己手中。如果你正在回复邮件，先写下一句能够表达你的主要观点和决定的话。如果你要进行一次重要的谈话，请提前决定谈话的地点。拥有掌控权比做出具体的选择更能激发你的动力。 &gt; - 想清楚这个任务为什么与你在乎的事物息息相关。告诉自己这项任务为什么能让你离那个有意义的目标更近一步。告诉自己为什么这项任务很重要，然后你就会发现迈出第一步变得容易了。 &gt; 需要设定两种目标 &gt; - 一种是符合我的远大志向的延展目标。 &gt; - 一种是有助于我制订具体行动计划的SMART目标。 如何设定目标 &gt; - 设定一个延展目标，一个能够代表你远大志向的目标。 &gt; - 把这个目标分解成很多子目标和SMART目标。 如何保持专注力 &gt; 想象会发生什么。首先会发生什么？潜在的障碍是什么？你怎样提前扫除障碍？给自己讲一个你期待发生的故事，在你的计划和现实生活发生冲突时，你会很容易决定应该把注意力放在哪里。 如何更好地做决定 &gt; - 想象各种可能出现的结果，鼓励自己想象各种各样的可能性，即使有些可能是自相矛盾的，那么你会做出更明智的选择。 &gt; - 我们可以培养自己的贝叶斯思维，通过了解不同的经验、视角和其他人的观点，获取信息，并了解信息，你的选择就会更加正确。 如何使团队更高效 &gt; - 管理是指管理团队的方式，而不是团队里的人。当每一个团队成员感觉自己可以在公平的环境中发表意见，成员之间懂得关注彼此的感受时，心理安全感就会产生。 &gt; - 如果你正在管理一个团队，想想你传递出的信息是什么。你是鼓励每个成员拥有均等的发言机会，还是奖励发言最多的人？你想通过重复别人的观点、对他人的提问和看法做出回应来表示你在倾听吗？当有人感到沮丧或者不安时，你会表示关注吗？你会带头关注别人的感受，给其他成员做榜样吗？ 如何有效地管理他人 &gt; - 精益和敏捷管理方法告诉我们，如果员工认为自己拥有更多决定权，以及他们的同事都在朝着同一个方向努力，他们的工作就会更具智慧、更优秀。 &gt; - 让与问题最直接相关的人做决定，领导者可以借此充分利用每个人的专长，激发他们的创新力。 &gt; - 掌控力可以激发动力，要想激发人们思考和解决问题的动力，就要让人们知道他们的建议不会被忽略，也不会因为犯错而受到责怪。 如何鼓励创新 &gt; - 创造性源于把旧想法用新方式结合起来，“创意经纪人”是关键。首先，你自己要成为创意经纪人，然后鼓励团队成员也这样做。 &gt; - 要了解自己的个人经历，学会关注自己对事物的感受。这样做才可以很好地分清什么是陈词滥调，什么是真知灼见，还要研究自己的情感反应。 &gt; - 要明白，在创新过程中压力并不意味着一切都完了。相反，创造性绝望至关重要，焦虑能促使我们用新视角看待过去的想法。 &gt; - 最后，要记住，创造性突破虽然令人欣慰，但也会让我们“屏蔽”更多的选择。我们要客观地评价所取得的成绩，从不同的角度思考问题，赋权给某一成员，让她/他去做未做过的工作，使我们的思维保持活跃。 如何更好地获取数据 &gt; - 当我们得到新信息后，我们应该对信息进行处理。做笔记解释你刚刚学到的东西，或者想出能够验证观点的办法，或者在纸上把一系列数据点绘制成图表，或者跟朋友分享你的观点。人生中的每一个选择都是一种实验，关键是要看到蕴藏其中的数据，并加以运用，这样我们才能从中学到些什么。]]></content>
      <categories>
        <category>读书文摘</category>
      </categories>
      <tags>
        <tag>高效</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《A Programmer's Guide to Data Mining》读书笔记一]]></title>
    <url>%2F2018%2F01%2F22%2Fa-programmers-guid-to-data-mining-note-one%2F</url>
    <content type="text"><![CDATA[参考: A Programmer’s Guide to Data Mining 推荐系统中，最常用的方法就是协同过滤。顾名思义: 这个方法就是利用别人的喜好来推测你的喜好。 假如你喜欢看电影A和B， 另外一个和你相似的用户也喜欢电影A和B，同时他还喜欢看电影C，那么我们就会推测你也喜欢电影C。 如图所示 如何找到相似的用户 相似度计算方法有很多，本文将主要介绍以下几种 距离算法 曼哈顿距离(Manhattan Distance) 欧几里得距离(Euclidean Distance) 切比雪夫距离(Chebyshev Distance) 闵可夫斯基距离(Minkowski Distance) 相似度系数算法 皮尔逊相关系数(Pearson Correlation Coefficient) 向量空间余弦相似度(Cosine Similarity) 对于相同的计算数据，不同的相似度计算方法结果不同，所以需要根据计算样本的特性，选择合适的计算方式。 更多相似度计算方法可以参考: 常见的距离算法和相似度(相关系数)计算方法 机器学习中的相似性度量 常用的相似度计算方法原理及实现 曼哈顿距离(Manhattan Distance) 参考百度百科 曼哈顿距离，又叫出租车距离（Manhattan Distance）是由十九世纪的赫尔曼·闵可夫斯基所创词汇 ，是种使用在几何度量空间的几何学用语，用以标明两个点在标准坐标系上的绝对轴距总和。 图中红线代表曼哈顿距离，绿色代表欧氏距离，也就是直线距离，而蓝色和黄色代表等价的曼哈顿距离。曼哈顿距离——两点在南北方向上的距离加上在东西方向上的距离，即\(d(i, j）= |x_i - x_j|+|y_i - y_j|\)。对于一个具有正南正北、正东正西方向规则布局的城镇街道，从一点到达另一点的距离正是在南北方向上旅行的距离加上在东西方向上旅行的距离，因此，曼哈顿距离又称为出租车距离。曼哈顿距离不是距离不变量，当坐标轴变动时，点间的距离就会不同。曼哈顿距离示意图在早期的计算机图形学中，屏幕是由像素构成，是整数，点的坐标也一般是整数，原因是浮点运算很昂贵，很慢而且有误差，如果直接使用AB的欧氏距离(欧几里德距离: 在二维和三维空间中的欧氏距离的就是两点之间的距离），则必须要进行浮点运算，如果使用AC和CB，则只要计算加减法即可，这就大大提高了运算速度，而且不管累计运算多少次，都不会有误差。 二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离 \[ d_{ab} = |x_1 - x_2| + |y_1 - y_2| \] 两个n维向量a(x11,x12,…,x1n)与b(x21,x22,…,x2n)间的曼哈顿距离 $$d_{ab} =\sum\limits_{k=1}^n|x_{1k} - x_{2k}|$$ 欧几里得距离(Euclidean Distance) 参考维基百科 在数学中，欧几里得距离或欧几里得度量是欧几里得空间中两点间“普通”（即直线）距离 二维平面上两点a(x1,y1)与b(x2,y2)间的欧几里得距离: \[ d_{ab} = \sqrt{(x_1-x_2)^2 + (y_1-y_2)^2} \] 两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧几里得距离 $$d_{ab} = \sqrt{\sum\limits_{k=1}^n(x_{1k} - x_{2k})^2}$$ 切比雪夫距离(Chebyshev Distance) 参考维基百科 数学上，切比雪夫距离（Chebyshev distance）或是L∞度量是向量空间中的一种度量，二个点之间的距离定义为_其各座标数值差的最大值_。以(x1,y1)和(x2,y2)二点为例，其切比雪夫距离为max(|x2-x1|,|y2-y1|)。切比雪夫距离得名自俄罗斯数学家切比雪夫。 若将国际象棋棋盘放在二维直角座标系中，格子的边长定义为1，座标的x轴及y轴和棋盘方格平行，原点恰落在某一格的中心点，则王从一个位置走到其他位置需要的步数恰为二个位置的切比雪夫距离，因此切比雪夫距离也称为棋盘距离。例如位置F6和位置E2的切比雪夫距离为4。任何一个不在棋盘边缘的位置，和周围八个位置的切比雪夫距离都是1。 二维平面两点a(x1,y1)与b(x2,y2)间的切比雪夫距离 \[ d_{ab} = max(|x_1-x_2|, |y_1 - y_2|) \] 两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的切比雪夫距离 $$d_{ab} = max(|x_{1i}-x_{2i}|)$$ 等价于 $$d_{ab} = \lim\limits_{k \to \infty}(\sum\limits_{i=1}^{n}|x_{1i} - x_{2i}|^k)^{\frac{1}{k}}$$ 闵可夫斯基距离(Minkowski Distance) 可以将曼哈顿距离，欧几里得距离，切比雪夫距离三种距离的计算公式归纳 为一个计算公式，这就是闵可夫斯基距离(Minkowski Distance)。 \[ d(x, y) = (\sum\limits_{k=1}^n|x_k - y_k|^r)^{\frac{1}{r}} \] \(r = 1\), 该公式就是曼哈顿距离 \(r = 2\), 该公式就是曼哈顿距离 \(r = \infty\), 该公式就是切比雪夫距离 皮尔逊相关系数(Pearson Correlation Coefficient) 参考: 皮尔逊相关系数 余弦相似度(Cosine Similarity) 参考百度百科 余弦相似度，又称为余弦相似性，是通过计算两个向量的夹角余弦值来评估他们的相似度。余弦相似度将向量根据坐标值，绘制到向量空间中，如最常见的二维空间。 二维平面上，假设向量a、b的坐标分别为(x1,y1)、(x2,y2) 。则: \[ cos\theta = \frac{x_1x_2+y_1y_2}{\sqrt{x_1^2 + y_1^2}\sqrt{x_2^2 + y_2^2}} \] 推广到n维空间，计算公式为: \[ cos\theta = \frac{\sum\limits_1^n(Ai \times Bi)}{\sqrt{\sum\limits_1^nA_i^2}\sqrt{\sum\limits_1^nB_i^2}} \] 性质 余弦值的范围在[-1,1]之间，值越趋近于1，代表两个向量的方向越接近；越趋近于-1，他们的方向越相反；接近于0，表示两个向量近乎于正交。 实战 在介绍了一大堆的相似度计算之后，我们来通过一些例子，说明具体的用法。 假设我们要为一个在线音乐网站的用户推荐乐队。用户可以用1-5颗星来评价他喜欢的乐队，其中包含半颗星(如2.5星)，下表展示了8位用户对8支乐队的评价: 图中的-表示没有评分。在计算两个用户的距离时，只考虑他们都评价过的乐队。暂时不考虑数据缺失的情况。 数据转换成代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import jsonusers_string = '''&#123; "Angelica": &#123; "Blues Traveler": 3.5, "Broken Bells": 2, "Norah Jones": 4.5, "Phoenix": 5, "Slightly Stoopid": 1.5, "The Strokes": 2.5, "Vampire Weekend": 2 &#125;, "Bill": &#123; "Blues Traveler": 2, "Broken Bells": 3.5, "Deadmau5": 4, "Phoenix": 2, "Slightly Stoopid": 3.5, "Vampire Weekend": 3 &#125;, "Chan": &#123; "Blues Traveler": 5, "Broken Bells": 1, "Deadmau5": 1, "Norah Jones": 3, "Phoenix": 5, "Slightly Stoopid": 1 &#125;, "Dan": &#123; "Blues Traveler": 3, "Broken Bells": 4, "Deadmau5": 4.5, "Phoenix": 3, "Slightly Stoopid": 4.5, "The Strokes": 4, "Vampire Weekend": 2 &#125;, "Hailey": &#123; "Broken Bells": 4, "Deadmau5": 1, "Norah Jones": 4, "The Strokes": 4, "Vampire Weekend": 1 &#125;, "Jordyn": &#123; "Broken Bells": 4.5, "Deadmau5": 4, "Norah Jones": 5, "Phoenix": 5, "Slightly Stoopid": 4.5, "The Strokes": 4, "Vampire Weekend": 4 &#125;, "Sam": &#123; "Blues Traveler": 5, "Broken Bells": 2, "Norah Jones": 3, "Phoenix": 5, "Slightly Stoopid": 4, "The Strokes": 5 &#125;, "Veronica": &#123; "Blues Traveler": 3, "Norah Jones": 5, "Phoenix": 4, "Slightly Stoopid": 2.5, "The Strokes": 3 &#125;&#125;'''users = json.loads(users_string) 计算曼哈顿距离 12345678def manhattan(rating1, rating2): """计算曼哈顿距离""" distance = 0 for key in rating1: if key in rating2: difference = abs(rating1[key] - rating2[key]) distance += difference return distance 测试一下效果 12345&gt;&gt;&gt; manhattan(users['Hailey'], users['Veronica'])2&gt;&gt;&gt; manhattan(users['Hailey'], users['Jordyn'])7.5 下面再通过一个函数，实现找出最近的用户距离，该函数会返回一个用户列表，按照距离排序 12345678def computeNearestNeighbor(username, users): distances = [] for user in users: if user != username: distance = manhattan(users[username], users[user]) distances.append((distance, user)) distances.sort() return distances 测试一下，计算与用户Hailey最相似的用户 12345678&gt;&gt;&gt; computeNearestNeighbor('Hailey', users)[(2, 'Veronica'), (4, 'Chan'), (4, 'Sam'), (4.5, 'Dan'), (5.0, 'Angelica'), (5.5, 'Bill'), (7.5, 'Jordyn')] 最后再来根据距离做推荐 123456789101112def recommend(username, users): # 找到距离最近的用户 nearest = computeNearestNeighbor(username, users)[0][1] recommendations = [] # 找出这位用户评价过，但自己未评价过的乐队 neighborRatings = users[nearest] userRatings = users[username] for artist in neighborRatings: if not artist in userRatings: recommendations.append((artist, neighborRatings[artist])) # 按照评分进行排序 return sorted(recommendations, key=lambda recommend: recommend[1], reverse=True) 为用户Hailey做推荐 12&gt;&gt;&gt; recommend('Hailey', users)[('Phoenix', 4), ('Blues Traveler', 3), ('Slightly Stoopid', 2.5)] 闵科夫斯基算法 1234567def minkowski(rating1, rating2, r): """闵可夫斯基算法""" distance = 0 for key in rating1: if key in rating2: distance += pow(abs(rating1[key] - rating2[key]), r) return pow(distance, 1.0/r) 皮尔逊系数 1234567891011121314151617181920212223242526def pearson(rating1, rating2): sum_xy = 0 sum_x = 0 sum_y = 0 sum_x2 = 0 sum_y2 = 0 n = 0 for key in rating1: if key in rating2: n += 1 x = rating1[key] y = rating2[key] sum_xy += x * y sum_x += x sum_y += y sum_x2 += pow(x, 2) sum_y2 += pow(y, 2) if n == 0: return 0 # now compute denominator denominator = (sqrt(sum_x2 - pow(sum_x, 2) / n) * sqrt(sum_y2 - pow(sum_y, 2) / n)) if denominator == 0: return 0 else: return (sum_xy - (sum_x * sum_y) / n) / denominator 相似度算法选择 根据数据的特性，不同的算法计算的结果会有偏差，但是大体上满足如下规律: 如果数据存在“分数膨胀”，就使用皮尔逊相关系数 如果数据比较密集，变量之间基本存在公有值，且这些距离数据都是非常重要的，那就使用曼哈顿距离或欧几里得距离 如果数据是稀释的，就用余弦定理 距离计算库 scipy.spatial.distance模块包含了各种距离的计算方式。 scipy.spatial.distance.cityblock 曼哈顿距离 scipy.spatial.distance.euclidean 欧几里得距离 scipy.spatial.distance.chebyshev 切比雪夫距离 scipy.spatial.distance.minkowski 闵可夫斯基距离 scipy.stats.pearsonr 皮尔逊相关系数 K最邻近算法 如果只使用最相似的一个用户来做推荐，如果这个用户有特殊的偏好，就会直接反应在推荐内容里。解决方法之一就是寻找多个相似的用户，这里就要用到K最邻近算法了。 完整实现请参考: https://github.com/zacharski/pg2dm-python/blob/master/ch2/recommender.py]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
        <tag>相似度计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[皮尔逊相关系数(Pearson Correlation Coefficient)]]></title>
    <url>%2F2018%2F01%2F19%2Fpearson-correlation-coefficient%2F</url>
    <content type="text"><![CDATA[在介绍皮尔逊相关系数前，先来回顾一些统计相关的术语，便于我们更好的理解后面的计算公式。 方差（Variance） 参考百度百科 注意: 方差在统计描述和概率分布中各有不同的定义，并有不同的公式。 下面主要介绍在统计描述中的方差 在统计描述中，方差用来计算每一个变量（观察值）与总体均数之间的差异。为避免出现离均差总和为零，离均差平方和受样本含量的影响，统计学采用平均离均差平方和来描述变量的变异程度。 总体方差计算公式为: \[ \sigma^2 = \frac{\sum(X-\mu)^2}{N} \] \(\sigma^2\)为总体方差，\(X\)为变量，\(\mu\)为总体均值，\(N\)为总体例数。 实际工作中，总体均数难以得到时，应用样本统计量代替总体参数，经校正后， 样本方差计算公式： \[ S^2 = \frac{\sum(X-\bar{X}) ^2}{(n-1)} \] \(S^2\)为样本方差，\(X\)为变量，\(\bar{X}\)为样本均值，\(n\)为样本例数。 例如: 两个人X, Y 5次考试测验的分数分别是: X: 50, 100, 100, 60, 50 Y: 73, 70, 75, 72, 70 X, Y的平均值分别为: \[ \mu_x = \bar{X} = \frac{(50+100+100+60+50)}{5} = 72 \] \[ \mu_y = \bar{Y} = \frac{(73+70+75+72+70)}{5} = 72 \] 总体方差为: \[ \sigma^2_x = \frac{(50-72)^2+(100-72)^2+(100-72)^2+(60-72)^2+(50-72)^2}{5}= \frac{2680}{5}=536 \] \[ \sigma^2_y = \frac{(73-72)^2+(70-72)^2+(75-72)^2+(72-72)^2+(70-72)^2}{5}=\frac{18}{4}=3.6 \] 样本方差为: \[ S^2_x = \frac{(50-72)^2+(100-72)^2+(100-72)^2+(60-72)^2+(50-72)^2}{5-1} = \frac{2680}{4} = 670 \] \[ S^2_y = \frac{(73-72)^2+(70-72)^2+(75-72)^2+(72-72)^2+(70-72)^2}{5-1}=\frac{18}{4}=4.5 \] 使用python模块计算 statistic和numpy包含了计算方差的方式 statistics.variance计算样本方差 1234567&gt;&gt;&gt; import statistics&gt;&gt;&gt; statistics.variance([50, 100, 100, 60, 50])670&gt;&gt;&gt; statistics.variance([73, 70, 75, 72, 70])4.5 numpy可以通过设置参数ddof参数来选择计算方式。默认是计算全局方差 计算全局方差(ddof=0) 12345&gt;&gt;&gt; np.var([50, 100, 100, 60, 50])536.0&gt;&gt;&gt; np.var([73, 70, 75, 72, 70])3.6 计算样本方差(ddof=1) 12345&gt;&gt;&gt; np.var([50, 100, 100, 60, 50], ddof=1)670.0&gt;&gt;&gt; np.var([73, 70, 75, 72, 70], ddof=1)4.5 扩展阅读 Computing Sample Variance: Why Divide by N - 1? Variance: The Mystery of n-1 (Part 1: The problem with using n) 标准差(Standard Deviation) 参考百度百科 标准差（Standard Deviation）, 中文环境中又常称均方差，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的两组数据，标准差未必相同。 标准差的计算公式为: \[ \sigma = \sqrt{方差} = \sqrt{\frac{(X-\mu)^2}{N}} \] 同计算方差类似，计算标准差如下: 样本的标准差 12345&gt;&gt;&gt; statistics.stdev([50, 100, 100, 60, 50])25.88435821108957&gt;&gt;&gt; statistics.stdev([73, 70, 75, 72, 70])2.1213203435596424 总体的样本差 12345&gt;&gt;&gt; np.std([50, 100, 100, 60, 50])23.15167380558045&gt;&gt;&gt; np.std([73, 70, 75, 72, 70])1.8973665961010275 样本的标准差 12345&gt;&gt;&gt; np.std([50, 100, 100, 60, 50], ddof=1)25.88435821108957&gt;&gt;&gt; np.std([73, 70, 75, 72, 70], ddof=1)2.1213203435596424 由标准差可以看出，虽然学生X, Y的平均分数相同(都是72分) X: 50, 100, 100, 60, 50 Y: 73, 70, 75, 72, 70 但是,Y的标准差小于X的标准差。Y的考试成绩更稳定。 标准分数(Standard Score) 参考百度百科 标准分数也叫z分数(z-score)，是一种具有相等单位的量数。它是将原始分数与团体的平均数之差除以标准差所得的商数，是以标准差为单位度量原始分数离开其平均数的分数之上多少个标准差，或是在平均数之下多少个标准差。它是一个抽象值，不受原始测量单位的影响，并可接受进一步的统计处理 计算公式为: \[ z = \frac{x - \mu}{\sigma} \] \(x\)为某一具体分数，\(\mu\)为平均数，\(\sigma\)为标准差。 使用场景 某中学高一班期末考试，已知语文期末考试的全班平均分为73分，标准差为7分，甲得了78分；数学期末考试的全班平均分为80分，标准差为6.5分，甲得了83分。甲哪一门考试成绩比较好？ 因为两科期末考试的标准差不同，因此不能用原始分数直接比较。需要将原始分数转换成标准分数，然后进行比较。 12Z(语文)=(78-73)/7=0.71Z(数学)=(83-80)/6.5=0.46 甲的语文成绩在其整体分布中位于平均分之上0.71个标准差的地位，他的数学成绩在其整体分布中位于平均分之上0.46个标准差的地位。 由此可见，甲的语文期末考试成绩优于数学期末考试成绩。 由于标准分数不仅能表明原始分数在分布中的地位，它还是以标准差为单位的等距量表，故经过把原始分数转化为标准分数，可以在不同分布的各原始分数之间进行比较。 使用python模块计算 12345678910111213&gt;&gt;&gt; from scipy import stats&gt;&gt;&gt; stats.zscore([50, 100, 100, 60, 50])array([-0.95025527, 1.2094158 , 1.2094158 , -0.51832106, -0.95025527])&gt;&gt;&gt; stats.zscore([73, 70, 75, 72, 70])array([ 0.52704628, -1.05409255, 1.58113883, 0. , -1.05409255])&gt;&gt;&gt; stats.zscore([50, 100, 100, 60, 50], ddof=1)array([-0.84993415, 1.08173437, 1.08173437, -0.46360045, -0.84993415])&gt;&gt;&gt; stats.zscore([73, 70, 75, 72, 70], ddof=1)array([ 0.47140452, -0.94280904, 1.41421356, 0. , -0.94280904]) 皮尔逊相关系数(Pearson Correlation Coefficient) 参考百度百科 &gt; 相关系数是最早由统计学家卡尔·皮尔逊设计的统计指标，是研究变量之间线性相关程度的量，一般用字母 r 表示。由于研究对象的不同，相关系数有多种定义方式，较为常用的是皮尔逊相关系数。 需要说明的是，皮尔逊相关系数并不是唯一的相关系数，但是最常见的相关系数 定义1 两个变量之间的皮尔逊相关系数定义为两个变量之间的协方差和标准差的商 计算公式如下: \[ P_{xy}=\frac{cov(X, Y)}{\sigma_x\sigma_y} \] 定义2 &gt; 相关系数r亦可由\((X_i,Y_i)\)样本点的标准分数均值估计 计算公式如下: $$r_{xy} = \frac{1}{n}\sum\limits_{i=1}^n(Z_x)_i(Z_y)_i \\\\ = \frac{1}{n}\sum\limits_{i=1}^n\cdot\frac{x-\bar{x}}{\sqrt{\frac{1}{n}\sum\limits_{i=1}^n(x-\bar{x})^2}}\cdot\frac{y-\bar{y}}{\sqrt{\frac{1}{n}\sum\limits_{i=1}^n(y-\bar{y})^2}} \\\\ = \frac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum\limits_{i=1}^n(x-\bar{x})^2}\sqrt{\sum\limits_{i=1}^n(y-\bar{y})^2}}$$ 其中\(Z_x\)为\(x\)的标准分数，其中\(Z_y\)为\(y\)的标准分数。\(\bar{x}\)为x的平均数，\(\bar{y}\)为y的平均数。 举个例子(来自李政轩老师的视频教程皮尔逊相关系数): 有学生A, B, C…G的数学成绩和自然成绩分别如下, 现在需要分析学生的数学成绩和自然成绩之间是否存在关系。比如学生数学成绩好的自然成绩也好？ 相关系数 标准化成绩的计算方式如下: 123456789101112131415161718192021222324&gt;&gt;&gt; from scipy import stats&gt;&gt;&gt; import numpy as np# 设置打印3位小数&gt;&gt;&gt; np.set_printoptions(precision=3)# 标准化数学成绩&gt;&gt;&gt; zscore_math = stats.zscore([74, 76, 77, 63, 63, 61, 72], ddof=1)&gt;&gt;&gt; zscore_matharray([ 0.667, 0.959, 1.105, -0.938, -0.938, -1.23 , 0.375])# 标准化自然成绩&gt;&gt;&gt; zscore_sciense = stats.zscore([84, 83, 85, 74, 75, 81, 73], ddof=1)&gt;&gt;&gt; zscore_sciensearray([ 0.92 , 0.725, 1.116, -1.032, -0.837, 0.335, -1.227])# 两个标准化成绩相乘&gt;&gt;&gt; dot_math_sciense = zscore_math * zscore_sciense&gt;&gt;&gt; dot_math_sciensearray([ 0.614, 0.695, 1.233, 0.968, 0.785, -0.412, -0.461]) 相关系数 12345# 下面除以(n-1)是因为前面一直用样本方差来计算的&gt;&gt;&gt; r = sum(dot_math_sciense) / (7-1)&gt;&gt;&gt; r0.5704948696393594 相关系数为0.570, 说明数学成绩和自然成绩之间没有什么关系 利用现成的库计算 1234567# 返回的元组第一个数就是相关系数&gt;&gt;&gt; scipy.stats.pearsonr([74, 76, 77, 63, 63, 61, 72], [84, 83, 85, 74, 75, 81, 73])(0.5704948696393595, 0.18107908965203987)&gt;&gt;&gt; np.corrcoef([74, 76, 77, 63, 63, 61, 72], [84, 83, 85, 74, 75, 81, 73])array([[1. , 0.57], [0.57, 1. ]]) 相关系数的特性 \[ -1 \leq r_{xy} \leq 1 \] 1表示完全正相关，-1表示完全负相关]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>statistics</tag>
        <tag>variance</tag>
        <tag>standard deviation</tag>
        <tag>standard score</tag>
        <tag>Pearson Correlation Coefficient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Fast Data Processing with Spark 2（Third Edition)》读书笔记四]]></title>
    <url>%2F2018%2F01%2F17%2Ffast-data-processing-with-spark2-note-four%2F</url>
    <content type="text"><![CDATA[本书其它笔记《Fast Data Processing with Spark 2（Third Edition)》读书笔记目录 机器学习和Spark ML Pipelines Spark机器学习算法表 机器学习的相关算法: pyspark.ml 机器学习算法表 机器学习处理流程 机器学习WorkFlow 机器学习API结构图 下面部分等待完善 基本统计 线性回归 分类 Clustering Recommendation Hyper paramters GraphX]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Fast Data Processing with Spark 2（Third Edition)》读书笔记三]]></title>
    <url>%2F2018%2F01%2F16%2Ffast-data-processing-with-spark2-note-three%2F</url>
    <content type="text"><![CDATA[本书其它笔记《Fast Data Processing with Spark 2（Third Edition)》读书笔记目录 数据分析的主力Datasets/DataFrames DataSets概述 Spark中，Dataset就是一组各式各样的列，类似一张excel表格或关系型数据库中的表。可以用于类型检查和语义化查询。 在R和Python语言中，使用的依然是DataFrame类，但是包含了所有的DataSet APIs。因此可以这样认为，DataSet在Python和R语言中就叫做DataFrame。 在Scala和Java语言中，使用的是DataSet接口，不存在DataFrame。 DataSet API概览 DataSet APIs 下面的例子使用的数据文件下载地址: https://github.com/xsankar/fdps-v3 常见的Dataset接口和函数 读写操作 这个例子读取的csv文件内容如下: 123456$ cat data/spark-csv/cars.csvyear,make,model,comment,blank2012,Tesla,S,No comment,1997,Ford,E350,Go get one now they are going fast,2015,Chevy,Volt,,2016,Volvo,XC90,Good Car !, 读取和保存 12345678910111213141516171819202122232425262728293031# -*- coding: utf-8 -*-from pyspark.sql import SparkSessionimport osspark = SparkSession.builder.appName("Datasets APIs Demo")\ .master("local")\ .config("spark.logConf", "true")\ .config("spark.logLevel", "ERROR")\ .getOrCreate()print(f"Running Spark Version &#123;spark.version&#125;")filePath = "."cars = spark.read.option("header", "true")\ .option("inferSchema", "true")\ .csv(os.path.join(filePath, "data/spark-csv/cars.csv"))print(f"Cars has &#123;cars.count()&#125; rows")cars.printSchema()# 保存为csv格式cars.write.mode("overwrite").option("header", "true").csv( os.path.join(filePath, "data/cars-out-csv.csv"))# 按年分区，保存为parquet格式cars.write.mode("overwrite").partitionBy("year").parquet( os.path.join(filePath, "data/cars-out-pqt"))spark.stop() mode参数有: overwrite: 如果文件存在，则覆写已经存在的文件 append: 追加写入到已经存在的文件 ignore: 会忽略Data Exists, 并且不会保存数据，使用该模式需要特别注意 error: 如果文件存在，抛出异常 查看保存结果 12345678910111213141516$ tree data/cars-out-csv.csv data/cars-out-pqtdata/cars-out-csv.csv├── _SUCCESS└── part-00000-aa27bc5f-aaeb-4f0c-804f-2d5320803d0d-c000.csvdata/cars-out-pqt├── _SUCCESS├── year=1997│ └── part-00000-160cc90f-7c7a-4d88-b1c8-16c4fd8ec831.c000.snappy.parquet├── year=2012│ └── part-00000-160cc90f-7c7a-4d88-b1c8-16c4fd8ec831.c000.snappy.parquet├── year=2015│ └── part-00000-160cc90f-7c7a-4d88-b1c8-16c4fd8ec831.c000.snappy.parquet└── year=2016 └── part-00000-160cc90f-7c7a-4d88-b1c8-16c4fd8ec831.c000.snappy.parquet4 directories, 7 files 从保存结果可以看出，保存为csv文件时，是一个目录，而不是单个的csv文件。 保存为parquet格式时，由于按照年来分区，因此生成了4个年份的子目录。这样做的好处是可以节约查询时间，比如当查询语句包含year=2015, 只会去查询year=2015这个目录的数据。在数据量大时，非常有助于提高查询效率。 读取保存的数据 12345678910111213141516171819# -*- coding: utf-8 -*-from pyspark.sql import SparkSessionimport osspark = SparkSession.builder.appName("Datasets APIs Demo")\ .master("local")\ .config("spark.logConf", "true")\ .config("spark.logLevel", "ERROR")\ .getOrCreate()print(f"Running Spark Version &#123;spark.version&#125;")filePath = "."cars = spark.read.parquet(os.path.join(filePath, "data/cars-out-pqt"))print(f"Cars has &#123;cars.count()&#125; rows")cars.printSchema()spark.stop() 聚合函数 首页让我们加载测试数据 12345678910111213141516171819202122232425262728293031323334353637&gt;&gt;&gt; import os&gt;&gt;&gt; filePath = "."&gt;&gt;&gt; cars = spark.read.option('header', 'true')\... .option('inferSchema', 'true')\... .csv(os.path.join(filePath, 'data/car-data/car-mileage.csv'))&gt;&gt;&gt; print(f"Cars has &#123;cars.count()&#125; rows")Cars has 32 rows&gt;&gt;&gt; cars.printSchema()root |-- mpg: double (nullable = true) |-- displacement: double (nullable = true) |-- hp: integer (nullable = true) |-- torque: integer (nullable = true) |-- CRatio: double (nullable = true) |-- RARatio: double (nullable = true) |-- CarbBarrells: integer (nullable = true) |-- NoOfSpeed: integer (nullable = true) |-- length: double (nullable = true) |-- width: double (nullable = true) |-- weight: integer (nullable = true) |-- automatic: integer (nullable = true)&gt;&gt;&gt; cars.show(5)+-----+------------+---+------+------+-------+------------+---------+------+-----+------+---------+| mpg|displacement| hp|torque|CRatio|RARatio|CarbBarrells|NoOfSpeed|length|width|weight|automatic|+-----+------------+---+------+------+-------+------------+---------+------+-----+------+---------+| 18.9| 350.0|165| 260| 8.0| 2.56| 4| 3| 200.3| 69.9| 3910| 1|| 17.0| 350.0|170| 275| 8.5| 2.56| 4| 3| 199.6| 72.9| 3860| 1|| 20.0| 250.0|105| 185| 8.25| 2.73| 1| 3| 196.7| 72.2| 3510| 1||18.25| 351.0|143| 255| 8.0| 3.0| 2| 3| 199.9| 74.0| 3890| 1||20.07| 225.0| 95| 170| 8.4| 2.76| 1| 3| 194.1| 71.8| 3365| 0|+-----+------------+---+------+------+-------+------------+---------+------+-----+------+---------+only showing top 5 rows descripe统计列 12345678910&gt;&gt;&gt; cars.describe("mpg","hp","weight","automatic").show()+-------+-----------------+-----------------+----------------+-------------------+|summary| mpg| hp| weight| automatic|+-------+-----------------+-----------------+----------------+-------------------+| count| 32| 32| 32| 32|| mean| 20.223125| 136.875| 3586.6875| 0.71875|| stddev|6.318289089312789|44.98082028541039|947.943187269323|0.45680340939917435|| min| 11.2| 70| 1905| 0|| max| 36.5| 223| 5430| 1|+-------+-----------------+-----------------+----------------+-------------------+ 分组统计 123456789101112131415&gt;&gt;&gt; cars.groupBy("automatic").avg("mpg","torque").show()+---------+------------------+-----------------+|automatic| avg(mpg)| avg(torque)|+---------+------------------+-----------------+| 1|17.324782608695646|257.3636363636364|| 0|27.630000000000006| 109.375|+---------+------------------+-----------------+# 不分组的情况&gt;&gt;&gt; cars.groupBy().avg("mpg","torque").show()+---------+-----------+| avg(mpg)|avg(torque)|+---------+-----------+|20.223125| 217.9|+---------+-----------+ 其它的聚合函数 12345678&gt;&gt;&gt; from pyspark.sql.functions import avg, mean&gt;&gt;&gt; cars.agg(avg(cars["mpg"]), mean(cars["torque"]) ).show()+---------+-----------+| avg(mpg)|avg(torque)|+---------+-----------+|20.223125| 217.9|+---------+-----------+ 统计函数 从前面的DataSets API概览图里可以看到，统计相关的函数都是在sql.stat.*下面。 下面看一个利用统计函数的例子 12345678910111213141516171819202122232425262728293031323334353637383940&gt;&gt;&gt; import os&gt;&gt;&gt; filePath = "."&gt;&gt;&gt; cars = spark.read.option('header', 'true')\... .option('inferSchema', 'true')\... .csv(os.path.join(filePath, 'data/car-data/car-mileage.csv'))&gt;&gt;&gt; cars.show(5)+-----+------------+---+------+------+-------+------------+---------+------+-----+------+---------+| mpg|displacement| hp|torque|CRatio|RARatio|CarbBarrells|NoOfSpeed|length|width|weight|automatic|+-----+------------+---+------+------+-------+------------+---------+------+-----+------+---------+| 18.9| 350.0|165| 260| 8.0| 2.56| 4| 3| 200.3| 69.9| 3910| 1|| 17.0| 350.0|170| 275| 8.5| 2.56| 4| 3| 199.6| 72.9| 3860| 1|| 20.0| 250.0|105| 185| 8.25| 2.73| 1| 3| 196.7| 72.2| 3510| 1||18.25| 351.0|143| 255| 8.0| 3.0| 2| 3| 199.9| 74.0| 3890| 1||20.07| 225.0| 95| 170| 8.4| 2.76| 1| 3| 194.1| 71.8| 3365| 0|+-----+------------+---+------+------+-------+------------+---------+------+-----+------+---------+only showing top 5 rows# 计算相关性&gt;&gt;&gt; cor = cars.stat.corr("hp", "weight")&gt;&gt;&gt; print("hp to weight: Correlation = %.4f" % cor)hp to weight: Correlation = 0.8834# 计算协方差&gt;&gt;&gt; cov = cars.stat.cov("hp", "weight")&gt;&gt;&gt; print("hp to weight: Covariance = %.4f" % cov)hp to weight: Covariance = 37667.5403# 交叉表显示&gt;&gt;&gt; cars.stat.crosstab("automatic", "NoOfSpeed").show()+-------------------+---+---+---+|automatic_NoOfSpeed| 3| 4| 5|+-------------------+---+---+---+| 1| 23| 0| 0|| 0| 1| 5| 3|+-------------------+---+---+---+ 交叉表的功能是分组统计数据。上面的交叉表类似下面的效果 1234567891011&gt;&gt;&gt; cars.createOrReplaceGlobalTempView("Cars")&gt;&gt;&gt; spark.sql("Select automatic, NoOfSpeed, count(*) from global_temp.Cars group by automatic, NoOfSpeed").show()+---------+---------+--------+|automatic|NoOfSpeed|count(1)|+---------+---------+--------+| 0| 5| 3|| 1| 3| 23|| 0| 3| 1|| 0| 4| 5|+---------+---------+--------+ 交叉表在数据统计中非常有用，有利于我们观察各组数据直接的关系。下面是利用交叉表查看泰坦尼克号中幸存者和他的属性关系。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&gt;&gt;&gt; import os&gt;&gt;&gt; filePath = "."&gt;&gt;&gt; passengers = spark.read.option("header", "true")\... .option("inferSchema", "true")\... .csv(os.path.join(filePath, 'data/titanic3_02.csv'))&gt;&gt;&gt; result = passengers.select(passengers['Pclass'], passengers['Survived'], passengers['Gender'], passengers['Age'],... passengers['SibSp'], passengers['Parch'],passengers['Fare'])...# Pclass: 票的等级# SibSp: 兄弟姐妹/配偶在船上# Parch: 父母/子女在船上# Fare: 票价&gt;&gt;&gt; result.show(5)+------+--------+------+------+-----+-----+--------+|Pclass|Survived|Gender| Age|SibSp|Parch| Fare|+------+--------+------+------+-----+-----+--------+| 1| 1|female| 29.0| 0| 0|211.3375|| 1| 1| male|0.9167| 1| 2| 151.55|| 1| 0|female| 2.0| 1| 2| 151.55|| 1| 0| male| 30.0| 1| 2| 151.55|| 1| 0|female| 25.0| 1| 2| 151.55|+------+--------+------+------+-----+-----+--------+only showing top 5 rows&gt;&gt;&gt; result.printSchema()root |-- Pclass: integer (nullable = true) |-- Survived: integer (nullable = true) |-- Gender: string (nullable = true) |-- Age: double (nullable = true) |-- SibSp: integer (nullable = true) |-- Parch: integer (nullable = true) |-- Fare: double (nullable = true)&gt;&gt;&gt; result.groupBy('Gender').count().show()+------+-----+|Gender|count|+------+-----+|female| 466|| male| 843|+------+-----+&gt;&gt;&gt; result.stat.crosstab("Survived", "Gender").show()+---------------+------+----+|Survived_Gender|female|male|+---------------+------+----+| 1| 339| 161|| 0| 127| 682|+---------------+------+----+&gt;&gt;&gt; result.stat.crosstab("Survived","SibSp").show()+--------------+---+---+---+---+---+---+---+|Survived_SibSp| 0| 1| 2| 3| 4| 5| 8|+--------------+---+---+---+---+---+---+---+| 1|309|163| 19| 6| 3| 0| 0|| 0|582|156| 23| 14| 19| 6| 9|+--------------+---+---+---+---+---+---+---+# 因为Age是浮点型，直接用Age创建的交叉表的结果非常长，不便于观察。所以将它转换为整型&gt;&gt;&gt; ageDist = result.select(result["Survived"], (result["age"] - result["age"] % 10).cast("int").name("AgeBracket"))&gt;&gt;&gt; ageDist.show(5)+--------+----------+|Survived|AgeBracket|+--------+----------+| 1| 20|| 1| 0|| 0| 0|| 0| 30|| 0| 20|+--------+----------+only showing top 5 rows# 20s和30s之间的人最多，还有很多不知道年龄的人&gt;&gt;&gt; ageDist.crosstab("Survived", "AgeBracket").show()+-------------------+---+---+---+---+---+---+---+---+---+----+|Survived_AgeBracket| 0| 10| 20| 30| 40| 50| 60| 70| 80|null|+-------------------+---+---+---+---+---+---+---+---+---+----+| 1| 50| 56|127| 98| 52| 32| 10| 1| 1| 73|| 0| 32| 87|217|134| 83| 38| 22| 6| 0| 190|+-------------------+---+---+---+---+---+---+---+---+---+----+ 科学计算函数 科学统计函数基本都在sql.functions下面。 如: log(), log10(), sqrt(), cbrt(), exp(), pow(), sin(), cos(), tan(), acos(), asin(), atan(), toDegrees(), toRadians() 使用示例如下: 创建一个DataFrame 1234567891011121314151617181920212223&gt;&gt;&gt; from pyspark.sql import Row&gt;&gt;&gt; row = Row("val")&gt;&gt;&gt; l = [1,2,3]&gt;&gt;&gt; rdd = sc.parallelize(l)# 有两种方式将rdd转换为dataframe# 方法一:&gt;&gt;&gt; df = spark.createDataFrame(rdd.map(row))# 方法二# &gt;&gt;&gt; df = rdd.map(row).toDF()&gt;&gt;&gt; df.show()+---+|val|+---+| 1|| 2|| 3|+---+ 一些示例 12345678910111213141516171819202122232425262728293031323334353637&gt;&gt;&gt; from pyspark.sql.functions import log, log10, sart，log1p&gt;&gt;&gt; df.select(df['val'], log(df['val']).name('ln')).show()+---+------------------+|val| ln|+---+------------------+| 1| 0.0|| 2|0.6931471805599453|| 3|1.0986122886681098|+---+------------------+&gt;&gt;&gt; df.select(df['val'], log10(df['val']).name('log10')).show()+---+-------------------+|val| log10|+---+-------------------+| 1| 0.0|| 2| 0.3010299956639812|| 3|0.47712125471966244|+---+-------------------+&gt;&gt;&gt; df.select(df['val'], sqrt(df['val']).name('sqrt')).show()+---+------------------+|val| sqrt|+---+------------------+| 1| 1.0|| 2|1.4142135623730951|| 3|1.7320508075688772|+---+------------------+&gt;&gt;&gt; df.select(df['val'], log1p(df['val']).name('ln1p')).show()+---+------------------+|val| ln1p|+---+------------------+| 1|0.6931471805599453|| 2|1.0986122886681096|| 3|1.3862943611198906|+---+------------------+ 对于给定的直角三角形的两个直角边，求其斜边的长度 123456789101112131415161718192021222324252627282930313233343536&gt;&gt;&gt; import os&gt;&gt;&gt; filePath = '.'&gt;&gt;&gt; data = spark.read.option("header", "true")\... .option("inferScheam", "true")\... .csv(os.path.join(filePath, "data/hypot.csv"))&gt;&gt;&gt;&gt;&gt;&gt; data.show()+---+---+| X| Y|+---+---+| 3| 4|| 5| 12|| 7| 24|| 9| 40|| 11| 60|| 13| 84|+---+---+&gt;&gt;&gt; from pyspark.sql.functions import hypot&gt;&gt;&gt; data.select(data["X"], data["Y"],... hypot(data["X"], data["Y"]).name("hypot")).show()+---+---+-----+| X| Y|hypot|+---+---+-----+| 3| 4| 5.0|| 5| 12| 13.0|| 7| 24| 25.0|| 9| 40| 41.0|| 11| 60| 61.0|| 13| 84| 85.0|+---+---+-----+ 实战 下面让我们通过一个实战，使用前面学到的接口和函数。 我们使用Northwind Sales的数据集来分析下面的问题: How many orders were placed by each customer? How many orders were placed in each country? How many orders were placed for each month/year? What is the total number of sales for each customer, year-wise? What * is the average order by customer, year-wise? 读取数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&gt;&gt;&gt; import os&gt;&gt;&gt; filePath = '.'&gt;&gt;&gt; orders = spark.read.option("header", "true")\... .option("inferSchema", "true")\... .csv(os.path.join(filePath, "data/NW/NW-Orders-01.csv")... )&gt;&gt;&gt; print(f"Orders has &#123;orders.count()&#125; rows")Orders has 830 rows&gt;&gt;&gt; orders.show(5)+-------+----------+----------+-------------------+-----------+|OrderID|CustomerID|EmployeeID| OrderDate|ShipCountry|+-------+----------+----------+-------------------+-----------+| 10248| VINET| 5|1996-07-02 00:00:00| France|| 10249| TOMSP| 6|1996-07-03 00:00:00| Germany|| 10250| HANAR| 4|1996-07-06 00:00:00| Brazil|| 10251| VICTE| 3|1996-07-06 00:00:00| France|| 10252| SUPRD| 4|1996-07-07 00:00:00| Belgium|+-------+----------+----------+-------------------+-----------+only showing top 5 rows&gt;&gt;&gt; orders.printSchema()root |-- OrderID: integer (nullable = true) |-- CustomerID: string (nullable = true) |-- EmployeeID: integer (nullable = true) |-- OrderDate: timestamp (nullable = true) |-- ShipCountry: string (nullable = true)&gt;&gt;&gt; orderDetails = spark.read.option("header", "true")\... .option("inferSchema", "true")\... .csv(os.path.join(filePath, "data/NW/NW-Order-Details.csv"))&gt;&gt;&gt; print(f"Order Details has &#123;orderDetails.count()&#125; rows")Order Details has 2155 rows&gt;&gt;&gt; orderDetails.show(5)+-------+---------+---------+---+--------+|OrderID|ProductId|UnitPrice|Qty|Discount|+-------+---------+---------+---+--------+| 10248| 11| 14.0| 12| 0.0|| 10248| 42| 9.8| 10| 0.0|| 10248| 72| 34.8| 5| 0.0|| 10249| 14| 18.6| 9| 0.0|| 10249| 51| 42.4| 40| 0.0|+-------+---------+---------+---+--------+only showing top 5 rows&gt;&gt;&gt; orderDetails.printSchema()root |-- OrderID: integer (nullable = true) |-- ProductId: integer (nullable = true) |-- UnitPrice: double (nullable = true) |-- Qty: integer (nullable = true) |-- Discount: double (nullable = true) 解答问题1: How many orders were placed by each customer? 1234567891011121314&gt;&gt;&gt; orderByCustomer = orders.groupBy("CustomerID").count()&gt;&gt;&gt; from pyspark.sql.functions import desc&gt;&gt;&gt; orderByCustomer.sort(desc("count")).show(5)+----------+-----+|CustomerID|count|+----------+-----+| SAVEA| 31|| ERNSH| 30|| QUICK| 28|| HUNGO| 19|| FOLKO| 19|+----------+-----+ 解答问题2: How many orders were placed in each country? 12345678910111213&gt;&gt;&gt; orderByCountry = orders.groupBy("ShipCountry").count()&gt;&gt;&gt; orderByCountry.sort(desc("count")).show(5)+-----------+-----+|ShipCountry|count|+-----------+-----+| Germany| 122|| USA| 122|| Brazil| 82|| France| 77|| UK| 56|+-----------+-----+only showing top 5 rows 后面三个问题可以如下解答 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# -*- coding: utf-8 -*-from pyspark.sql import SparkSessionfrom pyspark.sql.functions import to_date, month, yearimport osspark = SparkSession.builder.appName("Datasets APIs Demo") \ .master("local") \ .config("spark.logConf", "true") \ .config("spark.logLevel", "ERROR") \ .getOrCreate()print(f"Running Spark Version &#123;spark.version&#125;")filePath = "."orders = spark.read.option("header", "true") \ .option("inferSchema", "true") \ .csv(os.path.join(filePath, "data/NW/NW-Orders-01.csv"))orderDetails = spark.read.option("header", "true") \ .option("inferSchema", "true") \ .csv(os.path.join(filePath, "data/NW/NW-Order-Details.csv"))result = orderDetails.select(orderDetails['OrderID'], ((orderDetails['UnitPrice'] * orderDetails['Qty'] - ( orderDetails['UnitPrice'] * orderDetails['Qty'] * orderDetails['Discount'])). name('OrderPrice')))result.show(5)# 设置DataFrame orderTot的别名为OrderTotalorderTot = result.groupBy('OrderID').sum('OrderPrice').alias('OrderTotal')orderTot.sort("OrderID").show(5)orders_df = orders.join(orderTot, orders["OrderID"] == orderTot["OrderID"], "inner") \ .select(orders["OrderID"], orders["CustomerID"], orders["OrderDate"], orders["ShipCountry"].alias("ShipCountry"), orderTot["sum(OrderPrice)"].alias("Total"))orders_df.sort("CustomerID").show()# 过滤出空行orders_df.filter(orders_df["Total"].isNull()).show()# 添加Date列orders_df2 = orders_df.withColumn('Date', to_date(orders_df['OrderDate']))orders_df2.show(5)orders_df2.printSchema()# 添加Month和Year列orders_df3 = orders_df2.withColumn("Month", month(orders_df2["OrderDate"])) \ .withColumn("Year", year(orders_df2["OrderDate"]))orders_df3.show(5)orders_df3.printSchema()# 问题3ordersByYM = orders_df3.groupBy("Year", "Month").sum("Total").alias("Total")ordersByYM.sort(ordersByYM["Year"], ordersByYM["Month"]).show()# 问题4ordersByCY = orders_df3.groupBy("CustomerID", "Year").sum("Total").alias("Total")ordersByCY.sort(ordersByCY["CustomerID"], ordersByCY["Year"]).show()# 问题5ordersCA = orders_df3.groupBy("CustomerID").avg("Total").alias("Total")ordersCA.sort("avg(Total)", ascending=False).show()spark.stop()]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Fast Data Processing with Spark 2（Third Edition)》读书笔记二]]></title>
    <url>%2F2018%2F01%2F15%2Ffast-data-processing-with-spark2-note-two%2F</url>
    <content type="text"><![CDATA[本书其它笔记《Fast Data Processing with Spark 2（Third Edition)》读书笔记目录 在Spark中加载和保存数据 在我们开始操作数据之前，让我们先看一些Spark的概念以及了解一下不同的数据形态 Spark抽象概念 Saprk的主要特点就是分布式的数据描述(representation)和计算，因此拥有大规模的数据操作。Spark主要的数据描述单元就是RDD(原句是: Spark’s primary unit for representation of data is RDD)， 可以很方便的允许并行的数据计算。在Saprk 2.0.0版本之前，都是基于RDDs工作的。然而，它们都是低级别的原始结构，在执行和扩展上有很大的优化空间。因此才有了Datasets/DataFrames。Datasets/DataFrames是API级别的抽象，也是编程的主要接口，它提供了大量操作RDD, 但是通过优化查询计划在RDDs上封装了一层。因此，底层仍然是RDD, 只是通过Datasets/DataFrames的API来访问。 RDDs can be viewed as arrays of arrays with primitive data types, such as integers, floats, and strings. Datasets/DataFrames, on the other hand, are similar to a table or a spreadsheet with column headings-such as name, title, order number, order date, and movie rating-and the associated data types. RDDs可以看做是一系列原始数据数组的集合，比如: 整型，浮点型和字符串。Datasets/DataFrames在另一方面来说，有点类似一张表单或表格，有许多列标题（比如姓名，标题，订单号，订单日期，电影评分）以及关联的数据类型。 无论在什么情况下。当使用Datasets/DataFrames,通过用SparkSession操作。然而，当需要进行一些低级别的操作来实复杂的计算或累加时，就是用RDDs,通过SparkContext来操作。 使用dataset.rdd()和SparkSession.createDataset(rdd)/SparkSession.createDataFr ame(rdd)方法，可以将RDDs转变为Datasets/DataFrames。 RDDs 我们可以使用任何Hadoop支持的数据源来创建RDDs。Scala, Java, Python语言的原始数据结构都可以作为RDD的基础。使用原始的数据集合创建RDDs在测试时非常有用。 由于RDD是懒加载，它只有在它被需要时才会去计算。也就是说：当你尝试从RDD获取数据时，可能会失败。RDD里的数据只有在被缓存引用或输出时才会通过计算创建，这也意味着你可以构造大量的计算而不用担心阻塞计算线程。甚至，只有当你在具体化RDD（materialize the RDD）时，代码才会去加载原始数据。 Tips: 每次你具体化RDD时，它都会计算一次。因此，当频繁的使用时，可以借助缓存机制来提高效率。 数据形态 从数据形态的角度来说，所有的数据可以分成三类 结构化的数据（Structured data），通常存储在数据库里，如Oracle, HBase, Cassandra等，关系型数据库是最常见的结构化数据的存储方式。结构化的数据样式，数据类型和数据大小都是已知的。 半结构化的数据（Semi-structured data）， 正如它的名字一样，它也是有结构的数据，只是数据样式，数据类型和数据大小是可变。常见的半结构化数据格式有:csv, json和parquet 无结构的数据（Unstructured data），目前我们遇到的85%的数据都是这种格式。如图形，音频文件，社交文化等。大部分的数据处理，都是从无结构的数据开始，通过ETL(Extract-Transform-Load)，变型和其它技术，最终变成结构化的数据。 数据形态和Datasets/DataFrames/RDDs 现在让我们结合数据形态以及spark的抽象概念来看看如何使用spark读写数据。 在Spark 2.0.0之前，我们只需要使用RDDs和map()函数来按照需要改变数据。但是，当使用Dataset/DataFrame, 变得稍微复杂一些， 我们可以直接读入类似一张表的数据，包含数据类型和列信息，也可以更高效的工作。 通常来说: 使用SparkContext和RDDs来处非无结构的数据 使用SparkSession和Datasets/DataFrames来处理结构化和半结构化的数据。SparkSession有统一的标准处理各种格式的数据，如: .csv, .json, .parquet, .jdbc, .orc。还有一个可插拔的结构叫做DataSource API, 可以处理任意类型的结构化数据 加载数据到RDD 创建RDD最简单的方式就是使用编程语言(Scala, Python, Java)原始的数据结构， SparkContext对象有个方法parallelize可以实现这个功能。 1234&gt;&gt;&gt; rdd = sc.parallelize([1,2,3])&gt;&gt;&gt; rdd.take(3)[1, 2, 3] 加载外部数据最简单的方式就是读取一个文本文件。如果是单机模式的话，这个很简单，主要确保文件在本机即可。但是在集群环境下时需要注意，文件需要存在集群上的每个节点上，在分布式环境下，我们可以使用addFile方法，让Spark把文件拷贝到集群的每个机器上，如: 12345678910&gt;&gt;&gt; from pyspark.files import SparkFiles&gt;&gt;&gt; sc.addd("data/spam.data")&gt;&gt;&gt; sc.addFile("data/spam.data")&gt;&gt;&gt; in_file = sc.textFile(SparkFiles.get("spam.data"))&gt;&gt;&gt; in_file.take(1)[u'0 0.64 0.64 0 0.32 0 0 0 0 0 0 0.64 0 0 0 0.32 0 1.29 1.93 0 0.96 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.778 0 0 3.756 61 278 1'] 通常，我们输入的文件都是CSV或TSV文件。一般在创建RDDs时，都需要先解析数据。 通常有两种做法: 自己写函数读取和解析CSV文件 使用第三库，比如opencsv 我们看看自己写函数的方式 1234567891011121314151617181920212223&gt;&gt;&gt; !cat data/Line_of_numbers.csv42,42,55,61,53,49,43,47,49,60,68,54,34,35,35,39&gt;&gt;&gt; inp_file = sc.textFile(&quot;data/Line_of_numbers.csv&quot;)&gt;&gt;&gt; numbers_rdd = inp_file.map(lambda line: line.split(&apos;,&apos;))&gt;&gt;&gt; numbers_rdd.take(10)[[u&apos;42&apos;, u&apos;42&apos;, u&apos;55&apos;, u&apos;61&apos;, u&apos;53&apos;, u&apos;49&apos;, u&apos;43&apos;, u&apos;47&apos;, u&apos;49&apos;, u&apos;60&apos;, u&apos;68&apos;, u&apos;54&apos;, u&apos;34&apos;, u&apos;35&apos;, u&apos;35&apos;, u&apos;39&apos;]] 得到的数据都是字符串类型，我们希望把它转换为整型或浮点型 1234567891011&gt;&gt;&gt; numbers_rdd = inp_file.flatMap(lambda line:... line.split(&apos;,&apos;)).map(lambda x:float(x))...&gt;&gt;&gt; numbers_rdd.take(10)[42.0, 42.0, 55.0, 61.0, 53.0, 49.0, 43.0, 47.0, 49.0, 60.0]&gt;&gt;&gt; numbers_sum = numbers_rdd.sum()&gt;&gt;&gt; numbers_sum766.0 注意上面解析文件时先使用的是flatMap，再使用的map。flatMap会将结果变成数组 1234567891011121314151617181920&gt;&gt;&gt; inp_file.map(lambda line: line.split(&apos;,&apos;)).take(10)[[u&apos;42&apos;, u&apos;42&apos;, u&apos;55&apos;, u&apos;61&apos;, u&apos;53&apos;, u&apos;49&apos;, u&apos;43&apos;, u&apos;47&apos;, u&apos;49&apos;, u&apos;60&apos;, u&apos;68&apos;, u&apos;54&apos;, u&apos;34&apos;, u&apos;35&apos;, u&apos;35&apos;, u&apos;39&apos;]]&gt;&gt;&gt; inp_file.flatMap(lambda line: line.split(&apos;,&apos;)).take(10)[u&apos;42&apos;, u&apos;42&apos;, u&apos;55&apos;, u&apos;61&apos;, u&apos;53&apos;, u&apos;49&apos;, u&apos;43&apos;, u&apos;47&apos;, u&apos;49&apos;, u&apos;60&apos;] 从Spark获取数据还可以使用collect()方法，用它可以获得原始的数据形式，跟parallelize()作用相反，parallelize()解析数据把它转换成RDD，collect()获取执行结果。 1234567891011121314151617&gt;&gt;&gt; inp_file.flatMap(lambda line: line.split(',')).collect()[u'42', u'42', u'55', u'61', u'53', u'49', u'43', u'47', u'49', u'60', u'68', u'54', u'34', u'35', u'35', u'39'] 保存RDD数据 使用saveAsTextFile方法可以保存RDD数据 1234567891011&gt;&gt;&gt; !cat data/Line_of_numbers.csv42,42,55,61,53,49,43,47,49,60,68,54,34,35,35,39&gt;&gt;&gt; inp_file = sc.textFile("data/Line_of_numbers.csv")&gt;&gt;&gt; inp_file.saveAsTextFile("out.txt")&gt;&gt;&gt; !file out.txt/*out.txt/_SUCCESS: emptyout.txt/part-00000: ASCII textout.txt/part-00001: empty 可以看到是以目录的形式保存的数据 读取保存的数据 1234&gt;&gt;&gt; read_file = sc.textFile("out.txt")&gt;&gt;&gt; read_file.take(10)[u'42,42,55,61,53,49,43,47,49,60,68,54,34,35,35,39'] Spark2.0 概念 下图是一个数据掮客的架构: Stack Architecture Data Hub Analytics Hub Reporting Hub Visualization ETL Data Hub 数据中心 存储所有数据，数据来自于ETL服务，Kafaka等。数据可以是多种主题: 如市场数据，交易数据，文本日志，社交数据，非结构化的数据。也可以是一些跟时间序列相关的数据。 Reporting Hub 报告中心 通常包含结构化和聚合之后的数据，用于日常报表和可视化面板。spark在这里主要做ETL和变形。数据可视化工具,比如: Tableau Quilk Pentaho 可以直接通过SparkSQL读取Spark里的数据。 Analytics Hub 分析中心 是数据分析师花费大部分时间的地方。分析中心可以从数据中心获取大量数据， 然后生成intermediate Datasets，特征提取，model Datasets。 DataFrames, MLlib, GraphX, and ML pipelines 也在这里生成。 Spark Full Stack Spark Full Stack 大数据存储Parquet Spark SQL Spark SQL Architeture 下面的例子使用的数据文件下载地址: https://github.com/xsankar/fdps-v3 单表查询 读取csv文件, 返回的employees是一个pyspark.sql.dataframe.DataFrame类型 1234&gt;&gt;&gt; print('Running Spark version: %s' % spark.version)Running Spark version: 2.2.1&gt;&gt;&gt; employees = spark.read.option('header', 'true').csv('data/NW-Employees.csv') 统计总行数 12&gt;&gt;&gt; print('Employees has %d rows' % employees.count())Employees has 9 rows 打印前5行 1234567891011&gt;&gt;&gt; employees.show(5)+----------+---------+---------+--------------------+---------+--------+--------+-----+-------+-------+---------+|EmployeeID| LastName|FirstName| Title|BirthDate|HireDate| City|State| Zip|Country|ReportsTo|+----------+---------+---------+--------------------+---------+--------+--------+-----+-------+-------+---------+| 1| Fuller| Andrew|Sales Representative| 12/6/48| 4/29/92| Seattle| WA| 98122| USA| 2|| 2| Davolio| Nancy|Vice President, S...| 2/17/52| 8/12/92| Tacoma| WA| 98401| USA| 0|| 3|Leverling| Janet|Sales Representative| 8/28/63| 3/30/92|Kirkland| WA| 98033| USA| 2|| 4| Peacock| Margaret|Sales Representative| 9/17/37| 5/1/93| Redmond| WA| 98052| USA| 2|| 5|Dodsworth| Anne| Sales Manager| 3/2/55|10/15/93| London| null|SW1 8JR| UK| 2|+----------+---------+---------+--------------------+---------+--------+--------+-----+-------+-------+---------+only showing top 5 rows 返回前3行 1234&gt;&gt;&gt; employees.head(3)[Row(EmployeeID='1', LastName='Fuller', FirstName='Andrew', Title='Sales Representative', BirthDate='12/6/48', HireDate='4/29/92', City='Seattle', State='WA', Zip='98122', Country='USA', ReportsTo='2'), Row(EmployeeID='2', LastName='Davolio', FirstName='Nancy', Title='Vice President, Sales', BirthDate='2/17/52', HireDate='8/12/92', City='Tacoma', State='WA', Zip='98401', Country='USA', ReportsTo='0'), Row(EmployeeID='3', LastName='Leverling', FirstName='Janet', Title='Sales Representative', BirthDate='8/28/63', HireDate='3/30/92', City='Kirkland', State='WA', Zip='98033', Country='USA', ReportsTo='2')] 查看每一列的数据类型 1234567891011121314&gt;&gt;&gt; for column_name, data_type in employees.dtypes:... print(f'&#123;column_name&#125;: &#123;data_type&#125;')...EmployeeID: stringLastName: stringFirstName: stringTitle: stringBirthDate: stringHireDate: stringCity: stringState: stringZip: stringCountry: stringReportsTo: string 查看schema 12345678910111213&gt;&gt;&gt; employees.printSchema()root |-- EmployeeID: string (nullable = true) |-- LastName: string (nullable = true) |-- FirstName: string (nullable = true) |-- Title: string (nullable = true) |-- BirthDate: string (nullable = true) |-- HireDate: string (nullable = true) |-- City: string (nullable = true) |-- State: string (nullable = true) |-- Zip: string (nullable = true) |-- Country: string (nullable = true) |-- ReportsTo: string (nullable = true) 默认读取的数据类型都是String,可以设置inferSchema为true来根据数据自动推测类型。(如下: EmployeeID和EmployeeID自动推测为int) 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; employees = spark.read.option('header', 'true').option('inferSchema', 'true').csv('data/NW-Employees.csv')&gt;&gt;&gt; for column_name, data_type in employees.dtypes:... print(f'&#123;column_name&#125;: &#123;data_type&#125;')...EmployeeID: intLastName: stringFirstName: stringTitle: stringBirthDate: stringHireDate: stringCity: stringState: stringZip: stringCountry: stringReportsTo: int&gt;&gt;&gt; employees.printSchema()root |-- EmployeeID: integer (nullable = true) |-- LastName: string (nullable = true) |-- FirstName: string (nullable = true) |-- Title: string (nullable = true) |-- BirthDate: string (nullable = true) |-- HireDate: string (nullable = true) |-- City: string (nullable = true) |-- State: string (nullable = true) |-- Zip: string (nullable = true) |-- Country: string (nullable = true) |-- ReportsTo: integer (nullable = true) 创建全局临时视图 1&gt;&gt;&gt; employees.createOrReplaceGlobalTempView("EmployeesTable") 查询视图(表名前面的global_temp不能省略，否则会报Table or view not found) 12345678910111213&gt;&gt;&gt; result = spark.sql('select * from global_temp.EmployeesTable')&gt;&gt;&gt; result.show(5)+----------+---------+---------+--------------------+---------+--------+--------+-----+-------+-------+---------+|EmployeeID| LastName|FirstName| Title|BirthDate|HireDate| City|State| Zip|Country|ReportsTo|+----------+---------+---------+--------------------+---------+--------+--------+-----+-------+-------+---------+| 1| Fuller| Andrew|Sales Representative| 12/6/48| 4/29/92| Seattle| WA| 98122| USA| 2|| 2| Davolio| Nancy|Vice President, S...| 2/17/52| 8/12/92| Tacoma| WA| 98401| USA| 0|| 3|Leverling| Janet|Sales Representative| 8/28/63| 3/30/92|Kirkland| WA| 98033| USA| 2|| 4| Peacock| Margaret|Sales Representative| 9/17/37| 5/1/93| Redmond| WA| 98052| USA| 2|| 5|Dodsworth| Anne| Sales Manager| 3/2/55|10/15/93| London| null|SW1 8JR| UK| 2|+----------+---------+---------+--------------------+---------+--------+--------+-----+-------+-------+---------+only showing top 5 rows 查看分析结果 12345678910111213&gt;&gt;&gt; employees.explain(True)== Parsed Logical Plan ==Relation[EmployeeID#12,LastName#13,FirstName#14,Title#15,BirthDate#16,HireDate#17,City#18,State#19,Zip#20,Country#21,ReportsTo#22] csv== Analyzed Logical Plan ==EmployeeID: string, LastName: string, FirstName: string, Title: string, BirthDate: string, HireDate: string, City: string, State: string, Zip: string, Country: string, ReportsTo: stringRelation[EmployeeID#12,LastName#13,FirstName#14,Title#15,BirthDate#16,HireDate#17,City#18,State#19,Zip#20,Country#21,ReportsTo#22] csv== Optimized Logical Plan ==Relation[EmployeeID#12,LastName#13,FirstName#14,Title#15,BirthDate#16,HireDate#17,City#18,State#19,Zip#20,Country#21,ReportsTo#22] csv== Physical Plan ==*FileScan csv [EmployeeID#12,LastName#13,FirstName#14,Title#15,BirthDate#16,HireDate#17,City#18,State#19,Zip#20,Country#21,ReportsTo#22] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/linliang/Src/github/fdps-v3/data/NW-Employees.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;EmployeeID:string,LastName:string,FirstName:string,Title:string,BirthDate:string,HireDate:... 过滤查询 123456789101112&gt;&gt;&gt; result = spark.sql(&apos;select * from global_temp.EmployeesTable where State=&quot;WA&quot;&apos;)&gt;&gt;&gt; result.show(5)+----------+---------+---------+--------------------+---------+--------+--------+-----+-----+-------+---------+|EmployeeID| LastName|FirstName| Title|BirthDate|HireDate| City|State| Zip|Country|ReportsTo|+----------+---------+---------+--------------------+---------+--------+--------+-----+-----+-------+---------+| 1| Fuller| Andrew|Sales Representative| 12/6/48| 4/29/92| Seattle| WA|98122| USA| 2|| 2| Davolio| Nancy|Vice President, S...| 2/17/52| 8/12/92| Tacoma| WA|98401| USA| 0|| 3|Leverling| Janet|Sales Representative| 8/28/63| 3/30/92|Kirkland| WA|98033| USA| 2|| 4| Peacock| Margaret|Sales Representative| 9/17/37| 5/1/93| Redmond| WA|98052| USA| 2|| 8| Callahan| Laura|Inside Sales Coor...| 1/7/58| 3/3/94| Seattle| WA|98105| USA| 2|+----------+---------+---------+--------------------+---------+--------+--------+-----+-----+-------+---------+ 综合上面的所有语句到单个文件 single_table.py 123456789101112131415161718192021222324252627282930313233343536373839404142# -*- coding: utf-8 -*-from pyspark.sql import SparkSessionimport osspark = SparkSession.builder.appName('SQL Demo').getOrCreate()print('Running Spark version: %s' % spark.version)filePath = '.'# 读取csv文件employees = spark.read.option('header', 'true').csv(os.path.join(filePath, 'data/NW-Employees.csv'))# 统计总行数print('Employees has %d rows' % employees.count())# 打印前5行employees.show(5)# 返回前3行employees.head(3)# 查看每一列的数据类型，默认读取的数据都是Stringfor column_name, data_type in employees.dtypes: print(f'&#123;column_name&#125;: &#123;data_type&#125;')employees.printSchema()# 创建全局的临时视图employees.createOrReplaceGlobalTempView("EmployeesTable")# 查询视图result = spark.sql('select * from global_temp.EmployeesTable')result.show(5)# 查看分析结果employees.explain(True)# 添加过滤条件result = spark.sql('select * from global_temp.EmployeesTable where State="WA"')result.show(5)spark.stop() 多表查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475&gt;&gt;&gt; orders = spark.read.option('header', 'true').option("inferSchema", "true").csv('data/NW-Orders.csv')&gt;&gt;&gt; order_details = spark.read.option('header', 'true').option("inferSchema", "true").csv('data/NW-Order-Details.csv')&gt;&gt;&gt; orders.createOrReplaceTempView("OrdersTable")&gt;&gt;&gt; order_details.createOrReplaceTempView("OrderDetailsTable")&gt;&gt;&gt; result = spark.sql('''... SELECT OrderDetailsTable.OrderID, ShipCountry,... UnitPrice, Qty, Discount FROM OrdersTable INNER JOIN OrderDetailsTable ON... OrdersTable.OrderID = OrderDetailsTable.OrderID... ''')&gt;&gt;&gt; result.show(10)+-------+-----------+---------+---+--------+|OrderID|ShipCountry|UnitPrice|Qty|Discount|+-------+-----------+---------+---+--------+| 10248| France| 34.8| 5| 0.0|| 10248| France| 9.8| 10| 0.0|| 10248| France| 14.0| 12| 0.0|| 10249| Germany| 42.4| 40| 0.0|| 10249| Germany| 18.6| 9| 0.0|| 10250| Brazil| 16.8| 15| 0.15|| 10250| Brazil| 42.4| 35| 0.15|| 10250| Brazil| 7.7| 10| 0.0|| 10251| France| 16.8| 20| 0.0|| 10251| France| 15.6| 15| 0.05|+-------+-----------+---------+---+--------+only showing top 10 rows&gt;&gt;&gt; result = spark.sql('''... SELECT ShipCountry, SUM(OrderDetailsTable.UnitPrice *... Qty * Discount) AS ProductSales FROM OrdersTable INNER JOIN... OrderDetailsTable ON OrdersTable.OrderID = OrderDetailsTable.OrderID GROUP... BY ShipCountry... ''')&gt;&gt;&gt;&gt;&gt;&gt; result.show(10)+-----------+------------------+|ShipCountry| ProductSales|+-----------+------------------+| Sweden|5028.5599999999995|| Germany| 14355.9965|| France| 4140.437499999999|| Argentina| 0.0|| Belgium|1310.1250000000002|| Finland| 968.3975|| Italy| 934.995|| Norway| 0.0|| Spain| 1448.69|| Denmark| 2121.2275|+-----------+------------------+only showing top 10 rows# 排序&gt;&gt;&gt; result.orderBy("ProductSales", ascending=False).show(10)+-----------+------------------+|ShipCountry| ProductSales|+-----------+------------------+| USA|17982.369499999997|| Germany| 14355.9965|| Austria|11492.791500000001|| Brazil| 8029.7585|| Ireland| 7337.485|| Canada|5137.8099999999995|| Sweden|5028.5599999999995|| France| 4140.437499999999|| Venezuela| 4004.261|| Denmark| 2121.2275|+-----------+------------------+only showing top 10 rows]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Fast Data Processing with Spark 2（Third Edition)》读书笔记一]]></title>
    <url>%2F2018%2F01%2F11%2Ffast-data-processing-with-spark2-note-one%2F</url>
    <content type="text"><![CDATA[本书其它笔记《Fast Data Processing with Spark 2（Third Edition)》读书笔记目录 Spark概览 Apache Spark is a fast and general-purpose cluster computing system 本文基于目前最新的spark版本2.2.1整理，以Python3.6为例，在Mac单机情况下使用。 安装 部署环境需求 Window, Linux, Mac均可 Java8+(需要设置JAVA_HOME环境变量) 下面三个根据自己习惯使用的语言选择安装 Python2.7+/3.4+ R 3.1+ 为了使用Scala API, 需要安装完整的Scala 2.11.x版本 解压安装 从Spark官网选择合适的版本下载。 123$ wget https://www.apache.org/dyn/closer.lua/spark/spark-2.2.1/spark-2.2.1-bin-hadoop2.7.tgz$ tar -zxvf spark-2.2.1-bin-hadoop2.7.tgz 解压后即可使用, 不过在使用之前，让我们先配置几个常用的环境变量: 1234567891011# SPARK_HOME根据本机实际情况修改路径export SPARK_HOME="/path/to/spark-2.2.1-bin-hadoop2.7"# 当用pyspark时默认使用ipytonexport PYSPARK_DRIVER_PYTHON="ipython"# 如果习惯使用Jupyter, 可以添加下面的配置，那么在用pyspark时，默认会调用Jupyter notebook，否则为默认的ipython consoleexport PYSPARK_DRIVER_PYTHON_OPTS="notebook"# 添加到PATH，方便随时调用pyspark命令export PATH="$SPARK_HOME/bin:$PATH" 关于Spark集群的搭建，练习阶段暂时不会使用到，等以后需要再补充。 快速开始 运行例子和交互式shell Spark内置了一些Scala, Java列子，同时也有一些Python和R语言的例子在example/src/main目录里。 为了运行Scala, Java 例子，在$SPARK_HOME目录下, 用bin/run-example &lt;class&gt; [params]形式运行。如 1./bin/run-example SparkPi 10 也可以启动一个Scala语言的交互式的Shell 1./bin/spark-shell --master local[4] --master选项设置在本地以两个线程运行。 local[N]中的N一般设置为本机的CPU核数。更多关于--master的选项可以参考Master URLs 启动Python语言的交互式Shell 1./bin/pyspark --master local[4] 运行Python例子 1./bin/spark-submit examples/src/main/python/pi.py 10 启动R语言的交互式Shell 1./bin/sparkR --master local[4] R语言的例子 1./bin/spark-submit examples/src/main/r/dataframe.R 使用Spark shell做交互式分析 进入到$SPARK_HOME目录下, 启动pyspark 1$ ./bin/pyspark 12345678910111213141516171819202122232425262728293031323334353637383940# 加载README.md文件&gt;&gt;&gt; textFile = spark.read.text("README.md")# 统计文件行数&gt;&gt;&gt; textFile.count()103# 获取第一行&gt;&gt;&gt; textFile.first()Row(value='# Apache Spark')# 过滤出包含"Spark"的行&gt;&gt;&gt; linesWithSpark = textFile.filter(textFile.value.contains("Spark"))# 统计包含"Spark"的行的行数&gt;&gt;&gt; textFile.filter(textFile.value.contains("Spark")).count()&gt;&gt;&gt; from pyspark.sql.functions import *# 获取包含单词最多行的单词个数&gt;&gt;&gt; textFile.select(size(split(textFile.value, "\s+")).name("numWords")).agg(max... (col("numWords"))).collect()[Row(max(numWords)=22)]# 统计单词出现的频次&gt;&gt;&gt; wordCounts = textFile.select(explode(split(textFile.value, "\s+")).name("wor... d")).groupBy("word").count()&gt;&gt;&gt; wordCounts.collect()[Row(word='online', count=1), Row(word='graphs', count=1), ...]# 缓存&gt;&gt;&gt; linesWithSpark.cache()DataFrame[value: string]&gt;&gt;&gt; linesWithSpark.count()20 在使用Spark Shell时，也会开启一个Spark monitor UI，默认在本地4040端口运行。 Spark Monitor UI 创建一个简单的应用 一个简单的应用统计文件中包含字符“a”和“b”的行数的脚本SimpleApp.py 123456789101112131415$ cat SimpleApp.py"""SimpleApp.py"""from pyspark.sql import SparkSessionappName='Simple Application'logFile = "README.md" # Should be some file on your systemspark = SparkSession.builder.appName(appName).getOrCreate()logData = spark.read.text(logFile).cache()numAs = logData.filter(logData.value.contains('a')).count()numBs = logData.filter(logData.value.contains('b')).count()print("Lines with a: %i, lines with b: %i" % (numAs, numBs))spark.stop() 运行应用 1$ ./bin/spark-submit --master "local[4]" SimpleApp.py 创建SparkSession对象 一个SparkSession对象表示与本地(或远程)的Spark集群连接，是与Spark交互的主要入口。 为了建立到Spark的连接，SparkSession需要配置如下信息: Master URL: spark服务器的连接地址, 具体可参考Master URLs Application Name: 便于识别的应用名 Spark Home: spark的安装路径 JARs: 任务依赖的JAR包路径 SparkSession vs SparkContext SparkSession和SparkContext两者之间有什么关系呢？ 让我们回顾一下spark的发展历史，看能不能找到一点线索: 在Spark 2.0.0 之前。主要有三个连接对象: SparkContext: 主要连接Spark执行环境和完成进行创建RDDs等其它功能 SqlContext: 和SparkSQL一起在SparkContext背后工作 HiveContext: 和Hive仓库交互 Saprk 2.0.0之后，引入了Datasets/DataFrames作为主要的分数式数据抽象接口。SparkSession对象成为了Spark操作的主要入口。有几点需要注意: 在Scala和Java中Datasets是主要的数据抽象类型。而在R和Python语言中，DataFrame是主要的抽象类型。它们在API操作上基本没有区别(可以认为是一个东西，只是在不同语言里叫法不一样) 尽管在2.0.0之后，Datasets/DataFrames作为新的接口形式，但是RDDs依然在被使用，所以当操作RDDs，主要使用SparkContext。 SparkSession对象包含了SparkContext对象。在Spark 2.0.0版本之后，SparkContext仍然是作为连接Spark集群的管道，因此SparkContext主要执行环境操作，比如: 累加器(accumulators)，addFile, addJars等。而SparkSession则用于读取和创建Datasets/DataFrames等操作。 创建一个SparkSession对象 我们可以使用下面的语句创建一个SparkSession对象 1234&gt;&gt;&gt; from pyspark.sql import SparkSession&gt;&gt;&gt; sparkSession = SparkSession.builder.appName('app name').\ master('local').getOrCreate() 其实当我们运行pyspark时，其实就已经自动创建了一个SparkSession并且分配给了spark变量。 在pyspark脚本里面，我们可以看到下面一句话。 1export PYTHONSTARTUP="$&#123;SPARK_HOME&#125;/python/pyspark/shell.py" 它设置了环境变量PYTHONSTARTUP, 那么在我们运行pyspark时，会先自动运行脚本 1$&#123;SPARK_HOME&#125;/python/pyspark/shell.py 我们再来看看这个脚本里有些什么，下图是脚本的部分截图: 从红色框里的代码，我们可以看到创建了SparkSession对象并赋值给spark，SparkSession包含了SparkContext(spark.sparkContext), 并且使用sc指向SparkContext。 简要的说，我们一般采用如下规则: 创建一个SparkSession对象 使用SparkSession读取，创建SQL语句的视图，创建Datasets/DataFrames 从SparkSession得到sparkContext, 用于完成累加器(accumulators)，分发缓存文件和RDDs的相关操作。 sparkContext元数据 sparkContext包含了一些实用的元数据信息。 获取spark版本信息 12345&gt;&gt;&gt; spark.versionu'2.2.1'&gt;&gt;&gt; sc.versionu'2.2.1' 获取部署的应用名 12&gt;&gt;&gt; sc.appNameu'PySparkShell' 获取内存信息(Python中暂时没有实现) 1234567&gt;&gt;&gt; sc.getExecutorMemoryStatus---------------------------------------------------------------------------AttributeError Traceback (most recent call last)&lt;ipython-input-4-755a4391f6a7&gt; in &lt;module&gt;()----&gt; 1 sc.getExecutorMemoryStatusAttributeError: 'SparkContext' object has no attribute 'getExecutorMemoryStatus' 但是Scala语言中有 12scala&gt; sc.getExecutorMemoryStatusres0: scala.collection.Map[String,(Long, Long)] = Map(169.254.93.254:55267 -&gt; (384093388,384093388)) 169.254.93.254:55267代办主机信息。 (384093388,384093388)分别代表当前分配的最大内存和剩余内存 获取主机信息 12&gt;&gt;&gt; sc.masteru'local[*]' 获取配置信息 1234567891011&gt;&gt;&gt; sc.getConf().toDebugString().split('\n')[u'spark.app.id=local-1515662568196', u'spark.app.name=PySparkShell', u'spark.driver.host=169.254.93.254', u'spark.driver.port=55105', u'spark.executor.id=driver', u'spark.master=local[*]', u'spark.rdd.compress=True', u'spark.serializer.objectStreamReset=100', u'spark.sql.catalogImplementation=hive', u'spark.submit.deployMode=client']]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《利用Python进行数据分析第二版》读书笔记五]]></title>
    <url>%2F2018%2F01%2F10%2Fpython-for-data-analysis-2nd-edition-note-five%2F</url>
    <content type="text"><![CDATA[数据聚合和分组操作 分组机制 数据聚合 数据分割和合并 透视图和交叉表(Pivot Tables and Cross-Tabulation)]]></content>
      <categories>
        <category>python数据分析</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[《利用Python进行数据分析第二版》读书笔记四]]></title>
    <url>%2F2018%2F01%2F09%2Fpython-for-data-analysis-2nd-edition-note-four%2F</url>
    <content type="text"><![CDATA[绘图和可视化 绘图时，在ipython里运行 12&gt;&gt;&gt; %matplotlibUsing matplotlib backend: TkAgg 可以直接绘制图形，不然每次绘制图形最后必须调用plt.show()才能看到图形。 本文里使用的CSV测试数据可以从这里下载 matplotlib简单入门 关于matplotlib的使用入门，可以参考: matplotlib教程 使用pandas和seaborn绘图 数据可视化方面, matplotlib算是比较底层的工具了。 pandas也简单内置了一些常用的画图方法。seaborn则是另一个专门用于数据统计的可视化库。 注意 在代码中引入searborn会修改matplotlib默认的画图颜色和样式，即便不使用searborn画图，也可以引入它来提高画图的美化效果。 线性图 Series和DataFrame都有plot方法来绘制一些基本图形，默认调用plot方法会绘制线性图 12345678910111213141516&gt;&gt;&gt; s = pd.Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))&gt;&gt;&gt; s0 0.47115410 0.03709320 -0.13016530 -0.90708740 -0.49593850 0.52516660 0.82241670 0.66737680 1.48817090 2.041607dtype: float64&gt;&gt;&gt; s.plot() SeriesPlot 默认使用Series的index作为x轴。也可以通过参数use_index=False来设置不用Series的index作为x轴。其它可以定制的参数如下: Series Plot Method DataFrame的plot方法会把每一列绘制成一条线，并且自动绘制图例 123456&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(10, 4).cumsum(0),... columns=['A', 'B', 'C', 'D'],... index=np.arange(0, 100, 10))...&gt;&gt;&gt; df.plot() DataFrame plot DataFrame的plot方法常用的参数如下： 条形图(Bar Plots) plot.bar()和plot.barh()可以用来绘垂直和水平的制柱状图。 大多数pandas的plot方法都可以接受ax参数（一个matplotlib subplot对象），让我们更方便在表格样式中绘制子图。 123456789&gt;&gt;&gt; fig, axes = plt.subplots(2, 1)&gt;&gt;&gt; data = pd.Series(np.random.rand(16), index=list('abcdefghijklmnop'))&gt;&gt;&gt; data.plot.bar(ax=axes[0], color='k', alpha=0.7)&lt;matplotlib.axes._subplots.AxesSubplot at 0x1076809e8&gt;&gt;&gt;&gt; data.plot.barh(ax=axes[1], color='k', alpha=0.7)&lt;matplotlib.axes._subplots.AxesSubplot at 0x10be498d0&gt; Series plot bar DataFrame在做柱状图时会自动按照列分组 123456789101112131415&gt;&gt;&gt; df = pd.DataFrame(np.random.rand(6, 4),... index=['one', 'two', 'three', 'four', 'five', 'six'],... columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))...&gt;&gt;&gt; dfGenus A B C Done 0.682972 0.387897 0.052612 0.249418two 0.628619 0.415778 0.412352 0.455799three 0.330864 0.472638 0.780744 0.606024four 0.665816 0.172358 0.294163 0.038945five 0.390196 0.354316 0.566566 0.592186six 0.556651 0.721578 0.857398 0.939181&gt;&gt;&gt; df.plot.bar() DataFrame plot bar 我们可以可以通过参数stacked=True来创建堆叠的柱状图 1&gt;&gt;&gt; df.plot.barh(stacked=True, alpha=0.5) DataFrame plot barh 小贴士: 一个比较常见的关于Series绘制柱状图就是使用value_counts, 可以很直观的看到统计的数据 12345678910111213141516171819202122232425&gt;&gt;&gt; s = pd.Series(np.random.randint(0, 10, 20))&gt;&gt;&gt; s0 41 92 03 44 95 06 27 88 09 610 311 612 913 414 715 116 117 318 719 6&gt;&gt;&gt; s.value_counts().plot.bar() 让我们来看一个统计聚会人数多少和星期几的关系, 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&gt;&gt;&gt; tips = pd.read_csv('examples/tips.csv')# size: 聚会人数# day: 星期几&gt;&gt;&gt; tips.head() total_bill tip smoker day time size0 16.99 1.01 No Sun Dinner 21 10.34 1.66 No Sun Dinner 32 21.01 3.50 No Sun Dinner 33 23.68 3.31 No Sun Dinner 24 24.59 3.61 No Sun Dinner 4# 可以得知聚会人数介于1-6之间&gt;&gt;&gt; tips['size'].unique()array([2, 3, 4, 1, 6, 5])&gt;&gt;&gt; tips[tips['size'] == 1] total_bill tip smoker day time size67 3.07 1.00 Yes Sat Dinner 182 10.07 1.83 No Thur Lunch 1111 7.25 1.00 No Sat Dinner 1222 8.58 1.92 Yes Fri Lunch 1# 以星期为index, size为列统计数据&gt;&gt;&gt; party_counts = pd.crosstab(tips['day'], tips['size'])&gt;&gt;&gt; party_countssize 1 2 3 4 5 6dayFri 1 16 1 1 0 0Sat 2 53 18 13 1 0Sun 0 39 15 18 3 1Thur 1 48 4 5 1 3# 排除1个人的情况&gt;&gt;&gt; party_counts = party_counts.loc[:, 2:5]&gt;&gt;&gt; party_countssize 2 3 4 5dayFri 16 1 1 0Sat 53 18 13 1Sun 39 15 18 3Thur 48 4 5 1# 计算星期的总人数&gt;&gt;&gt; party_counts.sum(1)dayFri 18Sat 85Sun 75Thur 58dtype: int64# 计算size占星期的百分比&gt;&gt;&gt; party_pcts = party_counts.div(party_counts.sum(1), axis=0)&gt;&gt;&gt; party_pctssize 2 3 4 5dayFri 0.888889 0.055556 0.055556 0.000000Sat 0.623529 0.211765 0.152941 0.011765Sun 0.520000 0.200000 0.240000 0.040000Thur 0.827586 0.068966 0.086207 0.017241party_pcts.plot.bar() 从上图可以看出，周末的时候，聚会人数(size)多的是在周末。 当数据需要聚合和汇总时，使用seaborn更方便一点。让我们来计算每天的小费情况 12345678910111213&gt;&gt;&gt; import seaborn as sns&gt;&gt;&gt; tips['tip_pct'] = tips['tip'] / (tips['total_bill'] - tips['tip'])&gt;&gt;&gt; tips.head() total_bill tip smoker day time size tip_pct0 16.99 1.01 No Sun Dinner 2 0.0632041 10.34 1.66 No Sun Dinner 3 0.1912442 21.01 3.50 No Sun Dinner 3 0.1998863 23.68 3.31 No Sun Dinner 2 0.1624944 24.59 3.61 No Sun Dinner 4 0.172069&gt;&gt;&gt; sns.barplot(x='tip_pct', y='day', data=tips, orient='h') 我们还可以通过hue参数来让它来添加额外的分类 1&gt;&gt;&gt; sns.barplot(x='tip_pct', y='day', hue='time', data=tips, orient='h') 我们还可以改变searborn的绘图样式 1&gt;&gt;&gt; sns.set(style="whitegrid") 柱状图和密度图(Histograms and Density Plots) 使用Series和DataFrame的plot函数 首先我们看一下小费百分比的分布区间 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&gt;&gt;&gt; pd.cut(tips['tip_pct'], 50)0 (0.0345, 0.0853]1 (0.182, 0.23]2 (0.182, 0.23]3 (0.134, 0.182]4 (0.134, 0.182]5 (0.182, 0.23]6 (0.278, 0.327]7 (0.0853, 0.134]8 (0.134, 0.182]9 (0.278, 0.327]10 (0.182, 0.23]11 (0.134, 0.182]12 (0.0853, 0.134]13 (0.182, 0.23]14 (0.23, 0.278]15 (0.182, 0.23]16 (0.182, 0.23]17 (0.278, 0.327]18 (0.23, 0.278]19 (0.182, 0.23]20 (0.278, 0.327]21 (0.134, 0.182]22 (0.134, 0.182]23 (0.23, 0.278]24 (0.182, 0.23]25 (0.134, 0.182]26 (0.134, 0.182]27 (0.182, 0.23]28 (0.23, 0.278]29 (0.134, 0.182] ...214 (0.278, 0.327]215 (0.0853, 0.134]216 (0.0853, 0.134]217 (0.134, 0.182]218 (0.182, 0.23]219 (0.0853, 0.134]220 (0.182, 0.23]221 (0.327, 0.375]222 (0.278, 0.327]223 (0.23, 0.278]224 (0.0853, 0.134]225 (0.134, 0.182]226 (0.23, 0.278]227 (0.134, 0.182]228 (0.23, 0.278]229 (0.134, 0.182]230 (0.0853, 0.134]231 (0.23, 0.278]232 (0.375, 0.423]233 (0.134, 0.182]234 (0.23, 0.278]235 (0.134, 0.182]236 (0.0853, 0.134]237 (0.0345, 0.0853]238 (0.134, 0.182]239 (0.23, 0.278]240 (0.0345, 0.0853]241 (0.0853, 0.134]242 (0.0853, 0.134]243 (0.182, 0.23]Name: tip_pct, Length: 244, dtype: categoryCategories (50, interval[float64]): [(0.0345, 0.0853] &lt; (0.0853, 0.134] &lt; (0.134, 0.182] &lt; (0.182, 0.23] ... (2.259, 2.307] &lt; (2.307, 2.356] &lt; (2.356, 2.404] &lt; (2.404, 2.452]]&gt;&gt;&gt; pd.value_counts(bins)(0.134, 0.182] 76(0.182, 0.23] 57(0.23, 0.278] 41(0.0853, 0.134] 33(0.278, 0.327] 15(0.0345, 0.0853] 12(0.327, 0.375] 4(0.375, 0.423] 3(0.713, 0.762] 1(0.472, 0.52] 1(2.404, 2.452] 1(1.873, 1.921] 0(1.969, 2.018] 0(0.81, 0.858] 0(0.762, 0.81] 0(1.921, 1.969] 0(0.665, 0.713] 0(0.617, 0.665] 0(0.568, 0.617] 0(0.52, 0.568] 0(0.423, 0.472] 0(0.907, 0.955] 0(2.018, 2.066] 0(2.066, 2.114] 0(2.114, 2.163] 0(2.163, 2.211] 0(2.211, 2.259] 0(2.259, 2.307] 0(2.307, 2.356] 0(0.858, 0.907] 0(0.955, 1.003] 0(1.824, 1.873] 0(1.438, 1.486] 0(1.776, 1.824] 0(1.728, 1.776] 0(1.679, 1.728] 0(1.631, 1.679] 0(1.583, 1.631] 0(1.535, 1.583] 0(1.486, 1.535] 0(1.39, 1.438] 0(1.003, 1.051] 0(1.341, 1.39] 0(1.293, 1.341] 0(1.245, 1.293] 0(2.356, 2.404] 0(1.148, 1.196] 0(1.1, 1.148] 0(1.051, 1.1] 0(1.196, 1.245] 0Name: tip_pct, dtype: int64 从上面可以看出，小费百分比在(0.134, 0.182]里的最多，下面的图形也能反应这一点 12&gt;&gt;&gt; tips['tip_pct'].plot.hist(bins=50)&lt;matplotlib.axes._subplots.AxesSubplot at 0x12607ce10&gt; 12&gt;&gt;&gt; tips['tip_pct'].plot.density()&lt;matplotlib.axes._subplots.AxesSubplot at 0x12607ce10&gt; 使用searborn的distplot方法更简便，可以同时在一个图里绘制柱状图和密度曲线 12345678&gt;&gt;&gt; comp1 = np.random.normal(0, 1, size=200)&gt;&gt;&gt; comp2 = np.random.normal(10, 2, size=200)&gt;&gt;&gt; values = pd.Series(np.concatenate([comp1, comp2]))&gt;&gt;&gt; sns.distplot(values, bins=100, color='k')&lt;matplotlib.axes._subplots.AxesSubplot at 0x120e1c588&gt; 散点图或点图(Scatter or Point Plots) 使用seaborn的regplot函数，不仅可以画散点图，还可以绘制一个线性回归线 12345678910111213141516171819&gt;&gt;&gt; macro = pd.read_csv('examples/macrodata.csv')&gt;&gt;&gt; data = macro[['cpi', 'm1', 'tbilrate', 'unemp']]&gt;&gt;&gt; trans_data = np.log(data).diff().dropna()&gt;&gt;&gt; trans_data[-5:] cpi m1 tbilrate unemp198 -0.007904 0.045361 -0.396881 0.105361199 -0.021979 0.066753 -2.277267 0.139762200 0.002340 0.010286 0.606136 0.160343201 0.008419 0.037461 -0.200671 0.127339202 0.008894 0.012202 -0.405465 0.042560# 以m1为x轴，unemp为y轴&gt;&gt;&gt; sns.regplot('m1', 'unemp', data=trans_data)&lt;matplotlib.axes._subplots.AxesSubplot at 0x116c026d8&gt;&gt;&gt;&gt; plt.title('Changes in log %s versus log %s' % ('m1', 'unemp'))Text(0.5,1,'Changes in log m1 versus log unemp') 使用pairplot函数可以绘制散点图矩阵 12&gt;&gt;&gt; sns.pairplot(trans_data, diag_kind='kde', plot_kws=&#123;'alpha': 0.2&#125;)&lt;seaborn.axisgrid.PairGrid at 0x12760e0b8&gt; 多面网格和类别数据(Facet Grids and Categorical Data) 如果遇到数据集需要额外的分组维度，可以利用多面网格。seaborn有一个有用的内建函数factorplot 从day/time/smoker来展示小费的百分比 123456&gt;&gt;&gt; tips = pd.read_csv('examples/tips.csv')&gt;&gt;&gt; tips['tip_pct'] = tips['tip'] / (tips['total_bill'] - tips['tip'])&gt;&gt;&gt; sns.factorplot(x='day', y='tip_pct', hue='time', col='smoker', kind='bar', data=tips[tips.tip_pct &lt; 1])&lt;seaborn.axisgrid.FacetGrid at 0x12724b390&gt; 除了按照time作为bar分组之外，我们还可以通过添加参数row=time添加额外的面 12&gt;&gt;&gt; sns.factorplot(x='day', y='tip_pct', row='time', col='smoker', kind='bar', data=tips[tips.tip_pct &lt; 1])&lt;seaborn.axisgrid.FacetGrid at 0x1261bc6d8&gt; factorplot还支持一些其它的绘图参数, 如绘制box图 12&gt;&gt;&gt; sns.factorplot(x='tip_pct', y='day', kind='box', data=tips[tips.tip_pct &lt; 0.5])&lt;seaborn.axisgrid.FacetGrid at 0x127caeac8&gt; 其它的python可视化工具 除了上面提到的之外，还有许多开源的可视化工具值得我们去探索 Bokeh Plotly Graph 对于制作用于打印或网页的静态图形，作者推荐使用matplotlib以及pandas和seaborn这样的工具。对于其他的数据可视化需求，去学一个有用的工具可能会有帮助。]]></content>
      <categories>
        <category>python数据分析</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
        <tag>pandas</tag>
        <tag>seaborn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pipenv使用指南]]></title>
    <url>%2F2018%2F01%2F08%2Fpipenv-tour%2F</url>
    <content type="text"><![CDATA[pipenv是Python官方推荐的包管理工具。可以说，它集成了virtualenv, pip和pyenv三者的功能。其目的旨在集合了所有的包管理工具的长处，如: npm, yarn, composer等的优点。 它能够自动为项目创建和管理虚拟环境，从Pipfile文件添加或删除安装的包，同时生成Pipfile.lock来锁定安装包的版本和依赖信息，避免构建错误。 pipenv主要解决了如下问题: 不用再单独使用pip和virtualenv, 现在它们合并在一起了 不用再维护requirements.txt, 使用Pipfile和Pipfile.lock来代替 可以使用多个python版本(python2和python3) 在安装了pyenv的条件下，可以自动安装需要的Python版本 安装 为了方便使用, 建议全局安装 1$ pip install pipenv 基本概念 虚拟环境如果不存在的话，会自动创建 当install命令没有传递参数指定安装包时，所有[packages]里指定的包都会被安装 pipenv --three可以初始化一个python3版本的虚拟环境 pipenv --two可以初始化一个python2版本的虚拟环境 添加shell补齐 如果使用的是bash, 可添加下面语句到.bashrc或.bash_profile 1eval "$(pipenv --completion)" pipenv命令 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152$ pipenvUsage: pipenv [OPTIONS] COMMAND [ARGS]...Options: --update Update Pipenv &amp; pip to latest. # 更新pipenv和pip到最新版本 --where Output project home information. # 获取项目路径 --venv Output virtualenv information. # 获取虚拟环境的路径 --py Output Python interpreter information. # 获取python解释器的路径 --envs Output Environment Variable options. # 输出当前的环境变量 --rm Remove the virtualenv. # 删除虚拟环境 --bare Minimal output. --completion Output completion (to be eval'd). --man Display manpage. --three / --two Use Python 3/2 when creating virtualenv. # 使用python3/2创建虚拟环境 --python TEXT Specify which version of Python virtualenv should use. # 指定python的版本信息 --site-packages Enable site-packages for the virtualenv. --jumbotron An easter egg, effectively. --version Show the version and exit. -h, --help Show this message and exit.Usage Examples: Create a new project using Python 3.6, specifically: $ pipenv --python 3.6 Install all dependencies for a project (including dev): $ pipenv install --dev Create a lockfile containing pre-releases: $ pipenv lock --pre Show a graph of your installed dependencies: $ pipenv graph Check your installed dependencies for security vulnerabilities: $ pipenv check Install a local setup.py into your virtual environment/Pipfile: $ pipenv install -e .Commands: check Checks for security vulnerabilities and against PEP 508 markers provided in Pipfile. graph Displays currently–installed dependency graph information. install Installs provided packages and adds them to Pipfile, or (if none is given), installs all packages. lock Generates Pipfile.lock. open View a given module in your editor. run Spawns a command installed into the virtualenv. shell Spawns a shell within the virtualenv. uninstall Un-installs a provided package and removes it from Pipfile. update Uninstalls all packages, and re-installs package(s) in [packages] to latest compatible versions. 常用命令介绍 1234567891011121314151617# 安装包$ pipenv install# 激活当前项目的虚拟环境$ pipenv shell# 安装开发依赖包$ pipenv install pytest --dev# 图形显示包依赖关系$ pipenv graph# 生成lockfile$ pipenv lock# 删除所有的安装包$ pipenv uninstall --all 高级技巧 导入requirements.txt 当在执行pipenv install命令的时候，如果有一个requirements.txt文件，那么会自动从requirements.txt文件导入安装包信息并创建一个Pipfile文件。 同样可以使用$ pipenv install -r path/to/requirements.txt来导入requirements.txt文件 注意: 默认情况下，我们都是在requirements.txt文件里指定了安装包的版本信息的，在导入requirements.txt文件时，版本信息也会被自动写Pipfile文件里， 但是这个信息我们一般不需要保存在Pipfile文件里，需要手动更新Pipfile来删除版本信息 指定安装包的版本信息 为了安装指定版本的包信息，可以使用: 1$ pipenv install requests==2.13.0 这个命令也会自动更新Pipfile文件 指定Python的版本信息 在创建虚拟环境的时候，我们可以指定使用的python版本信息，类似pyenv 123$ pipenv --python 3$ pipenv --python 3.6$ pipenv --python 2.7.14 pipenv会自动扫描系统寻找合适的版本信息，如果找不到的话，同时又安装了pyenv, 它会自动调用pyenv下载对应的版本的python 指定安装包的源 如果我们需要在安装包时，从一个源下载一个安装包，然后从另一个源下载另一个安装包，我们可以通过下面的方式配置 12345678910111213141516[[source]]url = &quot;https://pypi.python.org/simple&quot;verify_ssl = truename = &quot;pypi&quot;[[source]]url = &quot;http://pypi.home.kennethreitz.org/simple&quot;verify_ssl = falsename = &quot;home&quot;[dev-packages][packages]requests = &#123;version=&quot;*&quot;, index=&quot;home&quot;&#125;maya = &#123;version=&quot;*&quot;, index=&quot;pypi&quot;&#125;records = &quot;*&quot; 如上设置了两个源： pypi(https://pypi.python.org/simple) home(http://pypi.home.kennethreitz.org/simple) 同时指定requests包从home源下载，maya包从pypi源下载 生成requirements.txt文件 我们也可以从Pipfile和Pipfile.lock文件来生成requirements.txt 12345# 生成requirements.txt文件$ pipenv lock -r# 生成dev-packages的requirements.txt文件# pipenv lock -r -d 检查安全隐患 pipenv包含了safety模块，可以让我们坚持安装包是否存在安全隐患。 1234567891011121314151617181920212223242526272829303132333435363738$ cat Pipfile[packages]django = "==1.10.1"$ pipenv checkChecking PEP 508 requirements…Passed!Checking installed package safety…33075: django &gt;=1.10,&lt;1.10.3 resolved (1.10.1 installed)!Django before 1.8.x before 1.8.16, 1.9.x before 1.9.11, and 1.10.x before 1.10.3, when settings.DEBUG is True, allow remote attackers to conduct DNS rebinding attacks by leveraging failure to validate the HTTP Host header against settings.ALLOWED_HOSTS.33076: django &gt;=1.10,&lt;1.10.3 resolved (1.10.1 installed)!Django 1.8.x before 1.8.16, 1.9.x before 1.9.11, and 1.10.x before 1.10.3 use a hardcoded password for a temporary database user created when running tests with an Oracle database, which makes it easier for remote attackers to obtain access to the database server by leveraging failure to manually specify a password in the database settings TEST dictionary.33300: django &gt;=1.10,&lt;1.10.7 resolved (1.10.1 installed)!CVE-2017-7233: Open redirect and possible XSS attack via user-supplied numeric redirect URLs============================================================================================Django relies on user input in some cases (e.g.:func:`django.contrib.auth.views.login` and :doc:`i18n &lt;/topics/i18n/index&gt;`)to redirect the user to an "on success" URL. The security check for theseredirects (namely ``django.utils.http.is_safe_url()``) considered some numericURLs (e.g. ``http:999999999``) "safe" when they shouldn't be.Also, if a developer relies on ``is_safe_url()`` to provide safe redirecttargets and puts such a URL into a link, they could suffer from an XSS attack.CVE-2017-7234: Open redirect vulnerability in ``django.views.static.serve()``=============================================================================A maliciously crafted URL to a Django site using the:func:`~django.views.static.serve` view could redirect to any other domain. Theview no longer does any redirects as they don't provide any known, usefulfunctionality.Note, however, that this view has always carried a warning that it is nothardened for production use and should be used only as a development aid. 编码风格检查 pipenv默认集成了flake8, 可以用来检测编码风格 123456$ cat t.pyimport requests$ pipenv check --style t.pyt.py:1:1: F401 'requests' imported but unusedt.py:1:16: W292 no newline at end of file 浏览模块代码 12# 用编辑器打开requests模块$ pipenv open requests 自动加载环境变量.env 如果项目根目录下有.env文件，$ pipenv shell和$ pipenv run会自动加载它。 1234567891011$ cat .envHELLO=WORLD$ pipenv run pythonLoading .env environment variables…Python 2.7.13 (default, Jul 18 2017, 09:17:00)[GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.42)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import os&gt;&gt;&gt; os.environ[&apos;HELLO&apos;]&apos;WORLD&apos; 自定义虚拟环境的路径 默认情况下,pipenv使用pew来管理虚拟环境的路径，我们可以自定义WORKON_HOME环境变量来设置虚拟环境的路径。比如: 1export WORKON_HOME=~/.venvs 我们也可以通过社会环境变量PIPENV_VENV_IN_PROJECT使虚拟环境在每个项目的根目录下project/.venv。 自动激活虚拟环境 配合virtualenv-autodetect和设置PIPENV_VENV_IN_PROJECT环境变量可以自动激活虚拟环境。 在.bashrc或.bash_profile中配置如下 12export PIPENV_VENV_IN_PROJECT=1source /path/to/virtualenv-autodetect.sh 如果使用了oh-my-zsh, 可以直接使用它的插件形式 12# 安装插件$ git@github.com:RobertDeRose/virtualenv-autodetect.git ~/.oh-my-zsh/custom/plugins 再修改.zshrc文件启动插件 12# 找到启动plugins的行添加启用插件plugins=(... virtualenv-autodetect) 通过环境变量配置pipenv pipenv内置了很多环境变量，可以通过设置这些环境变量来配置pipenv 1234567891011121314151617181920212223242526$ pipenv --envsThe following environment variables can be set, to do various things: - PIPENV_MAX_DEPTH - PIPENV_SHELL - PIPENV_DOTENV_LOCATION - PIPENV_HIDE_EMOJIS - PIPENV_CACHE_DIR - PIPENV_VIRTUALENV - PIPENV_MAX_SUBPROCESS - PIPENV_COLORBLIND - PIPENV_VENV_IN_PROJECT # 在项目根路径下创建虚拟环境 - PIPENV_MAX_ROUNDS - PIPENV_USE_SYSTEM - PIPENV_SHELL_COMPAT - PIPENV_USE_HASHES - PIPENV_NOSPIN - PIPENV_PIPFILE - PIPENV_INSTALL_TIMEOUT - PIPENV_YES - PIPENV_SHELL_FANCY - PIPENV_DONT_USE_PYENV - PIPENV_DONT_LOAD_ENV - PIPENV_DEFAULT_PYTHON_VERSION # 设置创建虚拟环境时默认的python版本信息，如: 3.6 - PIPENV_SKIP_VALIDATION - PIPENV_TIMEOUT 需要修改某个默认配置时，只需要把它添加到.bashrc或.bash_profile文件里即可。 常见问题 pipenv install时报错pip.exceptions.InstallationError: Command &quot;python setup.py egg_info&quot; failed with error code 1 错误原因是pipenv是用python2安装的，解决办法是使用pip3重新安装pipenv 12$ pip unintall pipenv$ pip3 install pipenv 在项目目录里运行pipenv时报错AttributeError: module 'enum' has no attribute 'IntFlag' 12345678910111213141516171819202122$ pipenvFailed to import the site moduleTraceback (most recent call last): File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site.py", line 544, in &lt;module&gt; main() File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site.py", line 530, in main known_paths = addusersitepackages(known_paths) File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site.py", line 282, in addusersitepackages user_site = getusersitepackages() File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site.py", line 258, in getusersitepackages user_base = getuserbase() # this will also set USER_BASE File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site.py", line 248, in getuserbase USER_BASE = get_config_var('userbase') File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/sysconfig.py", line 601, in get_config_var return get_config_vars().get(name) File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/sysconfig.py", line 580, in get_config_vars import _osx_support File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/_osx_support.py", line 4, in &lt;module&gt; import re File "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/re.py", line 142, in &lt;module&gt; class RegexFlag(enum.IntFlag):AttributeError: module 'enum' has no attribute 'IntFlag' 是因为在项目目录里运行pipenv命令时，项目虚拟环境的python版本低于3.6.4, 由于IntFlag是从python3.6.4才开始集成到python内置模块的。当激活了项目的虚拟环境之后, 环境变量PYTHONPATH会被设置为当前虚拟环境的site-packages目录，因此pipenv依赖的IntFlag无法找到。 解决办法是在运行pipenv时设置环境变量PYTHONPATH为空 1$ PYTHONPATH= pipenv]]></content>
      <categories>
        <category>python库相关</category>
      </categories>
      <tags>
        <tag>pipenv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《利用Python进行数据分析第二版》读书笔记三]]></title>
    <url>%2F2018%2F01%2F04%2Fpython-for-data-analysis-2nd-edition-note-two%2F</url>
    <content type="text"><![CDATA[数据合并，拼接和变型 分层索引 分层索引是Pandas的一个重要特性，能够让我们在一个轴上拥有多层索引。简单来说，它提供了一个在降纬模式下处理多维数据的能力。 如下是一个多层索引的例子 12345678910111213141516171819202122232425262728293031323334353637&gt;&gt;&gt; data = pd.Series(np.random.randn(9),... index=[['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'], [1,2,3,1,3,1,2,2,3]])&gt;&gt;&gt; dataa 1 1.007189 2 -1.296221 3 0.274992b 1 0.228913 3 1.352917c 1 0.886429 2 -2.001637d 2 -0.371843 3 1.669025dtype: float64&gt;&gt;&gt; data.indexMultiIndex(levels=[['a', 'b', 'c', 'd'], [1, 2, 3]], labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 2, 0, 1, 1, 2]])# 访问外层索引&gt;&gt;&gt; data['b']1 0.2289133 1.352917dtype: float64&gt;&gt;&gt; data['b':'c']b 1 0.228913 3 1.352917c 1 0.886429 2 -2.001637# 访问内层索引&gt;&gt;&gt; data.loc[:, 2]a -1.296221c -2.001637d -0.371843dtype: float64 多层索引在数据变型和透视表中非常常见，可以使用unstack方法，将Series转换成DataFrame 123456&gt;&gt;&gt; data.unstack() 1 2 3a 1.007189 -1.296221 0.274992b 0.228913 NaN 1.352917c 0.886429 -2.001637 NaNd NaN -0.371843 1.669025 与unstack方法相反作用的就是stack 1234567891011&gt;&gt;&gt; data.unstack().stack()a 1 1.007189 2 -1.296221 3 0.274992b 1 0.228913 3 1.352917c 1 0.886429 2 -2.001637d 2 -0.371843 3 1.669025dtype: float64 DataFrame的任何一个轴都可以有多个索引 12345678910111213141516171819202122232425262728293031323334353637383940&gt;&gt;&gt; frame = pd.DataFrame(np.arange(12).reshape((4, 3)), index=[['a', 'a', 'b','b'], [1, 2, 1, 2]], columns=[['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']])&gt;&gt;&gt; frame Ohio Colorado Green Red Greena 1 0 1 2 2 3 4 5b 1 6 7 8 2 9 10 11# 可以给每层索引取一个名字&gt;&gt;&gt; frame.index.names = ["key1", "key2"]&gt;&gt;&gt; frame.columns.names = ["state", "color"]&gt;&gt;&gt; framestate Ohio Coloradocolor Green Red Greenkey1 key2a 1 0 1 2 2 3 4 5b 1 6 7 8 2 9 10 11# 通过外层索引选择列&gt;&gt;&gt; frame['Ohio']color Green Redkey1 key2a 1 0 1 2 3 4b 1 6 7 2 9 10# 通过内层索引选择列&gt;&gt;&gt; frame['Ohio']['Green']key1 key2a 1 0 2 3b 1 6 2 9 分层排序和分类 使用swaplevel，我们可以重新对一个轴上的多层索引排序 1234567891011121314151617&gt;&gt;&gt; framestate Ohio Coloradocolor Green Red Greenkey1 key2a 1 0 1 2 2 3 4 5b 1 6 7 8 2 9 10 11&gt;&gt;&gt; frame.swaplevel('key1', 'key2')state Ohio Coloradocolor Green Red Greenkey2 key11 a 0 1 22 a 3 4 51 b 6 7 82 b 9 10 11 我们也可以在一个分层上对数据进行分类 1234567891011121314151617&gt;&gt;&gt; frame.sort_index(level=1)state Ohio Coloradocolor Green Red Greenkey1 key2a 1 0 1 2b 1 6 7 8a 2 3 4 5b 2 9 10 11&gt;&gt;&gt; frame.swaplevel(0, 1).sort_index(level=0)state Ohio Coloradocolor Green Red Greenkey2 key11 a 0 1 2 b 6 7 82 a 3 4 5 b 9 10 11 分层统计 许多关于Series和DataFrame的统计操作，都接受一个level参数来对层上进行统计 1234567891011121314151617181920212223&gt;&gt;&gt; framestate Ohio Coloradocolor Green Red Greenkey1 key2a 1 0 1 2 2 3 4 5b 1 6 7 8 2 9 10 11&gt;&gt;&gt; frame.sum(level='key2')state Ohio Coloradocolor Green Red Greenkey21 6 8 102 12 14 16&gt;&gt;&gt; frame.sum(level='color', axis=1)color Green Redkey1 key2a 1 2 1 2 8 4b 1 14 7 2 20 10 利用DataFrame的列作为索引 使用set_index方法，可以将DataFrame的列作为索引 123456789101112131415161718192021222324252627282930313233343536373839404142434445&gt;&gt;&gt; frame = pd.DataFrame(&#123;'a': range(7), 'b': range(7, 0, -1),... 'c': ['one', 'one', 'one', 'two', 'two',... 'two', 'two'],... 'd':[0,1,2,0,1,2,3]&#125;)...&gt;&gt;&gt; frame a b c d0 0 7 one 01 1 6 one 12 2 5 one 23 3 4 two 04 4 3 two 15 5 2 two 26 6 1 two 3&gt;&gt;&gt; frame2 = frame.set_index(['c', 'd'])&gt;&gt;&gt; frame2 a bc done 0 0 7 1 1 6 2 2 5two 0 3 4 1 4 3 2 5 2 3 6 1&gt;&gt;&gt; frame2.indexMultiIndex(levels=[['one', 'two'], [0, 1, 2, 3]], labels=[[0, 0, 0, 1, 1, 1, 1], [0, 1, 2, 0, 1, 2, 3]], names=['c', 'd'])# 默认情况下，作为索引的列会自动从DataFrame里移除，可以通过drop参数保留它们&gt;&gt;&gt; frame.set_index(['c', 'd'], drop=False) a b c dc done 0 0 7 one 0 1 1 6 one 1 2 2 5 one 2two 0 3 4 two 0 1 4 3 two 1 2 5 2 two 2 3 6 1 two 3 reset_index函数的与set_index函数相反，多层索引会被放到列里面去 123456789&gt;&gt;&gt; frame2.reset_index() c d a b0 one 0 0 71 one 1 1 62 one 2 2 53 two 0 3 44 two 1 4 35 two 2 5 26 two 3 6 1 合并数据集 数据合并操作主要有以下几个方法: pandas.merge: 基于key来合并row，类似关系型数据库的JOIN操作 pandas.concat: 在轴上合并数据，类似stack操作 combine_first: 合并重复的数据和填充确实的数据 类似数据库的DataFrame JOIN操作 如下一个“多对一”的例子，在df1里有多行值是a和b, 而在df2里面各自只有一行。 123456789101112131415161718192021222324252627282930313233&gt;&gt;&gt; df1 = pd.DataFrame(&#123;'key':['b', 'b', 'a', 'c', 'a', 'a', 'b'],... 'data1': range(7)&#125;)...&gt;&gt;&gt; df2 = pd.DataFrame(&#123;'key': ['a', 'b', 'd'],... 'data2': range(3)&#125;)...&gt;&gt;&gt; df1 data1 key0 0 b1 1 b2 2 a3 3 c4 4 a5 5 a6 6 b&gt;&gt;&gt; df2 data2 key0 0 a1 1 b2 2 d# 默认是inner Join&gt;&gt;&gt; pd.merge(df1, df2) data1 key data20 0 b 11 1 b 12 6 b 13 2 a 04 4 a 05 5 a 0 上面的例子里，我们并没有指明以哪一列作为join on的条件。merge函数默认使用列名相同的列作为key。最好的使用建议是通过on参数明确指明key信息 12345678&gt;&gt;&gt; pd.merge(df1, df2, on='key') data1 key data20 0 b 11 1 b 12 6 b 13 2 a 04 4 a 05 5 a 0 如果两个DataFrame的列名都不相同，我们也可以单独指定它们 12345678910111213141516&gt;&gt;&gt; df3 = pd.DataFrame(&#123;'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],... 'data1': range(7)&#125;)...&gt;&gt;&gt; df4 = pd.DataFrame(&#123;'rkey': ['a', 'b', 'd'],... 'data2': range(3)&#125;)...&gt;&gt;&gt; pd.merge(df3, df4, left_on='lkey', right_on='rkey') data1 lkey data2 rkey0 0 b 1 b1 1 b 1 b2 6 b 1 b3 2 a 0 a4 4 a 0 a5 5 a 0 a merge的默认方式是inner join，我们还可以指定join方式为left, right, outer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&gt;&gt;&gt; df1 data1 key0 0 b1 1 b2 2 a3 3 c4 4 a5 5 a6 6 b&gt;&gt;&gt; df2 data2 key0 0 a1 1 b2 2 d&gt;&gt;&gt; pd.merge(df1, df2, how='inner') data1 key data20 0 b 11 1 b 12 6 b 13 2 a 04 4 a 05 5 a 0&gt;&gt;&gt; pd.merge(df1, df2, how='outer') data1 key data20 0.0 b 1.01 1.0 b 1.02 6.0 b 1.03 2.0 a 0.04 4.0 a 0.05 5.0 a 0.06 3.0 c NaN7 NaN d 2.0&gt;&gt;&gt; pd.merge(df1, df2, how='left') data1 key data20 0 b 1.01 1 b 1.02 2 a 0.03 3 c NaN4 4 a 0.05 5 a 0.06 6 b 1.0&gt;&gt;&gt; pd.merge(df1, df2, how='right') data1 key data20 0.0 b 11 1.0 b 12 6.0 b 13 2.0 a 04 4.0 a 05 5.0 a 06 NaN d 2 不同的Join方式表现行为如下图: 不同的Join方式 “多对多”的方式, 默认情况下，多对多的结果是“笛卡尔乘积”。 如下: df1的key列有3个b, df2的key列有2个b。因此结果里会有3x2=6个b。 不同的join方式只是会影响结果里key的出现个数，不会影响“笛卡尔乘积”的行为 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&gt;&gt;&gt; df1 = pd.DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'b'],... 'data1': range(6)&#125;)...&gt;&gt;&gt; df2 = pd.DataFrame(&#123;'key': ['a', 'b', 'a', 'b', 'd'],... 'data2': range(5)&#125;)...&gt;&gt;&gt; df1 data1 key0 0 b1 1 b2 2 a3 3 c4 4 a5 5 b&gt;&gt;&gt; df2 data2 key0 0 a1 1 b2 2 a3 3 b4 4 d&gt;&gt;&gt; pd.merge(df1, df2, on='key', how='left') data1 key data20 0 b 1.01 0 b 3.02 1 b 1.03 1 b 3.04 2 a 0.05 2 a 2.06 3 c NaN7 4 a 0.08 4 a 2.09 5 b 1.010 5 b 3.0# 不同的join方式，结果里的key不一样，单都是笛卡尔乘积结果&gt;&gt;&gt; pd.merge(df1, df2, on='key', how='inner') data1 key data20 0 b 11 0 b 32 1 b 13 1 b 34 5 b 15 5 b 36 2 a 07 2 a 28 4 a 09 4 a 2 也可以根据多个key来进行合并操作 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; left = pd.DataFrame(&#123;'key1': ['foo', 'foo', 'bar'],... 'key2': ['one', 'two', 'one'],... 'lval': [1, 2, 3]&#125;)...&gt;&gt;&gt; right = pd.DataFrame(&#123;'key1': ['foo', 'foo', 'bar', 'bar'],... 'key2': ['one', 'one', 'one', 'two'],... 'rval': [4, 5, 6, 7]&#125;)...&gt;&gt;&gt; left key1 key2 lval0 foo one 11 foo two 22 bar one 3&gt;&gt;&gt; right key1 key2 rval0 foo one 41 foo one 52 bar one 63 bar two 7&gt;&gt;&gt; pd.merge(left, right, on=['key1', 'key2'], how='outer') key1 key2 lval rval0 foo one 1.0 4.01 foo one 1.0 5.02 foo two 2.0 NaN3 bar one 3.0 6.04 bar two NaN 7.0 哪些key会出现在结果里是根据join方式来决定的，可以把多个key的组合当做是一个元组来作为单个key的join方式考虑(尽管实现方式并不是这样) 注意: 当列和列合并时，DataFrame的索引对象会被丢弃 最后一个关于合并的问题是处理结果中的重复列名。可以通过suffixes参数指定 12345678910111213141516171819# 默认对重复的列名是自动添加了"_x", "_y"&gt;&gt;&gt; pd.merge(left, right, on='key1') key1 key2_x lval key2_y rval0 foo one 1 one 41 foo one 1 one 52 foo two 2 one 43 foo two 2 one 54 bar one 3 one 65 bar one 3 two 7&gt;&gt;&gt; pd.merge(left, right, on='key1', suffixes=('_left', '_right')) key1 key2_left lval key2_right rval0 foo one 1 one 41 foo one 1 one 52 foo two 2 one 43 foo two 2 one 54 bar one 3 one 65 bar one 3 two 7 基于索引做合并(Merging on Index) 通过设置left_index=True或right_index=True（或者两个同时设置）可以实现基于索引做合并 123456789101112131415161718192021222324252627282930313233343536&gt;&gt;&gt; left1 = pd.DataFrame(&#123;'key': ['a', 'b', 'a', 'a', 'b', 'c'],... 'value': range(6)&#125;)...&gt;&gt;&gt; right1 = pd.DataFrame(&#123;'group_val': [3.5, 7]&#125;, index=['a', 'b'])&gt;&gt;&gt; left1 key value0 a 01 b 12 a 23 a 34 b 45 c 5&gt;&gt;&gt; right1 group_vala 3.5b 7.0&gt;&gt;&gt; pd.merge(left1, right1, left_on='key', right_index=True) key value group_val0 a 0 3.52 a 2 3.53 a 3 3.51 b 1 7.04 b 4 7.0&gt;&gt;&gt; pd.merge(left1, right1, left_on='key', right_index=True, how='outer') key value group_val0 a 0 3.52 a 2 3.53 a 3 3.51 b 1 7.04 b 4 7.05 c 5 NaN 多层索引的合并会更复杂一些 12345678910111213141516171819202122232425262728293031323334353637383940414243&gt;&gt;&gt; lefth = pd.DataFrame(&#123;'key1': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],... 'key2': [2000, 2001, 2002, 2001, 2002],... 'data': np.arange(5.)&#125;)...&gt;&gt;&gt; righth = pd.DataFrame(np.arange(12).reshape((6, 2)), index=[['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', 'Ohio'], [2001, 2000, 2000, 2000, 2001, 2002]],... columns=['event1', 'event2'])...&gt;&gt;&gt; lefth data key1 key20 0.0 Ohio 20001 1.0 Ohio 20012 2.0 Ohio 20023 3.0 Nevada 20014 4.0 Nevada 2002&gt;&gt;&gt; righth event1 event2Nevada 2001 0 1 2000 2 3Ohio 2000 4 5 2000 6 7 2001 8 9 2002 10 11&gt;&gt;&gt; pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True) data key1 key2 event1 event20 0.0 Ohio 2000 4 50 0.0 Ohio 2000 6 71 1.0 Ohio 2001 8 92 2.0 Ohio 2002 10 113 3.0 Nevada 2001 0 1&gt;&gt;&gt; pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True, how='outer') data key1 key2 event1 event20 0.0 Ohio 2000 4.0 5.00 0.0 Ohio 2000 6.0 7.01 1.0 Ohio 2001 8.0 9.02 2.0 Ohio 2002 10.0 11.03 3.0 Nevada 2001 0.0 1.04 4.0 Nevada 2002 NaN NaN4 NaN Nevada 2000 2.0 3.0 同时基于两边的索引合并 123456789101112131415161718192021222324&gt;&gt;&gt; left2 = pd.DataFrame([[1., 2.], [3., 4.], [5., 6.]], index=['a', 'c', 'e'], columns=['Ohio', 'Nevada'])&gt;&gt;&gt; right2 = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]], index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama'])&gt;&gt;&gt; left2 Ohio Nevadaa 1.0 2.0c 3.0 4.0e 5.0 6.0&gt;&gt;&gt; right2 Missouri Alabamab 7.0 8.0c 9.0 10.0d 11.0 12.0e 13.0 14.0&gt;&gt;&gt; pd.merge(left2, right2, how='outer', left_index=True, right_index=True) Ohio Nevada Missouri Alabamaa 1.0 2.0 NaN NaNb NaN NaN 7.0 8.0c 3.0 4.0 9.0 10.0d NaN NaN 11.0 12.0e 5.0 6.0 13.0 14.0 另外对于基于索引合并，DataFrame还提供了一种简单的方法 1234567&gt;&gt;&gt; left2.join(right2, how='outer') Ohio Nevada Missouri Alabamaa 1.0 2.0 NaN NaNb NaN NaN 7.0 8.0c 3.0 4.0 9.0 10.0d NaN NaN 11.0 12.0e 5.0 6.0 13.0 14.0 一次join多个DataFrame 1234567891011121314&gt;&gt;&gt; another = pd.DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]], index=['a', 'c', 'e', 'f'], columns=['New York', 'Oregon'])&gt;&gt;&gt; another New York Oregona 7.0 8.0c 9.0 10.0e 11.0 12.0f 16.0 17.0&gt;&gt;&gt; left2.join([right2, another]) Ohio Nevada Missouri Alabama New York Oregona 1.0 2.0 NaN NaN 7.0 8.0c 3.0 4.0 9.0 10.0 9.0 10.0e 5.0 6.0 13.0 14.0 11.0 12.0 基于轴连接 Numpy里有concatenate函数来连接数组 12345678910111213141516171819&gt;&gt;&gt; arr= np.arange(12).reshape((3, 4))&gt;&gt;&gt; arrarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; np.concatenate([arr, arr])array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; np.concatenate([arr, arr], axis=1)array([[ 0, 1, 2, 3, 0, 1, 2, 3], [ 4, 5, 6, 7, 4, 5, 6, 7], [ 8, 9, 10, 11, 8, 9, 10, 11]]) Pandas里面, 由于轴上多了索引信息，合并也更复杂一些。首先我们来看Series的合并 12345678910111213141516171819202122232425&gt;&gt;&gt; s1 = pd.Series([0, 1], index=['a', 'b'])&gt;&gt;&gt; s2 = pd.Series([2, 3, 4], index=['c', 'd', 'e'])&gt;&gt;&gt; s3 = pd.Series([5, 6], index=['f', 'g'])&gt;&gt;&gt; pd.concat([s1, s2, s3])a 0b 1c 2d 3e 4f 5g 6dtype: int64&gt;&gt;&gt; pd.concat([s1, s2, s3], axis=1) 0 1 2a 0.0 NaN NaNb 1.0 NaN NaNc NaN 2.0 NaNd NaN 3.0 NaNe NaN 4.0 NaNf NaN NaN 5.0g NaN NaN 6.0 默认的合并方式是“outer”join, 我们也可以通过join参数指定innerjoin 1234567891011121314151617181920&gt;&gt;&gt; s4 = pd.concat([s1, s3])&gt;&gt;&gt; s4a 0b 1f 5g 6dtype: int64&gt;&gt;&gt; pd.concat([s1, s3], axis=1) 0 1a 0.0 NaNb 1.0 NaNf NaN 5.0g NaN 6.0&gt;&gt;&gt; pd.concat([s1, s4], axis=1, join='inner') 0 1a 0 0b 1 1 通过参数join_axes, 我们可以指定join的列 123456&gt;&gt;&gt; pd.concat([s1, s1, s3], axis=1, keys=['one', 'two', 'three']) one two threea 0.0 0.0 NaNb 1.0 1.0 NaNf NaN NaN 5.0g NaN NaN 6.0 通过参数keys, 可以创建多层索引 1234567891011121314151617# keys参数的值和要合并的列表一一对应&gt;&gt;&gt; result = pd.concat([s1, s1, s3], keys=['one', 'two', 'three'])&gt;&gt;&gt; resultone a 0 b 1two a 0 b 1three f 5 g 6dtype: int64&gt;&gt;&gt; result.unstack() a b f gone 0.0 1.0 NaN NaNtwo 0.0 1.0 NaN NaNthree NaN NaN 5.0 6.0 通过设置axis=1， 也可以创建列索引 1234567891011121314&gt;&gt;&gt; pd.concat([s1, s1, s3], axis=1) 0 1 2a 0.0 0.0 NaNb 1.0 1.0 NaNf NaN NaN 5.0g NaN NaN 6.0&gt;&gt;&gt; pd.concat([s1, s1, s3], axis=1, keys=['one', 'two', 'three']) one two threea 0.0 0.0 NaNb 1.0 1.0 NaNf NaN NaN 5.0g NaN NaN 6.0 同样的逻辑，可以扩展到DataFrame 123456789101112131415161718192021&gt;&gt;&gt; df1 = pd.DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'], columns=['one', 'two'])&gt;&gt;&gt; df2 = pd.DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', 'c'],columns=['three', 'four'])&gt;&gt;&gt; df1 one twoa 0 1b 2 3c 4 5&gt;&gt;&gt; df2 three foura 5 6c 7 8&gt;&gt;&gt; pd.concat([df1, df2], axis=1, keys=['level1', 'level2']) level1 level2 one two three foura 0 1 5.0 6.0b 2 3 NaN NaNc 4 5 7.0 8.0 也可以通过传递字典的方式, 字典的keys将作为keys参数。 123456&gt;&gt;&gt; pd.concat(&#123;'level1': df1, 'level2': df2&#125;, axis=1) level1 level2 one two three foura 0 1 5.0 6.0b 2 3 NaN NaNc 4 5 7.0 8.0 还可以通过names参数指定所以的名字 123456&gt;&gt;&gt; pd.concat([df1, df2], axis=1, keys=['level1', 'level2'],names=['upper', 'lower'])upper level1 level2lower one two three foura 0 1 5.0 6.0b 2 3 NaN NaNc 4 5 7.0 8.0 最后一种情况是如果两个需要合并的DataFrame行索引没有任何相关的数据，可以通过ignore_index=True来合并。 12345678910111213141516171819202122&gt;&gt;&gt; df1 = pd.DataFrame(np.random.randn(3, 4), columns=['a', 'b', 'c', 'd'])&gt;&gt;&gt; df2 = pd.DataFrame(np.random.randn(2, 3), columns=['b', 'd', 'a'])&gt;&gt;&gt; df1 a b c d0 0.440681 0.961317 1.087380 0.9066321 1.614141 -0.511330 -0.384905 0.6910802 -0.526509 0.084097 -0.068141 1.002294&gt;&gt;&gt; df2 b d a0 0.264169 -1.022511 -1.3226451 -0.698436 -0.148560 1.582804&gt;&gt;&gt; pd.concat([df1, df2], ignore_index=True) a b c d0 0.440681 0.961317 1.087380 0.9066321 1.614141 -0.511330 -0.384905 0.6910802 -0.526509 0.084097 -0.068141 1.0022943 -1.322645 0.264169 NaN -1.0225114 1.582804 -0.698436 NaN -0.148560 合并重复的数据 有一种数据结合方法既不属于merge，也不属于concatenation。比如两个数据集，index可能完全覆盖，或覆盖一部分。这里举个例子，考虑下numpy的where函数，可以在数组上进行类似于if-else表达式般的判断： 1234567891011121314151617181920212223242526&gt;&gt;&gt; a = pd.Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan], index=['f', 'e', 'd', 'c', 'b', 'a'])&gt;&gt;&gt; b = pd.Series(np.arange(len(a), dtype=np.float64), index=['f', 'e', 'd', 'c', 'b', 'a'])&gt;&gt;&gt; b[-1] = np.nan&gt;&gt;&gt; af NaNe 2.5d NaNc 3.5b 4.5a NaNdtype: float64&gt;&gt;&gt; bf 0.0e 1.0d 2.0c 3.0b 4.0a NaNdtype: float64&gt;&gt;&gt; np.where(pd.isnull(a), b, a)array([ 0. , 2.5, 2. , 3.5, 4.5, nan]) Series的combine_first函数可以达到类似的效果 1234567891011121314151617181920212223&gt;&gt;&gt; b[:-2]f 0.0e 1.0d 2.0c 3.0dtype: float64&gt;&gt;&gt; a[2:]d NaNc 3.5b 4.5a NaNdtype: float64# b中有值时就用b中的，否则就用a的，index是两个Series的 outer join&gt;&gt;&gt; b[:-2].combine_first(a[2:])a NaNb 4.5c 3.0d 2.0e 1.0f 0.0dtype: float64 DataFrame的combine_first函数也有同样的功能，在DataFrame调用combine_first时，可以认为把传递给combine_first函数的DataFrame作为调用该函数的Dataframe缺失数据的补充。 1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; df1 = pd.DataFrame(&#123;'a': [1., np.nan, 5., np.nan],... 'b': [np.nan, 2., np.nan, 6.],... 'c': range(2, 18, 4)&#125;)...&gt;&gt;&gt; df2 = pd.DataFrame(&#123;'a': [5., 4., np.nan, 3., 7.],... 'b': [np.nan, 3., 4., 6., 8.]&#125;)...&gt;&gt;&gt; df1 a b c0 1.0 NaN 21 NaN 2.0 62 5.0 NaN 103 NaN 6.0 14&gt;&gt;&gt; df2 a b0 5.0 NaN1 4.0 3.02 NaN 4.03 3.0 6.04 7.0 8.0# df1中缺失的数据，使用df2中的补上&gt;&gt;&gt; df1.combine_first(df2) a b c0 1.0 NaN 2.01 4.0 2.0 6.02 5.0 4.0 10.03 3.0 6.0 14.04 7.0 8.0 NaN 变型和旋转(Reshaping and Pivoting) 多层索引变型 在DataFrame中，主要有两个方法用于多层索引变型 stack: 把列转换为行 unstack: 把行转换为列 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; data = pd.DataFrame(np.arange(6).reshape((2, 3)),... index=pd.Index(['Ohio', 'Colorado'], name='state'),... columns=pd.Index(['one', 'two', 'three'], name='number'))...&gt;&gt;&gt; datanumber one two threestateOhio 0 1 2Colorado 3 4 5# 把列转换为行&gt;&gt;&gt; result = data.stack()&gt;&gt;&gt; resultstate numberOhio one 0 two 1 three 2Colorado one 3 two 4 three 5dtype: int64# 把行转换为列&gt;&gt;&gt; result.unstack()number one two threestateOhio 0 1 2Colorado 3 4 5 默认情况下unstack是从最里层的列被转换为行(stack函数也类似)。也可以通过层的索引和名字来。 1234567891011121314151617181920&gt;&gt;&gt; result.indexMultiIndex(levels=[['Ohio', 'Colorado'], ['one', 'two', 'three']], labels=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]], names=['state', 'number'])# 0对应Index levels的索引&gt;&gt;&gt; result.unstack(0)state Ohio Coloradonumberone 0 3two 1 4three 2 5# state对应Index name&gt;&gt;&gt; result.unstack('state')state Ohio Coloradonumberone 0 3two 1 4three 2 5 unstack的过程中，也可能会引入一些缺失值 1234567891011121314151617181920&gt;&gt;&gt; s1 = pd.Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])&gt;&gt;&gt; s2 = pd.Series([4, 5, 6], index=['c', 'd', 'e'])&gt;&gt;&gt; data2 = pd.concat([s1, s2], keys=['one', 'two'])&gt;&gt;&gt; data2one a 0 b 1 c 2 d 3two c 4 d 5 e 6dtype: int64&gt;&gt;&gt; data2.unstack() a b c d eone 0.0 1.0 2.0 3.0 NaNtwo NaN NaN 4.0 5.0 6.0 stack函数默认会过滤掉NaN数据 12345678910111213141516171819202122&gt;&gt;&gt; data2.unstack().stack()one a 0.0 b 1.0 c 2.0 d 3.0two c 4.0 d 5.0 e 6.0dtype: float64&gt;&gt;&gt; data2.unstack().stack(dropna=False)one a 0.0 b 1.0 c 2.0 d 3.0 e NaNtwo a NaN b NaN c 4.0 d 5.0 e 6.0dtype: float64 当unstack作用于DataFrame时，则是从最低层开始。 123456789101112131415161718192021222324252627282930313233343536373839&gt;&gt;&gt; df = pd.DataFrame(&#123;'left': result, 'right': result + 5&#125;,... columns=pd.Index(['left', 'right'], name='side'))...&gt;&gt;&gt; dfside left rightstate numberOhio one 0 5 two 1 6 three 2 7Colorado one 3 8 two 4 9 three 5 10&gt;&gt;&gt; df.unstack()side left rightnumber one two three one two threestateOhio 0 1 2 5 6 7Colorado 3 4 5 8 9 10# 也可以通过索引名来指定要操作的列&gt;&gt;&gt; df.unstack('state')side left rightstate Ohio Colorado Ohio Coloradonumberone 0 3 5 8two 1 4 6 9three 2 5 7 10&gt;&gt;&gt; df.unstack('state').stack('side')state Colorado Ohionumber sideone left 3 0 right 8 5two left 4 1 right 9 6three left 5 2 right 10 7 把“长”格式转换为“宽”格式(Pivoting “Long” to “Wide” Format) 一种常见的把多个时间序列保存在数据库或csv文件的方式叫作long or stacked format。 让我们来看一些例子(例子里使用的文件可以从这里下载) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&gt;&gt;&gt; data = pd.read_csv('examples/macrodata.csv')&gt;&gt;&gt; data.head() year quarter realgdp realcons realinv realgovt realdpi cpi \0 1959.0 1.0 2710.349 1707.4 286.898 470.045 1886.9 28.981 1959.0 2.0 2778.801 1733.7 310.859 481.301 1919.7 29.152 1959.0 3.0 2775.488 1751.8 289.226 491.260 1916.4 29.353 1959.0 4.0 2785.204 1753.7 299.356 484.052 1931.3 29.374 1960.0 1.0 2847.699 1770.5 331.722 462.199 1955.5 29.54 m1 tbilrate unemp pop infl realint0 139.7 2.82 5.8 177.146 0.00 0.001 141.7 3.08 5.1 177.830 2.34 0.742 140.5 3.82 5.3 178.657 2.74 1.093 140.0 4.33 5.6 179.386 0.27 4.064 139.6 3.50 5.2 180.007 2.31 1.19&gt;&gt;&gt; periods = pd.PeriodIndex(year=data.year, quarter=data.quarter, name='date')&gt;&gt;&gt; columns = pd.Index(['realgdp', 'infl', 'unemp'], name='item')&gt;&gt;&gt; data = data.reindex(columns=columns)&gt;&gt;&gt; data.head()item realgdp infl unemp0 2710.349 0.00 5.81 2778.801 2.34 5.12 2775.488 2.74 5.33 2785.204 0.27 5.64 2847.699 2.31 5.2&gt;&gt;&gt; data.index = periods.to_timestamp('D', 'end')&gt;&gt;&gt; data.head()item realgdp infl unempdate1959-03-31 2710.349 0.00 5.81959-06-30 2778.801 2.34 5.11959-09-30 2775.488 2.74 5.31959-12-31 2785.204 0.27 5.61960-03-31 2847.699 2.31 5.2&gt;&gt;&gt; data.stack().head()date item1959-03-31 realgdp 2710.349 infl 0.000 unemp 5.8001959-06-30 realgdp 2778.801 infl 2.340dtype: float64&gt;&gt;&gt; data.stack().head().reset_index() date item 00 1959-03-31 realgdp 2710.3491 1959-03-31 infl 0.0002 1959-03-31 unemp 5.8003 1959-06-30 realgdp 2778.8014 1959-06-30 infl 2.340&gt;&gt;&gt; data.stack().head().reset_index().rename(columns=&#123;0: 'value'&#125;) date item value0 1959-03-31 realgdp 2710.3491 1959-03-31 infl 0.0002 1959-03-31 unemp 5.8003 1959-06-30 realgdp 2778.8014 1959-06-30 infl 2.340&gt;&gt;&gt; ldata[:10] date item value0 1959-03-31 realgdp 2710.3491 1959-03-31 infl 0.0002 1959-03-31 unemp 5.8003 1959-06-30 realgdp 2778.8014 1959-06-30 infl 2.3405 1959-06-30 unemp 5.1006 1959-09-30 realgdp 2775.4887 1959-09-30 infl 2.7408 1959-09-30 unemp 5.3009 1959-12-31 realgdp 2785.204 上面这种ldata的格式就被称作long format for multiple time series 12345678910&gt;&gt;&gt; pivoted = ldata.pivot('date', 'item', 'value')&gt;&gt;&gt; pivoted.head()item infl realgdp unempdate1959-03-31 0.00 2710.349 5.81959-06-30 2.34 2778.801 5.11959-09-30 2.74 2775.488 5.31959-12-31 0.27 2785.204 5.61960-03-31 2.31 2847.699 5.2 把“宽”格式转换为“长”格式(Pivoting “Long” to “Wide” Format) 与pivot相反的操作就是pandas.melt 12345678910111213141516171819202122232425&gt;&gt;&gt; df = pd.DataFrame(&#123;'key': ['foo', 'bar', 'baz'],... 'A': [1, 2, 3],... 'B': [4, 5, 6],... 'C': [7, 8, 9]&#125;)...&gt;&gt;&gt; df A B C key0 1 4 7 foo1 2 5 8 bar2 3 6 9 baz&gt;&gt;&gt; melted = pd.melt(df, ['key'])&gt;&gt;&gt; melted key variable value0 foo A 11 bar A 22 baz A 33 foo B 44 bar B 55 baz B 66 foo C 77 bar C 88 baz C 9]]></content>
      <categories>
        <category>python数据分析</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《利用Python进行数据分析第二版》读书笔记二]]></title>
    <url>%2F2017%2F12%2F28%2Fpython-for-data-analysis-2nd-edition-note-two%2F</url>
    <content type="text"><![CDATA[数据清洗和预处理 过滤出缺失的数据 Series 12345678910111213141516&gt;&gt;&gt; from numpy import nan as NA&gt;&gt;&gt; data = pd.Series([1, NA, 3.5, NA, 7])&gt;&gt;&gt; data.dropna()0 1.02 3.54 7.0dtype: float64# 等价于上面的dropna&gt;&gt;&gt; data[data.notnull()]0 1.02 3.54 7.0dtype: float64 DataFrame 1234567891011121314&gt;&gt;&gt; data = pd.DataFrame([[1., 6.5, 3.], [1., NA, NA], [NA, NA, NA], [NA, 6.5, 3.]])&gt;&gt;&gt; cleaned = data.dropna()&gt;&gt;&gt; data 0 1 20 1.0 6.5 3.01 1.0 NaN NaN2 NaN NaN NaN3 NaN 6.5 3.0&gt;&gt;&gt; cleaned 0 1 20 1.0 6.5 3.0 过滤至少包含几个值的数据 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(7, 3))&gt;&gt;&gt; df.iloc[:4, 1] = NA&gt;&gt;&gt; df.iloc[:2, 2] = NA&gt;&gt;&gt; df 0 1 20 0.006690 NaN NaN1 0.850493 NaN NaN2 -0.601578 NaN -0.4535663 -0.819006 NaN -0.1761614 -0.440714 -0.700860 -1.2668245 -1.347181 1.561944 -0.2032586 3.195225 1.604114 -1.243471&gt;&gt;&gt; df.dropna() 0 1 24 -0.440714 -0.700860 -1.2668245 -1.347181 1.561944 -0.2032586 3.195225 1.604114 -1.243471# 保留至少有2个为非NaN值的的行&gt;&gt;&gt; df.dropna(thresh=2) 0 1 22 -0.601578 NaN -0.4535663 -0.819006 NaN -0.1761614 -0.440714 -0.700860 -1.2668245 -1.347181 1.561944 -0.2032586 3.195225 1.604114 -1.243471 填充缺失的数据 补充缺失的值为0 12345678910111213141516171819&gt;&gt;&gt; df 0 1 20 0.006690 NaN NaN1 0.850493 NaN NaN2 -0.601578 NaN -0.4535663 -0.819006 NaN -0.1761614 -0.440714 -0.700860 -1.2668245 -1.347181 1.561944 -0.2032586 3.195225 1.604114 -1.243471&gt;&gt;&gt; df.fillna(0) 0 1 20 0.006690 0.000000 0.0000001 0.850493 0.000000 0.0000002 -0.601578 0.000000 -0.4535663 -0.819006 0.000000 -0.1761614 -0.440714 -0.700860 -1.2668245 -1.347181 1.561944 -0.2032586 3.195225 1.604114 -1.243471 按照指定的列填充缺失的值。第1列为NaN的填充为0.5，第2列为NaN的填充为0 123456789&gt;&gt;&gt; df.fillna(&#123;1: 0.5, 2: 0&#125;) 0 1 20 0.006690 0.500000 0.0000001 0.850493 0.500000 0.0000002 -0.601578 0.500000 -0.4535663 -0.819006 0.500000 -0.1761614 -0.440714 -0.700860 -1.2668245 -1.347181 1.561944 -0.2032586 3.195225 1.604114 -1.243471 默认情况下，fillna都是返回一个新的对象，使用inplace参数可以直接修改原来的DataFame 1234567891011&gt;&gt;&gt; _ = df.fillna(0, inplace=True)&gt;&gt;&gt; df 0 1 20 0.006690 0.000000 0.0000001 0.850493 0.000000 0.0000002 -0.601578 0.000000 -0.4535663 -0.819006 0.000000 -0.1761614 -0.440714 -0.700860 -1.2668245 -1.347181 1.561944 -0.2032586 3.195225 1.604114 -1.243471 去掉重复的行 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&gt;&gt;&gt; data = pd.DataFrame(&#123;'k1': ['one', 'two'] * 3 + ['two'],... 'k2': [1, 1, 2, 3, 3, 4, 4]&#125;)...&gt;&gt;&gt; data k1 k20 one 11 two 12 one 23 two 34 one 35 two 46 two 4# 重复的行返回为True&gt;&gt;&gt; data.duplicated()0 False1 False2 False3 False4 False5 False6 Truedtype: bool# 去掉重复的行(即data.duplicated()为True的行)&gt;&gt;&gt; data.drop_duplicates() k1 k20 one 11 two 12 one 23 two 34 one 35 two 4&gt;&gt;&gt; data['v1'] = range(7)&gt;&gt;&gt; data k1 k2 v10 one 1 01 two 1 12 one 2 23 two 3 34 one 3 45 two 4 56 two 4 6# 去掉k1列重复的行&gt;&gt;&gt; data.drop_duplicates(['k1']) k1 k2 v10 one 1 01 two 1 1# 默认情况下，遇到重复的值时，只会保留第一个出现的，可以通过keep参数保留最后一次出现的行&gt;&gt;&gt; data.drop_duplicates(['k1', 'k2'], keep='last') k1 k2 v10 one 1 01 two 1 12 one 2 23 two 3 34 one 3 46 two 4 6 替换值(Replace) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&gt;&gt;&gt; data = pd.Series([1., -999., 2., -999., -1000., 3.])&gt;&gt;&gt; data0 1.01 -999.02 2.03 -999.04 -1000.05 3.0dtype: float64# 将-999替换成nan&gt;&gt;&gt; data.replace(-999, np.nan)0 1.01 NaN2 2.03 NaN4 -1000.05 3.0dtype: float64# 将多个值-999和-1000替换成nan&gt;&gt;&gt; data.replace([-999, -1000], np.nan)0 1.01 NaN2 2.03 NaN4 NaN5 3.0dtype: float64# 将-999替换成nan， -1000替换成0&gt;&gt;&gt; data.replace([-999, -1000], [np.nan, 0])0 1.01 NaN2 2.03 NaN4 0.05 3.0dtype: float64# 效果同上&gt;&gt;&gt; data.replace(&#123;-999: np.nan, -1000: 0&#125;)0 1.01 NaN2 2.03 NaN4 0.05 3.0dtype: float64 重命名轴的索引(Renaming Axis Indexes) 123456789101112131415161718192021222324252627282930313233343536373839&gt;&gt;&gt; data = pd.DataFrame(np.arange(12).reshape((3, 4)),... index=['Ohio', 'Colorado', 'New York'],... columns=['one', 'two', 'three', 'four'])...&gt;&gt;&gt; transform = lambda x: x[:4].upper()# map方法返回一个新的Index类型，不会修改原来的值&gt;&gt;&gt; data.index.map(transform)Index(['OHIO', 'COLO', 'NEW '], dtype='object')&gt;&gt;&gt; data one two three fourOhio 0 1 2 3Colorado 4 5 6 7New York 8 9 10 11# 可以重新赋值index达到replace的效果&gt;&gt;&gt; data.index = data.index.map(transform)&gt;&gt;&gt; data one two three fourOHIO 0 1 2 3COLO 4 5 6 7NEW 8 9 10 11# rename方法也可以接受函数&gt;&gt;&gt; data.rename(index=str.title, columns=str.upper) ONE TWO THREE FOUROhio 0 1 2 3Colo 4 5 6 7New 8 9 10 11# 同样rename方法可以接受字典&gt;&gt;&gt; data.rename(index=&#123;'OHIO': 'INDIANA'&#125;,columns=&#123;'three': 'peekaboo'&#125;) one two peekaboo fourINDIANA 0 1 2 3COLO 4 5 6 7NEW 8 9 10 11 离散和装箱 考虑这样一个场景，我们有一些关于人的年龄的数据，现在需要根据年龄大小来分组，如: 18-25岁，26岁到35岁， 36岁到60岁，61岁以上来分组，可以这样实现 12345678910111213141516171819202122232425262728&gt;&gt;&gt; ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]&gt;&gt;&gt; bins = [18, 25, 35, 60, 100]# 使用cut方法装箱&gt;&gt;&gt; cats = pd.cut(ages, bins)&gt;&gt;&gt; cats[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]Length: 12Categories (4, interval[int64]): [(18, 25] &lt; (25, 35] &lt; (35, 60] &lt; (60, 100]]&gt;&gt;&gt; cats.codesarray([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)&gt;&gt;&gt; cats.categoriesIntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]] closed='right', dtype='interval[int64]')# 统计每个年龄段的人数&gt;&gt;&gt; pd.value_counts(cats)(18, 25] 5(35, 60] 3(25, 35] 3(60, 100] 1dtype: int64 默认情况下括号(18, 25]左开右闭，表示&gt;18 并且 &lt;=25，可以指定括号的开闭情况 1234&gt;&gt;&gt; pd.cut(ages, bins, right=False)[[18, 25), [18, 25), [25, 35), [25, 35), [18, 25), ..., [25, 35), [60, 100), [35, 60), [35, 60), [25, 35)]Length: 12Categories (4, interval[int64]): [[18, 25) &lt; [25, 35) &lt; [35, 60) &lt; [60, 100)] 我们也可以指定每个箱子的名字 123456&gt;&gt;&gt; group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']&gt;&gt;&gt; pd.cut(ages, bins, labels=group_names)[Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult]Length: 12Categories (4, object): [Youth &lt; YoungAdult &lt; MiddleAged &lt; Senior] 在调用cut函数，指定bins参数时，如果传递是一个数字n而不是一个区间，那么默认会将Searies按照从小到到的顺序自动分成n个区间。 12345678910111213&gt;&gt;&gt; data = np.random.rand(20)&gt;&gt;&gt; dataarray([ 0.18918431, 0.84853486, 0.03490019, 0.99371555, 0.72304736, 0.3356213 , 0.40800301, 0.71508546, 0.45061796, 0.28337607, 0.31318716, 0.86330303, 0.88466783, 0.22120597, 0.25290735, 0.82730126, 0.14255095, 0.36400834, 0.75004135, 0.1090306 ])#precision表示小数展示的位数&gt;&gt;&gt; pd.cut(data, 4, precision=2)[(0.034, 0.27], (0.75, 0.99], (0.034, 0.27], (0.75, 0.99], (0.51, 0.75], ..., (0.75, 0.99], (0.034, 0.27], (0.27, 0.51], (0.51, 0.75], (0.034, 0.27]]Length: 20Categories (4, interval[float64]): [(0.034, 0.27] &lt; (0.27, 0.51] &lt; (0.51, 0.75] &lt; (0.75, 0.99]] 一个跟cut类似的办法是qcut,使用cut方法，通常无法保证每个装箱的元素个数是相同的，使用qcut可以按照百分比来分配元素 1234567891011121314151617&gt;&gt;&gt; data = np.random.randn(1000)&gt;&gt;&gt; cats = pd.qcut(data, 4)&gt;&gt;&gt; cats[(0.72, 3.328], (-0.632, 0.0365], (0.72, 3.328], (-3.237, -0.632], (-0.632, 0.0365], ..., (0.0365, 0.72], (0.0365, 0.72], (-0.632, 0.0365], (-0.632, 0.0365], (0.0365, 0.72]]Length: 1000Categories (4, interval[float64]): [(-3.237, -0.632] &lt; (-0.632, 0.0365] &lt; (0.0365, 0.72] &lt; (0.72, 3.328]]# 可以看到每个装箱的元素个数是一致的&gt;&gt;&gt; pd.value_counts(cats)(0.72, 3.328] 250(0.0365, 0.72] 250(-0.632, 0.0365] 250(-3.237, -0.632] 250dtype: int64 同样，我们可以指定每个装箱的百分比 123456789101112131415# 累进的百分比10%, 40%, 40%, 10%&gt;&gt;&gt; cats=pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])&gt;&gt;&gt; cats[(0.0365, 1.271], (-1.215, 0.0365], (0.0365, 1.271], (-1.215, 0.0365], (-1.215, 0.0365], ..., (0.0365, 1.271], (0.0365, 1.271], (-1.215, 0.0365], (-1.215, 0.0365], (0.0365, 1.271]]Length: 1000Categories (4, interval[float64]): [(-3.237, -1.215] &lt; (-1.215, 0.0365] &lt; (0.0365, 1.271] &lt; (1.271, 3.328]]&gt;&gt;&gt; pd.value_counts(cats)(0.0365, 1.271] 400(-1.215, 0.0365] 400(1.271, 3.328] 100(-3.237, -1.215] 100dtype: int64 检查和过滤异常值 12345678910111213141516171819202122232425262728293031323334353637383940&gt;&gt;&gt; data = pd.DataFrame(np.random.randn(1000, 4))&gt;&gt;&gt; data.describe() 0 1 2 3count 1000.000000 1000.000000 1000.000000 1000.000000mean 0.034294 -0.023318 0.009570 0.067062std 1.001493 0.971333 0.996241 0.959285min -2.937180 -3.697901 -3.195441 -3.01206325% -0.673629 -0.654237 -0.641570 -0.56736350% 0.030598 -0.004002 0.017635 0.09339475% 0.691715 0.636873 0.659011 0.697847max 3.736384 3.272939 3.114762 3.311736# 找出第3列绝对值大于3的行&gt;&gt;&gt; col = data[2]&gt;&gt;&gt; col[np.abs(col) &gt; 3]219 -3.195441980 3.114762Name: 2, dtype: float64# 找出所有包含绝对值大于3的行&gt;&gt;&gt; data[(np.abs(data) &gt; 3).any(1)] 0 1 2 321 1.082695 1.455599 0.740997 3.053146205 3.736384 -0.109024 0.252715 1.640695219 -1.513811 1.117404 -3.195441 0.975343# sign函数根据元素的值返回# 小于0 返回 -1# 等于0 返回 0# 大于0 返回1# NaN 返回 NaN&gt;&gt;&gt; np.sign(data).head() 0 1 2 30 1.0 -1.0 1.0 1.01 1.0 1.0 1.0 1.02 1.0 1.0 -1.0 1.03 1.0 1.0 1.0 1.04 1.0 -1.0 1.0 -1.0 随机排序和随机取样 使用numpy.random.permutation函数，很容易实现对Series和DataFrame的随机排序 1234567891011121314151617181920212223242526# 初始化一个DataFrame&gt;&gt;&gt; df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4)))# df的索引时0至4按顺序排列的&gt;&gt;&gt; df 0 1 2 30 0 1 2 31 4 5 6 72 8 9 10 113 12 13 14 154 16 17 18 19# 生成一个随机的索引序列&gt;&gt;&gt; sampler = np.random.permutation(5)&gt;&gt;&gt; samplerarray([2, 4, 3, 0, 1])# 使用新的索引序列&gt;&gt;&gt; df.take(sampler) 0 1 2 32 8 9 10 114 16 17 18 193 12 13 14 150 0 1 2 31 4 5 6 7 随机返回一些行的数据 123456# 随机返回3行数据&gt;&gt;&gt; df.sample(3) 0 1 2 30 0 1 2 32 8 9 10 114 16 17 18 19 使用replace参数可以允许重复取样 1234567891011121314151617&gt;&gt;&gt; choices = pd.Series([5, 7, -1, 6, 4])# 随机从Series中取10个元素，允许重复使用&gt;&gt;&gt; draws = choices.sample(n=10, replace=True)&gt;&gt;&gt; draws4 44 40 52 -10 54 41 74 41 70 5dtype: int64 计算指标/虚拟变量(Computing Indicator/Dummy Variables) 定义参考: http://wiki.mbalib.com/wiki/%E8%99%9A%E6%8B%9F%E5%8F%98%E9%87%8F 一种常用于统计建模或机器学习的转换方式是: 将分类变量转换为“哑变量矩阵”或“指标矩阵”。如果 DataFrame 的某一列中含有 k 个 不同的值，则可以派生出一个 k 列矩阵或 DataFrame（其值全为1和0）。pandas 有一个 get_dummies 函数可以实现该功能。 123456789101112131415161718192021&gt;&gt;&gt; df = pd.DataFrame(&#123;'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)&#125;... )&gt;&gt;&gt; df data1 key0 0 b1 1 b2 2 a3 3 c4 4 a5 5 b# 指定列元素值根据是否出现，分别赋值为0和1&gt;&gt;&gt; pd.get_dummies(df['key']) a b c0 0 1 01 0 1 02 1 0 03 0 0 14 1 0 05 0 1 0 也可以指定一个前缀 12345678910&gt;&gt;&gt; dummies = pd.get_dummies(df['key'], prefix='key')&gt;&gt;&gt; dummies key_a key_b key_c0 0 1 01 0 1 02 1 0 03 0 0 14 1 0 05 0 1 0 另外一种常用的方法是和离散函数cut结合 123456789101112131415161718192021222324&gt;&gt;&gt; np.random.seed(12345)&gt;&gt;&gt; values = np.random.rand(10)&gt;&gt;&gt; valuesarray([ 0.92961609, 0.31637555, 0.18391881, 0.20456028, 0.56772503, 0.5955447 , 0.96451452, 0.6531771 , 0.74890664, 0.65356987])&gt;&gt;&gt; bins = [0, 0.2, 0.4, 0.6, 0.8, 1]# 统计了每个索引值在哪个区间# 索引为0的0.92961609，出现在区间(0.8, 1.0]&gt;&gt;&gt; pd.get_dummies(pd.cut(values, bins)) (0.0, 0.2] (0.2, 0.4] (0.4, 0.6] (0.6, 0.8] (0.8, 1.0]0 0 0 0 0 11 0 1 0 0 02 1 0 0 0 03 0 1 0 0 04 0 0 1 0 05 0 0 1 0 06 0 0 0 0 17 0 0 0 1 08 0 0 0 1 09 0 0 0 1 0]]></content>
      <categories>
        <category>python数据分析</category>
      </categories>
      <tags>
        <tag>numpy</tag>
        <tag>matplotlib</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《利用Python进行数据分析第二版》读书笔记一]]></title>
    <url>%2F2017%2F12%2F26%2Fpython-for-data-analysis-2nd-edition-note-one%2F</url>
    <content type="text"><![CDATA[Numpy基础 更多关于NumPy的使用, 可以参考NumPy快速入门指南 Numpy数据类型 Numpy DataTypes np.where np.where是一个向量版本的三元表达式x if c else y。假如我们有三个数组xarr, yarr, cond。 当cond为真时，取xarr的值，否则取yarr的值。我们可以这样做: 12345678910&gt;&gt;&gt; xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])&gt;&gt;&gt; yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])&gt;&gt;&gt; cond = np.array([True, False, True, True, False])&gt;&gt;&gt; result = [(x if c else y) for x, y, c in zip(xarr, yarr, cond)]&gt;&gt;&gt; result[1.1000000000000001, 2.2000000000000002, 1.3, 1.3999999999999999, 2.5] 但是上面这种做法是纯python的做法，效率很慢，而且无法用于多维数组。用np.where就很简单了。 1234&gt;&gt;&gt; result = np.where(cond, xarr, yarr)&gt;&gt;&gt; resultarray([ 1.1, 2.2, 1.3, 1.4, 2.5]) np.where的第二个和第三个参数，除了数组类型之外，也可以是标量 123456789101112131415161718192021222324252627&gt;&gt;&gt; arr = np.random.randn(4, 4)&gt;&gt;&gt; arrarray([[-0.16323877, -0.65162139, -1.53392306, -0.84839303], [ 0.67860968, 1.02998872, -1.66306679, -1.23551981], [ 0.66004673, -1.13882878, -0.90901002, -1.11414436], [ 0.24704485, -0.87881313, -1.06539659, -0.82992508]])&gt;&gt;&gt; arr &gt; 0array([[False, False, False, False], [ True, True, False, False], [ True, False, False, False], [ True, False, False, False]], dtype=bool)# 将数组中&gt;0的全部替换为2，小于0的全部用-2替换&gt;&gt;&gt; np.where(arr &gt; 0, 2, -2)array([[-2, -2, -2, -2], [ 2, 2, -2, -2], [ 2, -2, -2, -2], [ 2, -2, -2, -2]])# 将数组中&gt;0的全部替换为2，小于0的不变&gt;&gt;&gt; np.where(arr &gt; 0, 2, arr)array([[-0.16323877, -0.65162139, -1.53392306, -0.84839303], [ 2. , 2. , -1.66306679, -1.23551981], [ 2. , -1.13882878, -0.90901002, -1.11414436], [ 2. , -0.87881313, -1.06539659, -0.82992508]]) 布尔数组 当为True时，赋值为1，为False时，赋值为0。利用这个特性，很容易计算出一个数组里满足特定条件的元素个数。 123456789&gt;&gt;&gt; arr = np.random.randn(100)# 统计大于0的元素个数&gt;&gt;&gt; (arr &gt; 0).sum()48# 统计小于0的元素个数&gt;&gt;&gt; (arr &lt; 0).sum()52 any函数用于检查数组中有一个元素或多个元素的值为True。all函数用于检查数组中所有的元素是否为True。 1234567&gt;&gt;&gt; bools = np.array([False, False, True, False])&gt;&gt;&gt; bools.any()True&gt;&gt;&gt; bools.all()False 数组排序 就跟python内置的sort函数一样，Numpy提供了排序功能 1234567891011&gt;&gt;&gt; arr = np.random.randn(6)&gt;&gt;&gt; arrarray([ 0.45462395, 1.89881592, 0.23095329, -0.15011573, -0.50788654, -0.19426878])&gt;&gt;&gt; arr.sort()&gt;&gt;&gt; arrarray([-0.50788654, -0.19426878, -0.15011573, 0.23095329, 0.45462395, 1.89881592]) 也可以在指定轴排序 12345678910111213141516171819202122232425262728&gt;&gt;&gt; arr = np.random.randn(5, 3)&gt;&gt;&gt; arrarray([[ 0.85243833, -0.20601835, 0.42260075], [ 0.0022698 , 0.69309103, -0.6865517 ], [ 0.17471696, 0.77361444, -0.25720617], [ 0.83807922, 0.43469269, -0.18505689], [-0.19003894, 0.29031477, 1.68124349]])# 在列方向排序&gt;&gt;&gt; arr.sort(axis=0)&gt;&gt;&gt; arrarray([[-0.19003894, -0.20601835, -0.6865517 ], [ 0.0022698 , 0.29031477, -0.25720617], [ 0.17471696, 0.43469269, -0.18505689], [ 0.83807922, 0.69309103, 0.42260075], [ 0.85243833, 0.77361444, 1.68124349]])# 在行方向排序&gt;&gt;&gt; arr.sort(axis=1)&gt;&gt;&gt; arrarray([[-0.6865517 , -0.20601835, -0.19003894], [-0.25720617, 0.0022698 , 0.29031477], [-0.18505689, 0.17471696, 0.43469269], [ 0.42260075, 0.69309103, 0.83807922], [ 0.77361444, 0.85243833, 1.68124349]]) 唯一性和in操作 Numpy专门为一维数组提供了一些函数 唯一性（unique) 12345678910&gt;&gt;&gt; names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])&gt;&gt;&gt; np.unique(names)array(['Bob', 'Joe', 'Will'], dtype='&lt;U4')&gt;&gt;&gt; ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])&gt;&gt;&gt; np.unique(ints)array([1, 2, 3, 4]) np.in1d 12345&gt;&gt;&gt; values = np.array([6, 0, 0, 3, 2, 5, 6])# 检查一个数组的值是否在另一个数组里，返回一个布尔数组&gt;&gt;&gt; np.in1d(values, [2, 3, 6])array([ True, False, False, True, True, False, True], dtype=bool) 文件输入输出 numpy可以使用save和load方法来输出和加载。默认情况下，是以未压缩的二进制格式保存，并会自动加上后缀扩展.npy 1234567891011&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; arr = np.arange(10)&gt;&gt;&gt; arrarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; np.save('some_array', arr)&gt;&gt;&gt; np.load('some_array.npy')array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 使用savez可以一次以未压缩的二进制格式保存多个数组 1234567891011121314&gt;&gt;&gt; xarr = np.arange(10)&gt;&gt;&gt; barr = np.arange(5)# 使用a,b参数分别保存xarr和barr&gt;&gt;&gt; np.savez('array_archive.npz', a=xarr, b=barr)&gt;&gt;&gt; arch = np.load('array_archive.npz')&gt;&gt;&gt; arch['a']array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; arch['b']array([0, 1, 2, 3, 4]) 以压缩的格式保存数组 1&gt;&gt;&gt; np.savez_compressed('arrays_compressed.npz', a=arr, b=arr) 矩阵乘法的另一种表示方法 在python3.5中, 可以使用@来表示矩阵乘法, 跟dot函数效果一样 1234567&gt;&gt;&gt; x = np.array([[1., 2., 3.], [4., 5., 6.]])&gt;&gt;&gt; np.dot(x, np.ones(3))array([ 6., 15.])&gt;&gt;&gt; x @ np.ones(3)array([ 6., 15.]) 线性代数(Linear Algebra) numpy.linalg 模块包含类线性代数常用的操作 Pandas基础 更多关于Pandas的使用, 可以参考10分钟入门Pandas Series对象和它的index都有一个name属性 1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; s = pd.Series(np.arange(10))&gt;&gt;&gt; s0 01 12 23 34 45 56 67 78 89 9dtype: int64&gt;&gt;&gt; s.name = 'IntSeries'&gt;&gt;&gt; s.index.name = 'IntSeriesIndex'&gt;&gt;&gt; sIntSeriesIndex0 01 12 23 34 45 56 67 78 89 9Name: IntSeries, dtype: int64 列操作 12345678910111213141516171819202122232425&gt;&gt;&gt; data = pd.DataFrame(np.arange(12).reshape(3,4), columns=['a', 'b', 'c', 'd'])&gt;&gt;&gt; data a b c d0 0 1 2 31 4 5 6 72 8 9 10 11# 添加列&gt;&gt;&gt; data['e']= 5&gt;&gt;&gt; data a b c d e0 0 1 2 3 51 4 5 6 7 52 8 9 10 11 5# 删除列&gt;&gt;&gt; del data['a']&gt;&gt;&gt; data b c d e0 1 2 3 51 5 6 7 52 9 10 11 5 DataFrame的index和column也可以设置name属性 123456789101112131415161718&gt;&gt;&gt; data = pd.DataFrame(np.arange(12).reshape(3,4), columns=['a', 'b', 'c', 'd'])&gt;&gt;&gt; data a b c d0 0 1 2 31 4 5 6 72 8 9 10 11&gt;&gt;&gt; data.index.name = 'index_name'&gt;&gt;&gt; data.columns.name = 'columns_name'&gt;&gt;&gt; datacolumns_name a b c dindex_name0 0 1 2 31 4 5 6 72 8 9 10 11 Reindexing Series和DataFrame对象可以使用reindex方法来修改行索引和列索引 1234567891011121314151617181920212223242526272829303132333435363738394041424344# Series reindex&gt;&gt;&gt; obj = pd.Series(range(3), index=['a', 'b', 'c'])&gt;&gt;&gt; obja 0b 1c 2dtype: int64&gt;&gt;&gt; obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])&gt;&gt;&gt; obj2a 0.0b 1.0c 2.0d NaNe NaNdtype: float64# DataFrame reindex&gt;&gt;&gt; frame = pd.DataFrame(np.arange(9).reshape(3,3), index=['a', 'c', 'd'], columns=['Ohio', 'Texas', 'California'])&gt;&gt;&gt; frame Ohio Texas Californiaa 0 1 2c 3 4 5d 6 7 8&gt;&gt;&gt; frame2 = frame.reindex(['a', 'b', 'c', 'd'])&gt;&gt;&gt; frame2 Ohio Texas Californiaa 0.0 1.0 2.0b NaN NaN NaNc 3.0 4.0 5.0d 6.0 7.0 8.0&gt;&gt;&gt; states = ['Texas', 'Utah', 'California']&gt;&gt;&gt; frame.reindex(columns=states) Texas Utah Californiaa 1 NaN 2c 4 NaN 5d 7 NaN 8 reindex后的值还可以使用method参数来补充新添加的值 12345678910111213141516171819202122&gt;&gt;&gt; obj = pd.Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])&gt;&gt;&gt; obj0 blue2 purple4 yellowdtype: object# method的可选值# * default: don't fill gaps# * pad / ffill: propagate last valid observation forward to next# valid# * backfill / bfill: use next valid observation to fill gap# * nearest: use nearest valid observations to fill gap&gt;&gt;&gt; obj.reindex(range(6), method='ffill')0 blue1 blue2 purple3 purple4 yellow5 yellowdtype: object Drop行或列 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&gt;&gt;&gt; obj = pd.Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])&gt;&gt;&gt; obja 0.0b 1.0c 2.0d 3.0e 4.0dtype: float64# drop掉一个索引&gt;&gt;&gt; new_obj = obj.drop('c')&gt;&gt;&gt; new_obja 0.0b 1.0d 3.0e 4.0dtype: float64&gt;&gt;&gt; obj.drop(['d', 'c'])a 0.0b 1.0e 4.0dtype: float64&gt;&gt;&gt; data = pd.DataFrame(np.arange(16).reshape((4, 4)),... index=['Ohio', 'Colorado', 'Utah', 'New York'],... columns=['one', 'two', 'three', 'four'])...&gt;&gt;&gt; data one two three fourOhio 0 1 2 3Colorado 4 5 6 7Utah 8 9 10 11New York 12 13 14 15# drop行&gt;&gt;&gt; data.drop(['Colorado', 'Ohio']) one two three fourUtah 8 9 10 11New York 12 13 14 15# drop列&gt;&gt;&gt; data.drop('two', axis=1) one three fourOhio 0 2 3Colorado 4 6 7Utah 8 10 11New York 12 14 15# 默认情况下，drop方法会返回一个新的对象，但是不会修改原来的值# 使用inplace参数可以直接修改原来的值&gt;&gt;&gt; data.drop('Utah', inplace=True)&gt;&gt;&gt; data one two three fourOhio 0 1 2 3Colorado 4 5 6 7New York 12 13 14 15 函数应用和映射 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&gt;&gt;&gt; frame = pd.DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])&gt;&gt;&gt; frame b d eUtah 0.282987 -0.941163 -1.212623Ohio -1.072346 -0.245222 -0.731497Texas 2.264081 0.169154 -0.512539Oregon -0.795480 0.630302 1.127120&gt;&gt;&gt; np.abs(frame) b d eUtah 0.282987 0.941163 1.212623Ohio 1.072346 0.245222 0.731497Texas 2.264081 0.169154 0.512539Oregon 0.795480 0.630302 1.127120&gt;&gt;&gt; f = lambda x: x.max() - x.min()# 默认作用于每列&gt;&gt;&gt; frame.apply(f)b 3.336427d 1.571465e 2.339742dtype: float64# 指定作用于每行&gt;&gt;&gt; frame.apply(f, axis='columns')Utah 1.495609Ohio 0.827124Texas 2.776620Oregon 1.922600dtype: float64# applymap作用于每个元素&gt;&gt;&gt; format = lambda x: '%.2f' % x&gt;&gt;&gt; frame.applymap(format) b d eUtah 0.28 -0.94 -1.21Ohio -1.07 -0.25 -0.73Texas 2.26 0.17 -0.51Oregon -0.80 0.63 1.13&gt;&gt;&gt; frame['e'].map(format)Utah -1.21Ohio -0.73Texas -0.51Oregon 1.13Name: e, dtype: object 排序(Sort) 按索引排序 1234567891011121314151617181920212223242526272829303132333435363738394041&gt;&gt;&gt; obj = pd.Series(range(4), index=['d', 'a', 'b', 'c'])&gt;&gt;&gt; objd 0a 1b 2c 3dtype: int64# 按索引排序&gt;&gt;&gt; obj.sort_index()a 1b 2c 3d 0dtype: int64&gt;&gt;&gt; frame = pd.DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'], columns=['d', 'a', 'b', 'c'])&gt;&gt;&gt; frame d a b cthree 0 1 2 3one 4 5 6 7# 按索引排序&gt;&gt;&gt; frame.sort_index() d a b cone 4 5 6 7three 0 1 2 3# 按列排序&gt;&gt;&gt; frame.sort_index(axis=1) a b c dthree 1 2 3 0one 5 6 7 4# 按照降序排序&gt;&gt;&gt; frame.sort_index(axis=1, ascending=False) d c b athree 0 3 2 1one 4 7 6 5 按值排序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt;&gt;&gt; obj = pd.Series([4, 7, -3, 2])&gt;&gt;&gt; obj0 41 72 -33 2dtype: int64&gt;&gt;&gt; obj.sort_values()2 -33 20 41 7dtype: int64&gt;&gt;&gt; obj = pd.Series([4, np.nan, 7, np.nan, -3, 2])# 排序时NaN会自动排在最后&gt;&gt;&gt; obj.sort_values()4 -3.05 2.00 4.02 7.01 NaN3 NaNdtype: float64&gt;&gt;&gt; frame = pd.DataFrame(&#123;'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]&#125;)&gt;&gt;&gt; frame a b0 0 41 1 72 0 -33 1 2# 按照指定的列排序&gt;&gt;&gt; frame.sort_values(by='b') a b2 0 -33 1 20 0 41 1 7# 按照多个列排序&gt;&gt;&gt; frame.sort_values(by=['a', 'b']) a b2 0 -30 0 43 1 21 1 7 排名(rank) ranking（排名）是根据数字的排序，分配一个数字。rank方法能用于series和DataFrame，rank方法默认会给每个group一个mean rank（平均排名）。 rank表示在这个数在原来的Series中排第几名，有相同的数，取其排名平均（默认）作为值。 123456789101112131415161718192021222324252627282930313233&gt;&gt;&gt; obj = pd.Series([7, -5, 7, 4, 2, 0, 4])&gt;&gt;&gt; obj0 71 -52 73 44 25 06 4dtype: int64# 索引1 排第一名。rank值是1.0# 索引3和6分别排4，5名，所以rank是平均是4.5&gt;&gt;&gt; obj.rank()0 6.51 1.02 6.53 4.54 3.05 2.06 4.5dtype: float64# 可以看出rank分配的值跟sort_values的排序是一致的&gt;&gt;&gt; obj.sort_values()1 -55 04 23 46 40 72 7 排序也可以根据数据出现的顺序来排序，而不是使用平均值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&gt;&gt;&gt; obj0 71 -52 73 44 25 06 4dtype: int64# 下面的排序中，虽然索引3和6对应的值是一样的，但是这次rank值并没有取平均值4.5，# 而是根据它们出现的顺序，rank值依次为4和5&gt;&gt;&gt; obj.rank(method='first')0 6.01 1.02 7.03 4.04 3.05 2.06 5.0dtype: float64# 排序时，rank值取大的排名# 索引3和6对应的值是一样的。rank值同时排第5&gt;&gt;&gt; obj.rank(method='max')0 7.01 1.02 7.03 5.04 3.05 2.06 5.0dtype: float64# 排序时，rank值取小的排名# 索引3和6对应的值是一样的。rank值同时排第4&gt;&gt;&gt; obj.rank(method='min')0 6.01 1.02 6.03 4.04 3.05 2.06 4.0dtype: float64# 按照倒序排名&gt;&gt;&gt; obj.rank(method='min', ascending=False)0 1.01 7.02 1.03 3.04 5.05 6.06 3.0dtype: float64&gt;&gt;&gt; frame = pd.DataFrame(&#123;'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1], 'c': [-2, 5, 8, -2.5]&#125;)&gt;&gt;&gt; frame a b c0 0 4.3 -2.01 1 7.0 5.02 0 -3.0 8.03 1 2.0 -2.5# 按照行返回rank值&gt;&gt;&gt; frame.rank(axis='columns') a b c0 2.0 3.0 1.01 1.0 3.0 2.02 2.0 1.0 3.03 2.0 3.0 1.0 重复的索引 12345678910111213&gt;&gt;&gt; obj = pd.Series(range(5), index=['a', 'a', 'b', 'b', 'c'])&gt;&gt;&gt; obja 0a 1b 2b 3c 4dtype: int64# is_unique 属性可以确定index是否含有重复的值&gt;&gt;&gt; obj.index.is_uniqueFalse Pandas数据加载，存储 备注: 下面例子使用的csv文件可以从这里下载。 读取CSV文件 使用read_csv加载文件 123456789101112131415&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; ! cat examples/ex1.csva,b,c,d,message1,2,3,4,hello5,6,7,8,world9,10,11,12,foo&gt;&gt;&gt; df = pd.read_csv('examples/ex1.csv')&gt;&gt;&gt; df a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 foo 使用read_table加载文件 12345678910&gt;&gt;&gt; pd.read_table('examples/ex1.csv', sep=',') a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 foo&gt;&gt;&gt; !cat examples/ex2.csv1,2,3,4,hello5,6,7,8,world9,10,11,12,foo 针对没有header的情况 12345&gt;&gt;&gt; pd.read_csv('examples/ex2.csv', header=None) 0 1 2 3 40 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 foo 指定columns索引 1234567&gt;&gt;&gt; pd.read_csv('examples/ex2.csv', names=['a', 'b', 'c', 'd', 'message']) a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 foo&gt;&gt;&gt; names = ['a', 'b', 'c', 'd', 'message'] 使用csv中的message列作为index 123456&gt;&gt;&gt; pd.read_csv('examples/ex2.csv', names=names, index_col='message') a b c dmessagehello 1 2 3 4world 5 6 7 8foo 9 10 11 12 构建hierarchical index 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; !cat examples/csv_mindex.csvkey1,key2,value1,value2one,a,1,2one,b,3,4one,c,5,6one,d,7,8two,a,9,10two,b,11,12two,c,13,14two,d,15,16&gt;&gt;&gt; parsed = pd.read_csv('examples/csv_mindex.csv', index_col=['key1', 'key2'])&gt;&gt;&gt; parsed value1 value2key1 key2one a 1 2 b 3 4 c 5 6 d 7 8two a 9 10 b 11 12 c 13 14 d 15 16&gt;&gt;&gt; parsed.indexMultiIndex(levels=[['one', 'two'], ['a', 'b', 'c', 'd']], labels=[[0, 0, 0, 0, 1, 1, 1, 1], [0, 1, 2, 3, 0, 1, 2, 3]], names=['key1', 'key2']) 使用其他的分隔符号(如下使用空格作为列分隔符) 123456789101112131415&gt;&gt;&gt; list(open('examples/ex3.txt'))[' A B C\n', 'aaa -0.264438 -1.026059 -0.619500\n', 'bbb 0.927272 0.302904 -0.032399\n', 'ccc -0.264273 -0.386314 -0.217601\n', 'ddd -0.871858 -0.348382 1.100491\n']&gt;&gt;&gt; result = pd.read_table('examples/ex3.txt', sep='\s+')&gt;&gt;&gt; result A B Caaa -0.264438 -1.026059 -0.619500bbb 0.927272 0.302904 -0.032399ccc -0.264273 -0.386314 -0.217601ddd -0.871858 -0.348382 1.100491 过滤掉指定的行 12345678910111213&gt;&gt;&gt; !cat examples/ex4.csv# hey!a,b,c,d,message# just wanted to make things more difficult for you# who reads CSV files with computers, anyway?1,2,3,4,hello5,6,7,8,world9,10,11,12,foo&gt;&gt;&gt; pd.read_csv('examples/ex4.csv', skiprows=[0, 2, 3]) a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 foo 默认情况下，pandas认为NULL和NA为缺失的值 123456789101112131415161718&gt;&gt;&gt; !cat examples/ex5.csvsomething,a,b,c,d,messageone,1,2,3,4,NAtwo,5,6,,8,worldthree,9,10,11,12,foo&gt;&gt;&gt; result = pd.read_csv('examples/ex5.csv')&gt;&gt;&gt; result something a b c d message0 one 1 2 3.0 4 NaN1 two 5 6 NaN 8 world2 three 9 10 11.0 12 foo&gt;&gt;&gt; pd.isnull(result) something a b c d message0 False False False False False True1 False False False True False False2 False False False False False False 也可以通过na_values来指定缺失的值 1234567891011121314151617# 认为NULL为NaN&gt;&gt;&gt; result = pd.read_csv('examples/ex5.csv', na_values=['NULL'])&gt;&gt;&gt; result something a b c d message0 one 1 2 3.0 4 NaN1 two 5 6 NaN 8 world2 three 9 10 11.0 12 foo&gt;&gt;&gt; sentinels = &#123;'message': ['foo', 'NA'], 'something': ['two']&#125;# 通过字典，可以为每一列设置NaN值&gt;&gt;&gt; pd.read_csv('examples/ex5.csv', na_values=sentinels) something a b c d message0 one 1 2 3.0 4 NaN1 NaN 5 6 NaN 8 world2 three 9 10 11.0 12 NaN 读取文件的一部分 1234567891011121314151617181920# 设置只显示10行&gt;&gt;&gt; pd.options.display.max_rows = 10&gt;&gt;&gt; result = pd.read_csv('examples/ex6.csv')&gt;&gt;&gt; result one two three four key0 0.467976 -0.038649 -0.295344 -1.824726 L1 -0.358893 1.404453 0.704965 -0.200638 B2 -0.501840 0.659254 -0.421691 -0.057688 G3 0.204886 1.074134 1.388361 -0.982404 R4 0.354628 -0.133116 0.283763 -0.837063 Q... ... ... ... ... ..9995 2.311896 -0.417070 -1.409599 -0.515821 L9996 -0.479893 -0.650419 0.745152 -0.646038 E9997 0.523331 0.787112 0.486066 1.093156 K9998 -0.362559 0.598894 -1.843201 0.887292 G9999 -0.096376 -1.012999 -0.657431 -0.573315 0[10000 rows x 5 columns] 分块读取文件 1234567891011121314151617&gt;&gt;&gt; chunker = pd.read_csv('examples/ex6.csv', chunksize=1000)&gt;&gt;&gt; chunker&lt;pandas.io.parsers.TextFileReader at 0x1136a62b0&gt;# 通过遍历chunker，可以读取整个文件，1次读取chunksize行&gt;&gt;&gt; [type(piece) for piece in chunker][pandas.core.frame.DataFrame, pandas.core.frame.DataFrame, pandas.core.frame.DataFrame, pandas.core.frame.DataFrame, pandas.core.frame.DataFrame, pandas.core.frame.DataFrame, pandas.core.frame.DataFrame, pandas.core.frame.DataFrame, pandas.core.frame.DataFrame, pandas.core.frame.DataFrame] 保存数据 保存到CSV文件 123456789101112131415&gt;&gt;&gt; data = pd.read_csv('examples/ex5.csv')&gt;&gt;&gt; data something a b c d message0 one 1 2 3.0 4 NaN1 two 5 6 NaN 8 world2 three 9 10 11.0 12 foo&gt;&gt;&gt; data.to_csv('examples/out.csv')&gt;&gt;&gt; !cat examples/out.csv,something,a,b,c,d,message0,one,1,2,3.0,4,1,two,5,6,,8,world2,three,9,10,11.0,12,foo 指定输出到标准输出和列分隔符 1234567&gt;&gt;&gt; import sys&gt;&gt;&gt; data.to_csv(sys.stdout, sep='|')|something|a|b|c|d|message0|one|1|2|3.0|4|1|two|5|6||8|world2|three|9|10|11.0|12|foo 默认情况下，确实的数据是以空字符串输出的，我们也可以指定缺失的数据的输出内容 12345&gt;&gt;&gt; data.to_csv(sys.stdout, na_rep='NULL'),something,a,b,c,d,message0,one,1,2,3.0,4,NULL1,two,5,6,NULL,8,world2,three,9,10,11.0,12,foo 默认情况下，列索引和行索引都会写出到文件，也可以禁用这个行为 12345# columns的索引用的是header参数&gt;&gt;&gt; data.to_csv(sys.stdout, na_rep='NULL', index=False, header=False)one,1,2,3.0,4,NULLtwo,5,6,NULL,8,worldthree,9,10,11.0,12,foo 保存部分数据 123456# 只保存a,b,c列&gt;&gt;&gt; data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])a,b,c1,2,3.05,6,9,10,11.0]]></content>
      <categories>
        <category>python数据分析</category>
      </categories>
      <tags>
        <tag>numpy</tag>
        <tag>matplotlib</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib教程]]></title>
    <url>%2F2017%2F12%2F25%2Fmatplotlib-quickstart%2F</url>
    <content type="text"><![CDATA[本文参考: Matplotlib 画图教程系列 | 莫烦Python 安装 1$ pip install matplotlib 基本用法 画一个基本的线条 123456789import numpy as npimport matplotlib.pyplot as pltx = np.linspace(-1, 1, 50)y = 2*x + 1# x,y 的值是一一对应的plt.plot(x, y)# 让图片显示plt.show() 效果图 一次画多个图像(Figure) 默认情况下，一次只会画一张图片。通过figure，我们可以一次画多个图像。 1234567891011121314151617181920import numpy as npimport matplotlib.pyplot as pltx = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2# 画第一张图plt.figure()plt.plot(x, y1)# 画第二张图# num: 设置图片的序号, figsize: 设置图片的大小plt.figure(num=3, figsize=(8,5))plt.plot(x, y2)# 画第二张图的第二根图线# color: 设置图线的颜色, linewidth: 图线的宽度, linestyle: 图线的样式plt.plot(x, y1, color='red', linewidth=1.0, linestyle='--')plt.show() 设置x,y轴的坐标和标签 1234567891011121314151617181920212223242526272829import numpy as npimport matplotlib.pyplot as pltx = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2plt.figure(figsize=(9, 5))plt.plot(x, y2)plt.plot(x, y1, color='red', linewidth=1.0, linestyle='--')# 设置x轴的坐标范围在(-1, 2)plt.xlim(-1, 2)# 设置y轴的坐标范围在(-2, 3)plt.ylim(-2, 3)# 设置x轴的标签plt.xlabel("I am x")# 设置y轴的标签plt.ylabel("I am y")new_ticks = np.linspace(-1, 2, 5)# 设置x轴的新坐标plt.xticks(new_ticks)# 用文字代替y轴的坐标值, 首尾的$符号表示改变文字的字体plt.yticks([-2, -1.8, -1, 1.22, 3], [r'$really\ bad$', r'$bad$', r'$normal$', r'$good$', r'$really\ good$'])plt.show() Figure with custom ticks 设置x,y轴的坐标原点位置和x,y轴的刻度位置 1234567891011121314151617181920212223242526272829303132333435363738394041import numpy as npimport matplotlib.pyplot as pltx = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2plt.figure(figsize=(9, 5))plt.plot(x, y2)plt.plot(x, y1, color='red', linewidth=1.0, linestyle='--')plt.xlim(-1, 2)plt.ylim(-2, 3)plt.xlabel('I am x')plt.ylabel('I am y')new_ticks = np.linspace(-1, 2, 5)plt.xticks(new_ticks)plt.yticks([-2, -1.8, -1, 1.22, 3], [r'$really\ bad$', r'$bad$', r'$normal$', r'$good$', r'$really\ good$'])# gca = 'get current axis'# 获取图片的4个外边框线ax = plt.gca()# 去掉右边的边框ax.spines['right'].set_color('none')# 去掉上边的边框ax.spines['top'].set_color('none')# 设置x轴的坐标刻度在下面ax.xaxis.set_ticks_position('bottom')# 设置y轴的坐标刻度在左边ax.yaxis.set_ticks_position('left')# 下面两个设置坐标原点的位置(0, 0)# 设置y轴的值在0刻度ax.spines['bottom'].set_position(('data', 0))# 设置x轴的值在0刻度ax.spines['left'].set_position(('data', 0))plt.show() change figure current axis 添加图例(legend) 12345678910111213141516171819202122232425import numpy as npimport matplotlib.pyplot as pltx = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2plt.figure(figsize=(9, 5))plt.xlim(-1, 2)plt.ylim(-2, 3)plt.xlabel('I am x')plt.ylabel('I am y')new_ticks = np.linspace(-1, 2, 5)plt.xticks(new_ticks)plt.yticks([-2, -1.8, -1, 1.22, 3], [r'$really\ bad$', r'$bad$', r'$normal$', r'$good$', r'$really\ good$'])# 通过label参数设置线的名称l1, = plt.plot(x, y2, label='up')l2, = plt.plot(x, y1, color='red', linewidth=1.0, linestyle='--', label='down')# 调用legend方法显示图例，如果这里使用了labels参数，会覆盖上面plot函数的label参数，否则图例就是用plot时指定的label。loc参数指定图例的位置plt.legend(handles=[l1, l2], labels=['aaa', 'bbb'], loc='best') figure with legend 添加标注(Annotation) 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npimport matplotlib.pyplot as pltx = np.linspace(-3, 3, 50)y = 2 * x + 1plt.figure(num=1, figsize=(9, 5))plt.plot(x, y)ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position('bottom')ax.yaxis.set_ticks_position('left')ax.spines['bottom'].set_position(('data', 0))ax.spines['left'].set_position(('data', 0))x0 = 1y0 = 2 * x0 + 1plt.scatter(x0, y0, s=50, color='b')plt.plot([x0, x0], [y0, 0], 'k--', lw=2.5)# method 1: 添加标注 annotateplt.annotate( r'$2x+1=%s$' % y0, # 标注的文本 xy=(x0, y0), # 标注的位置 xycoords='data', xytext=(+30, -30), # 标注文本相对于标注位置的偏移 textcoords="offset points", fontsize=16, # 标注的文字字体大小 arrowprops=dict( # 设置标注的箭头样式 arrowstyle='-&gt;', connectionstyle='arc3, rad=.2' ))# method 2: 添加标注 textplt.text( -3.7, 3, # 标注的位置 r'$This\ is\ the\ some\ text.\mu\ \sigma_i\ \alpha_t$', # 标注的文本 fontdict=&#123;'size': 16, 'color': 'r'&#125; # 标注的字体设置)plt.show() annotate 能见度(tick) 当图片中的内容较多，相互遮盖时，我们可以通过设置相关内容的透明度来使图片更易于观察，即是通过bbox(backgroud box)参数设置来调节图像信息. 123456789101112131415161718192021222324252627282930import numpy as npimport matplotlib.pyplot as pltx = np.linspace(-3, 3, 50)y = 0.1 * xplt.figure()# 参数zorder指定plot在z轴方向排序plt.plot(x, y, linewidth=10, zorder=1)plt.ylim(-2, 2)ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.spines['right'].set_color('none')ax.xaxis.set_ticks_position('bottom')ax.yaxis.set_ticks_position('left')ax.spines['bottom'].set_position(('data', 0))ax.spines['left'].set_position(('data', 0))# 获取所有x轴和y轴的tick labelfor label in ax.get_xticklabels() + ax.get_yticklabels(): label.set_fontsize(12) # 设置label的文字大小 label.set_bbox(dict( facecolor='white', # 设置前景色 edgecolor='None', # 设置边框为无 alpha=0.7, # 设置透明度 zorder=2 ))plt.show() figure tick 画图的种类 Scatter 散点图 123456789101112131415161718import matplotlib.pyplot as pltimport numpy as npn = 1024X = np.random.normal(0, 1, n)Y = np.random.normal(0, 1, n)T = np.arctan2(Y, X) # 用于设置颜色plt.scatter(X, Y, s=75, c=T, alpha=0.5)plt.xlim(-1.5, 1.5)plt.ylim(-1.5, 1.5)# 去掉x,y轴的坐标刻度plt.xticks(())plt.yticks(())plt.show() figure scatter Bar柱状图 1234567891011121314151617181920212223242526272829import matplotlib.pyplot as pltimport numpy as npn = 12X = np.arange(n)Y1 = (1 - X / float(n)) * np.random.uniform(0.5, 1.0, n)Y2 = (1 - X / float(n)) * np.random.uniform(0.5, 1.0, n)plt.bar(X, +Y1, facecolor='#9999ff', edgecolor='white')plt.bar(X, -Y2, facecolor='#ff9999', edgecolor='white')for x, y in zip(X, Y1): # ha: horizontal alignment # va: vertical alignment plt.text(x + 0.05, y + 0.02, '%.2f' % y, ha='center', va='bottom')for x, y in zip(X, Y2): # ha: horizontal alignment # va: vertical alignment plt.text(x + 0.05, -y - 0.02, '-%.2f' % y, ha='center', va='top')plt.xlim(-0.5, n)plt.xticks(())plt.ylim(-1.25, +1, 25)plt.yticks(())plt.show() figure bar Contours 等高线图 12345678910111213141516171819202122232425262728293031import matplotlib.pyplot as pltimport numpy as npdef f(x, y): # the height function return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)n = 256x = np.linspace(-3, 3, n)y = np.linspace(-3, 3, n)#X, Y为shape为(256, 256)的表X, Y = np.meshgrid(x, y)# 接下来进行颜色填充。使用函数plt.contourf把颜色加进去。# 位置参数分别为: X, Y, f(X,Y)。透明度0.75，并将 f(X,Y) 的值对应到color map的暖色组中寻找对应颜色。# use plt.contourf to filling contours# X, Y and value for (X,Y) point# 其中，8代表等高线的密集程度，这里被分为10个部分。如果是0，则图像被一分为二plt.contourf(X, Y, f(X, Y), 8, alpha=.75, cmap=plt.cm.hot)# 接下来进行等高线绘制。使用plt.contour函数划线。位置参数为：X, Y, f(X,Y)。颜色选黑色，线条宽度选0.5# use plt.contour to add contour linesC = plt.contour(X, Y, f(X, Y), 8, colors='black', linewidth=.5)# 最后加入Label，inline控制是否将Label画在线里面，字体大小为10。并将坐标轴隐藏：plt.clabel(C, inline=True, fontsize=10)plt.xticks(())plt.yticks(())plt.show() figure contours Image图片 1234567891011121314151617181920import matplotlib.pyplot as pltimport numpy as np# 三行三列的格子，a代表每一个值，图像右边有一个注释，白色代表值最大的地方，颜色越深值越小。a = np.array([0.313660827978, 0.365348418405, 0.423733120134, 0.365348418405, 0.439599930621, 0.525083754405, 0.423733120134, 0.525083754405, 0.651536351379]).reshape(3,3)# 之前选cmap的参数时用的是：cmap=plt.cmap.bone，而现在，我们可以直接用单引号传入参数。# origin='lower'代表的就是选择的原点的位置。# interpolation参数设置出图的方式, 具体可以参考:# https://matplotlib.org/examples/images_contours_and_fields/interpolation_methods.htmlplt.imshow(a, interpolation='nearest', cmap='bone', origin='lower')# 添加右边显示的colorbar ，其中我们添加一个shrink参数，使colorbar的长度变短为原来的92%：plt.colorbar(shrink=.92)plt.xticks(())plt.yticks(())plt.show() figure image 3D图像 123456789101112131415161718192021import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfig = plt.figure()ax = Axes3D(fig)ax.scatter(1, 1, 1)# X, Y valueX = np.arange(-4, 4, 0.25)Y = np.arange(-4, 4, 0.25)X, Y = np.meshgrid(X, Y) # x-y 平面的网格R = np.sqrt(X ** 2 + Y ** 2)# height valueZ = np.sin(R)# rstride 和 cstride 分别代表 row 和 column 的跨度。ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.get_cmap('rainbow'))# 添加 XY 平面的等高线,如果zdir 选择了x，那么效果将会是对于 XZ 平面的投影ax.contourf(X, Y, Z, zdir='z', offset=-2, cmap=plt.get_cmap('rainbow'))plt.show() figure 3d 多图合并显示（subplot） 多合一显示 均匀分割 12345678910111213141516171819import matplotlib.pyplot as pltplt.figure()# 分成两行两列，画在第一个图的位置plt.subplot(2, 2, 1)plt.plot([0, 1], [0, 1])plt.subplot(2, 2, 2)plt.plot([0, 1], [0, 2])# `223` 简单的写法，分成两行两列，画在第三个图的位置plt.subplot(223)plt.plot([0, 1], [0, 3])plt.subplot(224)plt.plot([0, 1], [0, 4])plt.show() figure subplot 1 不均匀分割 12345678910111213141516171819import matplotlib.pyplot as pltplt.figure()# 分成两行一列，画在第一个位置plt.subplot(2, 1, 1)plt.plot([0, 1], [0, 1])# 分成两行三列，注意是从位置4开始plt.subplot(2, 3, 4)plt.plot([0, 1], [0, 2])plt.subplot(235)plt.plot([0, 1], [0, 3])plt.subplot(236)plt.plot([0, 1], [0, 4])plt.show() figure subplot 2 分格显示 方法1: subplot2grid 12345678910111213141516171819import matplotlib.pyplot as pltplt.figure()# 将figure分成3行3列，从(0,0)开始， 横跨3列，1行ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=3, rowspan=1)ax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)ax3 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)ax4 = plt.subplot2grid((3, 3), (2, 0))ax5 = plt.subplot2grid((3, 3), (2, 1))ax1.plot([1, 2], [1, 2])ax1.set_title("ax1 title")ax2.set_title("ax2 title")ax3.set_title("ax3 title")ax4.set_title("ax4 title")ax5.set_title("ax5 title")plt.show() figure subplot2grid 方法2: gridspec 12345678910111213141516171819202122232425import matplotlib.pyplot as pltimport matplotlib.gridspec as gridspecplt.figure()# 将figure分成三行三列gs = gridspec.GridSpec(3, 3)# ax1, 第一行，所有列ax1 = plt.subplot(gs[0, :])# ax2, 第二行，前两列ax2 = plt.subplot(gs[1, :2])# ax2, 第二行到最后一行，第三列ax3 = plt.subplot(gs[1:, 2])# ax2, 最后一行，第一列ax4 = plt.subplot(gs[-1, 0])ax5 = plt.subplot(gs[-1, -2])ax1.plot([1, 2], [1, 2])ax1.set_title('ax1 title')ax2.set_title("ax2 title")ax3.set_title("ax3 title")ax4.set_title("ax4 title")ax5.set_title("ax5 title")plt.show() figure gridspec 方法3: subplots 12345678910111213141516import matplotlib.pyplot as pltplt.figure()# 使用plt.subplots建立一个2行2列的图像窗口# sharex=True表示共享x轴坐标, sharey=True表示共享y轴坐标.# f代表figure对象# ((ax11, ax12), (ax13, ax14))表示第1行从左至右依次放ax11和ax12, 第2行从左至右依次放ax13和ax14.f, ((ax11, ax12), (ax13, ax14)) = plt.subplots(2, 2, sharex=True, sharey=True)ax11.scatter([1, 2], [1, 2])# 紧凑显示图像plt.tight_layout()plt.show() figure subplots 图中图 12345678910111213141516171819202122232425262728293031323334353637import matplotlib.pyplot as pltfig = plt.figure()# 创建数据x = [1, 2, 3, 4, 5, 6, 7]y = [1, 3, 4, 2, 5, 8, 6]# 4个值都是占整个figure坐标系的百分比。# 在这里，假设figure的大小是10x10，那么大图就被包含在由(1, 1)开始，宽8，高8的坐标系内。left, bottom, width, height = 0.1, 0.1, 0.8, 0.8# 将大图坐标系添加到figure中，颜色为r(red)，取名为titleax1 = fig.add_axes([left, bottom, width, height])ax1.plot(x, y, 'r')ax1.set_xlabel('x')ax1.set_ylabel('y')ax1.set_title('title')# 绘制左上角的小图，步骤和绘制大图一样，注意坐标系位置和大小的改变left, bottom, width, height = 0.2, 0.6, 0.25, 0.25ax2 = fig.add_axes([left, bottom, width, height])ax2.plot(y, x, 'b')ax2.set_xlabel('x')ax2.set_ylabel('y')ax2.set_title('title inside 1')# 绘制右下角的小图。这里我们采用一种更简单方法，即直接往plt里添加新的坐标系：plt.axes([0.6, 0.2, 0.25, 0.25])plt.plot(y[::-1], x, 'g') # 注意对y进行了逆序处理plt.xlabel('x')plt.ylabel('y')plt.title('title inside 2')plt.show() figure insiede 次坐标轴 有时候我们会用到次坐标轴，即在同个图上有第2个y轴存在 1234567891011121314151617181920import matplotlib.pyplot as pltimport numpy as npx = np.arange(0, 10, 0.1)y1 = 0.05 * x ** 2y2 = -1 * y1# 获取figure默认的坐标系 ax1fig, ax1 = plt.subplots()# 对ax1调用twinx()方法，生成如同镜面效果后的ax2ax2 = ax1.twinx()ax1.plot(x, y1, 'g-') # green, solid lineax1.set_xlabel('X data')ax1.set_ylabel('Y1 data', color='g')ax2.plot(x, y2, 'b-') # blueax2.set_ylabel('Y2 data', color='b')plt.show() figure twinx 动画(Animation) 1234567891011121314151617181920212223242526272829303132333435363738394041from matplotlib import pyplot as pltfrom matplotlib import animationimport numpy as npfig, ax = plt.subplots()# 数据是一个0~2π内的正弦曲线x = np.arange(0, 2 * np.pi, 0.01)line, = ax.plot(x, np.sin(x))# 构造自定义动画函数animate，用来更新每一帧上各个x对应的y坐标值，参数表示第i帧def animate(i): line.set_ydata(np.sin(x + i / 10.0)) return line,# 构造开始帧函数initdef init(): line.set_ydata(np.sin(x)) return line,# 参数说明:# fig 进行动画绘制的figure# func 自定义动画函数，即传入刚定义的函数animate# frames 动画长度，一次循环包含的帧数# init_func 自定义开始帧，即传入刚定义的函数init# interval 更新频率，以ms计# blit 选择更新所有点，还是仅更新产生变化的点。应选择True，但mac用户请选择False，否则无法显示动画ani = animation.FuncAnimation(fig=fig, func=animate, frames=100, init_func=init, interval=20, blit=True)# 保存为MP4格式# ani.save('basic_animation.mp4', fps=30, extra_args=['-vcodec', 'libx264'])plt.show() 扩展阅读 Matplotlib gallery 可以查看各种图的效果和代码 Matplotlib 教程 十分钟入门Matplotlib]]></content>
      <categories>
        <category>python数据分析</category>
      </categories>
      <tags>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10分钟入门Pandas]]></title>
    <url>%2F2017%2F12%2F20%2F10-minutes-to-pandas%2F</url>
    <content type="text"><![CDATA[参考: 10 Minutes to pandas 安装 支持的python版本: 2.7, 3.5, 3.6 1$ pip install pandas 检查本地的pandas运行环境是否完整，可以运行pandas的单元测试用例 1$ pip install pytest 12&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; pd.test() 获取当前使用pandas的版本信息 123&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; pd.__version__'0.21.1' 概览 pandas的基本数据结构: Series: 一维数据 DataFrame: 二维数据 Panel: 三维数据(从0.20.0版本开始，已经不再推荐使用) Panel4D, PanelND(不再推荐使用) DataFrame是由Series构成的 创建Series 创建Series最简单的方法 1&gt;&gt;&gt; s = pd.Series(data, index=index) data可以是不同的类型: python字典 ndarray 标量(比如: 5) 使用ndarray创建(From ndarray) 如果data是ndarray,那么index的长度必须和data的长度相同，当没有明确index参数时，默认使用[0, ... len(data) - 1]作为index。 123456789101112131415161718192021222324&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])&gt;&gt;&gt; sa 0.654385b 0.055691c 0.856054d 0.621810e 1.802872dtype: float64&gt;&gt;&gt; s.indexIndex(['a', 'b', 'c', 'd', 'e'], dtype='object')&gt;&gt;&gt; pd.Series(np.random.randn(5))0 -0.4671831 -1.3333232 -0.4938133 -0.0677054 -1.310332dtype: float64 需要注意的是: pandas里的索引并不要求唯一性，如果一个操作不支持重复的索引，会自动抛出异常。这么做的原因是很多操作不会用到索引，比如GroupBy。 123456789&gt;&gt;&gt; s = pd.Series(np.random.randn(5), index=['a', 'a', 'a', 'a', 'a'])&gt;&gt;&gt; sa 0.847331a -2.138021a -0.364763a -0.603172a 0.363691dtype: float64 使用dict创建(From dict) 当data是dict类型时，如果指定了index参数，那么就使用index参数作为索引。否者，就使用排序后的data的key作为index。 12345678910111213141516&gt;&gt;&gt; d = &#123;'b': 0., 'a': 1., 'c': 2.&#125;# 索引的值是排序后的&gt;&gt;&gt; pd.Series(d)a 1.0b 0.0c 2.0dtype: float64# 字典中不存在的key, 直接赋值为NaN(Not a number)&gt;&gt;&gt; pd.Series(d, index=['b', 'c', 'd', 'a'])b 0.0c 2.0d NaNa 1.0dtype: float64 使用标量创建(From scalar value) 当data是标量时，必须提供index, 值会被重复到index的长度 1234567&gt;&gt;&gt; pd.Series(5., index=['a', 'b', 'c', 'd', 'e'])a 5.0b 5.0c 5.0d 5.0e 5.0dtype: float64 创建DataFrame DataFrame是一个二维的数据结构，可以看做是一个excel表格或一张SQL表，或者值为Series的字典。 跟Series一样，DataFrame也可以通过多种类型的数据结构来创建 字典(包含一维ndarray数组，列表，字典或Series) 二维的ndarray数组 结构化的ndarray Series 另一个DataFrame 除了data之外，还接受index和columns参数来分布指定行和列的标签 从Series字典或嵌套的字典创建(From dict of Series or dicts) 结果的索引是多个Series索引的合集，如果没有指定columns，就用排序后的字典的key作为列标签。 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; d = &#123;'one': pd.Series([1,2,3], index=['a', 'b', 'c']),... 'two': pd.Series([1,2,3,4], index=['a', 'b', 'c', 'd'])&#125;...&gt;&gt;&gt; df = pd.DataFrame(d)&gt;&gt;&gt; df one twoa 1.0 1b 2.0 2c 3.0 3d NaN 4&gt;&gt;&gt; pd.DataFrame(d, index=['d', 'b', 'a']) one twod NaN 4b 2.0 2a 1.0 1&gt;&gt;&gt; pd.DataFrame(d, index=['d', 'b', 'a'], columns=['two', 'three']) two threed 4 NaNb 2 NaNa 1 NaN&gt;&gt;&gt; df.indexIndex(['a', 'b', 'c', 'd'], dtype='object')&gt;&gt;&gt; df.columnsIndex(['one', 'two'], dtype='object') 从ndarray类型/列表类型的字典(From dict of ndarrays / lists) 123456789101112131415&gt;&gt;&gt; d = &#123;'one': [1,2,3,4], 'two': [4,3,2,1]&#125;&gt;&gt;&gt; pd.DataFrame(d) one two0 1 41 2 32 3 23 4 1&gt;&gt;&gt; pd.DataFrame(d, index=['a', 'b', 'c', 'd']) one twoa 1 4b 2 3c 3 2d 4 1 从结构化ndarray创建(From structured or record array) 12345678910111213141516171819202122&gt;&gt;&gt; data = np.zeros((2, ), dtype=[('A', 'i4'), ('B', 'f4'), ('C', 'a10')])&gt;&gt;&gt; dataarray([(0, 0., b''), (0, 0., b'')], dtype=[('A', '&lt;i4'), ('B', '&lt;f4'), ('C', 'S10')])&gt;&gt;&gt; data[:] = [(1, 2., 'Hello'), (2, 3., 'World')]&gt;&gt;&gt; pd.DataFrame(data) A B C0 1 2.0 b'Hello'1 2 3.0 b'World'&gt;&gt;&gt; pd.DataFrame(data, index=['first', 'second']) A B Cfirst 1 2.0 b'Hello'second 2 3.0 b'World'&gt;&gt;&gt; pd.DataFrame(data, index=['first', 'second'], columns=['C', 'A', 'B']) C A Bfirst b'Hello' 1 2.0second b'World' 2 3.0 从字典列表里创建(a list of dicts) 12345678910111213141516&gt;&gt;&gt; data2 = [&#123;"a": 1, "b": 2&#125;, &#123;"a": 5, "b": 10, "c": 20&#125;]&gt;&gt;&gt; pd.DataFrame(data2) a b c0 1 2 NaN1 5 10 20.0&gt;&gt;&gt; pd.DataFrame(data2, index=["first", "second"]) a b cfirst 1 2 NaNsecond 5 10 20.0&gt;&gt;&gt; pd.DataFrame(data2, columns=["a", "b"]) a b0 1 21 5 10 从元祖字典创建（From a dict of tuples） 通过元祖字典，可以创建多索引的DataFrame 1234567891011&gt;&gt;&gt; pd.DataFrame(&#123;('a', 'b'): &#123;('A', 'B'): 1, ('A', 'C'): 2&#125;,... ('a', 'a'): &#123;('A', 'C'): 3, ('A', 'B'): 4&#125;,... ('a', 'c'): &#123;('A', 'B'): 5, ('A', 'C'): 6&#125;,... ('b', 'a'): &#123;('A', 'C'): 7, ('A', 'B'): 8&#125;,... ('b', 'b'): &#123;('A', 'D'): 9, ('A', 'B'): 10&#125;&#125;)... a b a b c a bA B 4.0 1.0 5.0 8.0 10.0 C 3.0 2.0 6.0 7.0 NaN D NaN NaN NaN NaN 9.0 通过Series创建(From a Series) 12345&gt;&gt;&gt; pd.DataFrame(pd.Series([1,2,3])) 00 11 22 3 查看数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&gt;&gt;&gt; dates = pd.date_range('20130101', periods=6)&gt;&gt;&gt; datesDatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))&gt;&gt;&gt; df A B C D2013-01-01 1.231897 -0.169839 1.333295 0.3671422013-01-02 -0.127450 -1.716671 0.910350 0.1511862013-01-03 -0.241652 -0.984647 0.788656 -0.2036392013-01-04 0.044990 -0.255158 -1.213848 1.0767152013-01-05 0.418213 0.107400 0.619448 1.4940872013-01-06 -1.831020 0.813526 0.403101 -1.251946# 获取前几行(默认前5行)&gt;&gt;&gt; df.head() A B C D2013-01-01 1.231897 -0.169839 1.333295 0.3671422013-01-02 -0.127450 -1.716671 0.910350 0.1511862013-01-03 -0.241652 -0.984647 0.788656 -0.2036392013-01-04 0.044990 -0.255158 -1.213848 1.0767152013-01-05 0.418213 0.107400 0.619448 1.494087# 获取后3行&gt;&gt;&gt; df.tail(3) A B C D2013-01-04 0.044990 -0.255158 -1.213848 1.0767152013-01-05 0.418213 0.107400 0.619448 1.4940872013-01-06 -1.831020 0.813526 0.403101 -1.251946# 获取索引&gt;&gt;&gt; df.indexDatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D')# 获取列信息&gt;&gt;&gt; df.columnsIndex(['A', 'B', 'C', 'D'], dtype='object')# 获取数据信息&gt;&gt;&gt; df.valuesarray([[ 1.23189704, -0.16983942, 1.3332949 , 0.36714191], [-0.12744988, -1.71667129, 0.91034961, 0.15118638], [-0.24165226, -0.98464711, 0.78865554, -0.20363944], [ 0.04498958, -0.25515787, -1.21384804, 1.07671506], [ 0.41821265, 0.10740007, 0.61944799, 1.49408712], [-1.8310196 , 0.81352564, 0.40310115, -1.25194611]])# 获取简单的统计信息&gt;&gt;&gt; df.describe() A B C Dcount 6.000000 6.000000 6.000000 6.000000mean -0.084170 -0.367565 0.473500 0.272257std 1.007895 0.880134 0.883494 0.970912min -1.831020 -1.716671 -1.213848 -1.25194625% -0.213102 -0.802275 0.457188 -0.11493350% -0.041230 -0.212499 0.704052 0.25916475% 0.324907 0.038090 0.879926 0.899322max 1.231897 0.813526 1.333295 1.494087# 转置矩阵&gt;&gt;&gt; df.T 2013-01-01 2013-01-02 2013-01-03 2013-01-04 2013-01-05 2013-01-06A 1.231897 -0.127450 -0.241652 0.044990 0.418213 -1.831020B -0.169839 -1.716671 -0.984647 -0.255158 0.107400 0.813526C 1.333295 0.910350 0.788656 -1.213848 0.619448 0.403101D 0.367142 0.151186 -0.203639 1.076715 1.494087 -1.251946# 按照列排序&gt;&gt;&gt; df.sort_values(by='B') A B C D2013-01-02 -0.127450 -1.716671 0.910350 0.1511862013-01-03 -0.241652 -0.984647 0.788656 -0.2036392013-01-04 0.044990 -0.255158 -1.213848 1.0767152013-01-01 1.231897 -0.169839 1.333295 0.3671422013-01-05 0.418213 0.107400 0.619448 1.4940872013-01-06 -1.831020 0.813526 0.403101 -1.251946 选择数据 获取 选择列， 返回的是Series 1234567891011121314151617&gt;&gt;&gt; df['A']2013-01-01 1.2318972013-01-02 -0.1274502013-01-03 -0.2416522013-01-04 0.0449902013-01-05 0.4182132013-01-06 -1.831020Freq: D, Name: A, dtype: float64&gt;&gt;&gt; df.A2013-01-01 1.2318972013-01-02 -0.1274502013-01-03 -0.2416522013-01-04 0.0449902013-01-05 0.4182132013-01-06 -1.831020Freq: D, Name: A, dtype: float64 选择行 1234567891011&gt;&gt;&gt; df[0:3] A B C D2013-01-01 1.231897 -0.169839 1.333295 0.3671422013-01-02 -0.127450 -1.716671 0.910350 0.1511862013-01-03 -0.241652 -0.984647 0.788656 -0.203639&gt;&gt;&gt; df["20130102":"20130104"] A B C D2013-01-02 -0.127450 -1.716671 0.910350 0.1511862013-01-03 -0.241652 -0.984647 0.788656 -0.2036392013-01-04 0.044990 -0.255158 -1.213848 1.076715 通过Label选择 1234567891011121314151617181920212223242526272829# 返回的Series&gt;&gt;&gt; df.loc[dates[0]]A 1.231897B -0.169839C 1.333295D 0.367142Name: 2013-01-01 00:00:00, dtype: float64# 返回的DateFrame&gt;&gt;&gt; df.loc[:, ['A', 'B']] A B2013-01-01 1.231897 -0.1698392013-01-02 -0.127450 -1.7166712013-01-03 -0.241652 -0.9846472013-01-04 0.044990 -0.2551582013-01-05 0.418213 0.1074002013-01-06 -1.831020 0.813526&gt;&gt;&gt; df.loc['20130102':'20130104',['A','B']] A B2013-01-02 -0.127450 -1.7166712013-01-03 -0.241652 -0.9846472013-01-04 0.044990 -0.255158# 降维返回&gt;&gt;&gt; df.loc['20130102',['A','B']]A -0.127450B -1.716671Name: 2013-01-02 00:00:00, dtype: float64 通过Position选择 12345678910111213141516171819202122232425# 返回第4行&gt;&gt;&gt; df.iloc[3]A 0.044990B -0.255158C -1.213848D 1.076715Name: 2013-01-04 00:00:00, dtype: float64&gt;&gt;&gt; df.iloc[3:5,0:2] A B2013-01-04 0.044990 -0.2551582013-01-05 0.418213 0.107400&gt;&gt;&gt; df.iloc[1:3, :] A B C D2013-01-02 -0.127450 -1.716671 0.910350 0.1511862013-01-03 -0.241652 -0.984647 0.788656 -0.203639# 获得指定位置的元素&gt;&gt;&gt; df.iloc[1,1]-1.7166712884342545&gt;&gt;&gt; df.iat[1,1]-1.7166712884342545 布尔索引 1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; df[df.A &gt; 0] A B C D2013-01-01 1.231897 -0.169839 1.333295 0.3671422013-01-04 0.044990 -0.255158 -1.213848 1.0767152013-01-05 0.418213 0.107400 0.619448 1.494087&gt;&gt;&gt; df[df &gt; 0] A B C D2013-01-01 1.231897 NaN 1.333295 0.3671422013-01-02 NaN NaN 0.910350 0.1511862013-01-03 NaN NaN 0.788656 NaN2013-01-04 0.044990 NaN NaN 1.0767152013-01-05 0.418213 0.107400 0.619448 1.4940872013-01-06 NaN 0.813526 0.403101 NaN&gt;&gt;&gt; df2=df.copy()&gt;&gt;&gt; df2['E'] = ['one','one','two','three','four','three']&gt;&gt;&gt; df2 A B C D E2013-01-01 1.231897 -0.169839 1.333295 0.367142 one2013-01-02 -0.127450 -1.716671 0.910350 0.151186 one2013-01-03 -0.241652 -0.984647 0.788656 -0.203639 two2013-01-04 0.044990 -0.255158 -1.213848 1.076715 three2013-01-05 0.418213 0.107400 0.619448 1.494087 four2013-01-06 -1.831020 0.813526 0.403101 -1.251946 three# 使用isin()来过滤&gt;&gt;&gt; df2[df2['E'].isin(['two', 'four'])] A B C D E2013-01-03 -0.241652 -0.984647 0.788656 -0.203639 two2013-01-05 0.418213 0.107400 0.619448 1.494087 four 赋值 根据日期新增加一列 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&gt;&gt;&gt; s12013-01-02 12013-01-03 22013-01-04 32013-01-05 42013-01-06 52013-01-07 6Freq: D, dtype: int64&gt;&gt;&gt; df['F'] = s1&gt;&gt;&gt; df A B C D F2013-01-01 1.231897 -0.169839 1.333295 0.367142 NaN2013-01-02 -0.127450 -1.716671 0.910350 0.151186 1.02013-01-03 -0.241652 -0.984647 0.788656 -0.203639 2.02013-01-04 0.044990 -0.255158 -1.213848 1.076715 3.02013-01-05 0.418213 0.107400 0.619448 1.494087 4.02013-01-06 -1.831020 0.813526 0.403101 -1.251946 5.0# 通过label赋值&gt;&gt;&gt; df.at[dates[0], 'A'] = 0# 通过position赋值&gt;&gt;&gt; df.iat[0,1] = 0# 通过ndarray赋值&gt;&gt;&gt; df.loc[:, 'D'] = np.array([5] * len(df))&gt;&gt;&gt; df A B C D F2013-01-01 0.000000 0.000000 1.333295 5 NaN2013-01-02 -0.127450 -1.716671 0.910350 5 1.02013-01-03 -0.241652 -0.984647 0.788656 5 2.02013-01-04 0.044990 -0.255158 -1.213848 5 3.02013-01-05 0.418213 0.107400 0.619448 5 4.02013-01-06 -1.831020 0.813526 0.403101 5 5.0# 通过where操作&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))&gt;&gt;&gt; df A B C D2013-01-01 -1.231777 -0.068987 -0.105402 1.5120762013-01-02 -1.120426 -0.240417 0.223964 -0.5597932013-01-03 0.697097 0.758780 -1.191408 -0.7938822013-01-04 0.332519 0.784564 0.805932 -1.1691862013-01-05 0.010235 0.156115 0.419567 -2.2792142013-01-06 0.294819 -0.691370 0.294119 -0.208475&gt;&gt;&gt; df2 = df.copy()&gt;&gt;&gt; df2[df &gt; 0] = -df2&gt;&gt;&gt; df2 A B C D2013-01-01 -1.231777 -0.068987 -0.105402 -1.5120762013-01-02 -1.120426 -0.240417 -0.223964 -0.5597932013-01-03 -0.697097 -0.758780 -1.191408 -0.7938822013-01-04 -0.332519 -0.784564 -0.805932 -1.1691862013-01-05 -0.010235 -0.156115 -0.419567 -2.2792142013-01-06 -0.294819 -0.691370 -0.294119 -0.208475 数据缺失 pandas使用np.nan来表示缺失的数据，它默认不参与任何运算 1234567891011121314151617181920212223242526272829303132333435363738&gt;&gt;&gt; df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E'])&gt;&gt;&gt; df1 A B C D F E2013-01-01 0.000000 0.000000 1.333295 5 NaN NaN2013-01-02 -0.127450 -1.716671 0.910350 5 1.0 NaN2013-01-03 -0.241652 -0.984647 0.788656 5 2.0 NaN2013-01-04 0.044990 -0.255158 -1.213848 5 3.0 NaN&gt;&gt;&gt; df1.loc[dates[0]:dates[1], 'E'] = 1&gt;&gt;&gt; df1 A B C D F E2013-01-01 0.000000 0.000000 1.333295 5 NaN 1.02013-01-02 -0.127450 -1.716671 0.910350 5 1.0 1.02013-01-03 -0.241652 -0.984647 0.788656 5 2.0 NaN2013-01-04 0.044990 -0.255158 -1.213848 5 3.0 NaN# 丢弃所有包含NaN的行&gt;&gt;&gt; df1.dropna(how='any') A B C D F E2013-01-02 -0.12745 -1.716671 0.91035 5 1.0 1.0# 填充所有包含NaN的元素&gt;&gt;&gt; df1.fillna(value=5) A B C D F E2013-01-01 0.000000 0.000000 1.333295 5 5.0 1.02013-01-02 -0.127450 -1.716671 0.910350 5 1.0 1.02013-01-03 -0.241652 -0.984647 0.788656 5 2.0 5.02013-01-04 0.044990 -0.255158 -1.213848 5 3.0 5.0# 获取元素值为nan的布尔掩码&gt;&gt;&gt; pd.isna(df1) A B C D F E2013-01-01 False False False False True False2013-01-02 False False False False False False2013-01-03 False False False False False True2013-01-04 False False False False False True 运算操作 Stats统计 运算操作都会排除NaN元素 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&gt;&gt;&gt; dates = pd.date_range('20130101', periods=6)&gt;&gt;&gt; df = pd.DataFrame(np.arange(24).reshape(6,4),index=dates,columns=list('ABCD'))&gt;&gt;&gt; df A B C D2013-01-01 0 1 2 32013-01-02 4 5 6 72013-01-03 8 9 10 112013-01-04 12 13 14 152013-01-05 16 17 18 192013-01-06 20 21 22 23# 计算列的平均值&gt;&gt;&gt; df.mean()A 10.0B 11.0C 12.0D 13.0dtype: float64计算行的平均值&gt;&gt;&gt; df.mean(1)2013-01-01 1.52013-01-02 5.52013-01-03 9.52013-01-04 13.52013-01-05 17.52013-01-06 21.5Freq: D, dtype: float64# shift(n),按照列的方向，从上往下移动n个位置&gt;&gt;&gt; s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)&gt;&gt;&gt; s2013-01-01 NaN2013-01-02 NaN2013-01-03 1.02013-01-04 3.02013-01-05 5.02013-01-06 NaNFreq: D, dtype: float64# sub函数,DataFrame相减操作, 等于 df-s&gt;&gt;&gt; df.sub(s, axis='index') A B C D2013-01-01 NaN NaN NaN NaN2013-01-02 NaN NaN NaN NaN2013-01-03 7.0 8.0 9.0 10.02013-01-04 9.0 10.0 11.0 12.02013-01-05 11.0 12.0 13.0 14.02013-01-06 NaN NaN NaN NaN Apply 1234567891011121314151617181920212223242526&gt;&gt;&gt; df A B C D2013-01-01 0 1 2 32013-01-02 4 5 6 72013-01-03 8 9 10 112013-01-04 12 13 14 152013-01-05 16 17 18 192013-01-06 20 21 22 23# 在列方向累加&gt;&gt;&gt; df.apply(np.cumsum) A B C D2013-01-01 0 1 2 32013-01-02 4 6 8 102013-01-03 12 15 18 212013-01-04 24 28 32 362013-01-05 40 45 50 552013-01-06 60 66 72 78# 列方向的最大值-最小值， 得到的是一个Series&gt;&gt;&gt; df.apply(lambda x: x.max() - x.min())A 20B 20C 20D 20dtype: int64 直方图 Histogramming 123456789101112131415161718192021222324&gt;&gt;&gt; s = pd.Series(np.random.randint(0, 7, size=10))&gt;&gt;&gt; s0 61 52 03 24 55 16 37 38 39 1dtype: int64# 索引是出现的数字，值是次数&gt;&gt;&gt; s.value_counts()3 35 21 26 12 10 1dtype: int64 字符串方法 12345678910111213&gt;&gt;&gt; s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])&gt;&gt;&gt; s.str.lower()0 a1 b2 c3 aaba4 baca5 NaN6 caba7 dog8 catdtype: object 合并 Concat 12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(10, 4))&gt;&gt;&gt; df 0 1 2 30 -1.710767 -2.107488 1.441790 0.9599241 0.509422 0.099733 0.845039 0.2324622 -0.609247 0.533162 -0.387640 0.6688033 0.946219 -0.326805 1.245303 1.3360904 -1.069114 0.755313 -1.003991 -0.3270095 1.169418 -1.225637 -2.137500 1.7663416 -1.751095 0.279439 0.018053 1.8004357 -0.328828 -1.513893 1.879333 0.9452178 2.440123 -0.260918 -0.232951 -1.3377759 -0.876878 -1.153583 -1.487573 -1.509871# 分成小块&gt;&gt;&gt; pieces = [df[:3], df[3:7], df[7:]]# 合并&gt;&gt;&gt; pd.concat(pieces) 0 1 2 30 -1.710767 -2.107488 1.441790 0.9599241 0.509422 0.099733 0.845039 0.2324622 -0.609247 0.533162 -0.387640 0.6688033 0.946219 -0.326805 1.245303 1.3360904 -1.069114 0.755313 -1.003991 -0.3270095 1.169418 -1.225637 -2.137500 1.7663416 -1.751095 0.279439 0.018053 1.8004357 -0.328828 -1.513893 1.879333 0.9452178 2.440123 -0.260918 -0.232951 -1.3377759 -0.876878 -1.153583 -1.487573 -1.509871 Join 跟数据库的Join操作一样 1234567891011121314151617181920&gt;&gt;&gt; left = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'lval': [1, 2]&#125;)&gt;&gt;&gt; left key lval0 foo 11 foo 2&gt;&gt;&gt; right = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'rval': [4, 5]&#125;)&gt;&gt;&gt; right key rval0 foo 41 foo 5&gt;&gt;&gt; pd.merge(left, right, on='key') key lval rval0 foo 1 41 foo 1 52 foo 2 43 foo 2 5 另一个例子 123456789101112131415161718&gt;&gt;&gt; left = pd.DataFrame(&#123;'key': ['foo', 'bar'], 'lval': [1, 2]&#125;)&gt;&gt;&gt; left key lval0 foo 11 bar 2&gt;&gt;&gt; right = pd.DataFrame(&#123;'key': ['foo', 'bar'], 'rval': [4, 5]&#125;)&gt;&gt;&gt; right key rval0 foo 41 bar 5&gt;&gt;&gt; pd.merge(left, right, on='key') key lval rval0 foo 1 41 bar 2 5 Append 1234567891011121314151617181920212223242526&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])&gt;&gt;&gt; df A B C D0 -1.521762 -0.850721 1.322354 -0.2265621 -2.773304 -0.663303 0.895075 -0.1715242 0.322975 -0.796484 0.379920 0.0283333 -0.350795 1.839747 -0.359241 -0.0279214 -0.945340 1.062598 -2.208670 0.7690275 -0.329458 -0.145658 1.580258 -1.4148206 -0.261757 -1.435025 -0.512306 -0.2222877 -0.994207 -1.219057 0.781283 -1.795741&gt;&gt;&gt; s = df.iloc[3]&gt;&gt;&gt; df.append(s, ignore_index=True) A B C D0 -1.521762 -0.850721 1.322354 -0.2265621 -2.773304 -0.663303 0.895075 -0.1715242 0.322975 -0.796484 0.379920 0.0283333 -0.350795 1.839747 -0.359241 -0.0279214 -0.945340 1.062598 -2.208670 0.7690275 -0.329458 -0.145658 1.580258 -1.4148206 -0.261757 -1.435025 -0.512306 -0.2222877 -0.994207 -1.219057 0.781283 -1.7957418 -0.350795 1.839747 -0.359241 -0.027921 Grouping group by的操作需要经过以下1个或多个步骤 根据条件分组数据(Spliting) 在各个分组上执行函数(Applying) 合并结果(Combining) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&gt;&gt;&gt; df = pd.DataFrame(&#123;'A' : ['foo', 'bar', 'foo', 'bar',... 'foo', 'bar', 'foo', 'foo'],... 'B' : ['one', 'one', 'two', 'three',... 'two', 'two', 'one', 'three'],... 'C' : np.arange(1, 9), ... 'D' : np.arange(2, 10)&#125;)......&gt;&gt;&gt; df A B C D0 foo one 1 21 bar one 2 32 foo two 3 43 bar three 4 54 foo two 5 65 bar two 6 76 foo one 7 87 foo three 8 9# 分组求和&gt;&gt;&gt; df.groupby('A').sum() C DAbar 12 15foo 24 29# 多列分组&gt;&gt;&gt; df.groupby(['A','B']).sum() C DA Bbar one 2 3 three 4 5 two 6 7foo one 8 10 three 8 9 two 8 10&gt;&gt;&gt; b = df.groupby(['A','B']).sum()# 多索引&gt;&gt;&gt; b.indexMultiIndex(levels=[['bar', 'foo'], ['one', 'three', 'two']], labels=[[0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 1, 2]], names=['A', 'B'])&gt;&gt;&gt; b.columnsIndex(['C', 'D'], dtype='object') Reshaping Stack 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&gt;&gt;&gt; tuples = list(zip(*[['bar', 'bar', 'baz', 'baz',... 'foo', 'foo', 'qux', 'qux'],... ['one', 'two', 'one', 'two',... 'one', 'two', 'one', 'two']]))...&gt;&gt;&gt; tuples[('bar', 'one'), ('bar', 'two'), ('baz', 'one'), ('baz', 'two'), ('foo', 'one'), ('foo', 'two'), ('qux', 'one'), ('qux', 'two')]&gt;&gt;&gt; index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B'])&gt;&gt;&gt; df A Bfirst secondbar one 0.096893 0.479194 two -0.771606 0.331693baz one -0.022540 0.531284 two -0.039843 1.876942foo one 0.250473 1.163931 two -1.127163 1.447566qux one -0.410361 -0.734333 two -0.461247 0.018531&gt;&gt;&gt; df2 = df[:4]&gt;&gt;&gt; df2 A Bfirst secondbar one 0.096893 0.479194 two -0.771606 0.331693baz one -0.022540 0.531284 two -0.039843 1.876942&gt;&gt;&gt; stacked = df2.stack()&gt;&gt;&gt; stackedfirst secondbar one A 0.096893 B 0.479194 two A -0.771606 B 0.331693baz one A -0.022540 B 0.531284 two A -0.039843 B 1.876942dtype: float64&gt;&gt;&gt; type(stacked)pandas.core.series.Series&gt;&gt;&gt; stacked.indexMultiIndex(levels=[['bar', 'baz', 'foo', 'qux'], ['one', 'two'], ['A', 'B']], labels=[[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 1, 1, 0, 0, 1, 1], [0, 1, 0, 1, 0, 1, 0, 1]], names=['first', 'second', None])&gt;&gt;&gt; stacked.valuesarray([ 0.09689327, 0.47919417, -0.77160574, 0.3316934 , -0.02253955, 0.53128436, -0.03984337, 1.8769416 ])&gt;&gt;&gt; stacked.unstack() A Bfirst secondbar one 0.096893 0.479194 two -0.771606 0.331693baz one -0.022540 0.531284 two -0.039843 1.876942&gt;&gt;&gt; stacked.unstack(1)second one twofirstbar A 0.096893 -0.771606 B 0.479194 0.331693baz A -0.022540 -0.039843 B 0.531284 1.876942&gt;&gt;&gt; stacked.unstack(0)first bar bazsecondone A 0.096893 -0.022540 B 0.479194 0.531284two A -0.771606 -0.039843 B 0.331693 1.876942 数据透视表(Pivot Tables) 时间序列 pandas在时间序列上，提供了很方便的按照频率重新采样的功能，在财务分析上非常有用 123456# 把每秒的数据按5分钟聚合&gt;&gt;&gt; rng = pd.date_range('1/1/2012', periods=100, freq='S')&gt;&gt;&gt; ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)&gt;&gt;&gt; ts.resample('5Min').sum()2012-01-01 22073Freq: 5T, dtype: int64 加上时区信息 123456789101112131415161718192021&gt;&gt;&gt; rng = pd.date_range('3/6/2012 00:00', periods=5, freq='D')&gt;&gt;&gt; ts = pd.Series(np.random.randn(len(rng)), rng)&gt;&gt;&gt; ts2012-03-06 -0.3869742012-03-07 0.6577852012-03-08 1.3902342012-03-09 0.4129042012-03-10 -1.189340Freq: D, dtype: float64&gt;&gt;&gt; ts_utc = ts.tz_localize('UTC')&gt;&gt;&gt; ts_utc2012-03-06 00:00:00+00:00 -0.3869742012-03-07 00:00:00+00:00 0.6577852012-03-08 00:00:00+00:00 1.3902342012-03-09 00:00:00+00:00 0.4129042012-03-10 00:00:00+00:00 -1.189340Freq: D, dtype: float64 转换成另一个时区 1234567&gt;&gt;&gt; ts_utc.tz_convert('Asia/Shanghai')2012-03-06 08:00:00+08:00 -0.3869742012-03-07 08:00:00+08:00 0.6577852012-03-08 08:00:00+08:00 1.3902342012-03-09 08:00:00+08:00 0.4129042012-03-10 08:00:00+08:00 -1.189340Freq: D, dtype: float64 时间跨度转换 12345678910111213141516171819202122232425262728&gt;&gt;&gt; rng = pd.date_range('1/1/2012', periods=5, freq='M')&gt;&gt;&gt; ts = pd.Series(np.random.randn(len(rng)), index=rng)&gt;&gt;&gt; ts2012-01-31 0.8251742012-02-29 -2.1902582012-03-31 -0.0731712012-04-30 -0.4042082012-05-31 0.245025Freq: M, dtype: float64&gt;&gt;&gt; ps = ts.to_period()&gt;&gt;&gt; ps2012-01 0.8251742012-02 -2.1902582012-03 -0.0731712012-04 -0.4042082012-05 0.245025Freq: M, dtype: float64&gt;&gt;&gt; ps.to_timestamp()2012-01-01 0.8251742012-02-01 -2.1902582012-03-01 -0.0731712012-04-01 -0.4042082012-05-01 0.245025Freq: MS, dtype: float64 转换季度时间 123456789101112131415161718192021&gt;&gt;&gt; prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')&gt;&gt;&gt; ts = pd.Series(np.random.randn(len(prng)), prng)&gt;&gt;&gt; ts.head()1990Q1 -0.5900401990Q2 -0.7503921990Q3 -0.3855171990Q4 -0.3808061991Q1 -1.252727Freq: Q-NOV, dtype: float64&gt;&gt;&gt; ts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9&gt;&gt;&gt; ts.head()1990-03-01 09:00 -0.5900401990-06-01 09:00 -0.7503921990-09-01 09:00 -0.3855171990-12-01 09:00 -0.3808061991-03-01 09:00 -1.252727Freq: H, dtype: float64 Categoricals分类 12345678910&gt;&gt;&gt; df = pd.DataFrame(&#123;"id":[1,2,3,4,5,6], "raw_grade":['a', 'b', 'b', 'a', 'a', 'e']&#125;)&gt;&gt;&gt; df id raw_grade0 1 a1 2 b2 3 b3 4 a4 5 a5 6 e 转换原始类别为分类数据类型 1234567891011121314151617181920&gt;&gt;&gt; df["grade"] = df["raw_grade"].astype("category")&gt;&gt;&gt; df id raw_grade grade0 1 a a1 2 b b2 3 b b3 4 a a4 5 a a5 6 e e&gt;&gt;&gt; df["grade"]0 a1 b2 b3 a4 a5 eName: grade, dtype: categoryCategories (3, object): [a, b, e] 重命名分类为更有意义的名称 12345678910&gt;&gt;&gt; df["grade"].cat.categories = ["very good", "good", "very bad"]&gt;&gt;&gt; df id raw_grade grade0 1 a very good1 2 b good2 3 b good3 4 a very good4 5 a very good5 6 e very bad 重新安排顺分类,同时添加缺少的分类(序列 .cat方法下返回新默认序列) 1234567891011121314151617181920&gt;&gt;&gt; df["grade"] = df["grade"].cat.set_categories(["very bad", "bad", "medium", "good", "very good"])&gt;&gt;&gt; df id raw_grade grade0 1 a very good1 2 b good2 3 b good3 4 a very good4 5 a very good5 6 e very bad&gt;&gt;&gt; df["grade"]0 very good1 good2 good3 very good4 very good5 very badName: grade, dtype: categoryCategories (5, object): [very bad, bad, medium, good, very good] 按照分类排序 12345678&gt;&gt;&gt; df.sort_values(by="grade") id raw_grade grade5 6 e very bad1 2 b good2 3 b good0 1 a very good3 4 a very good4 5 a very good 按照分类分组，同时也会显示空的分类 12345678&gt;&gt;&gt; df.groupby("grade").size()gradevery bad 1bad 0medium 0good 2very good 3dtype: int64 Plotting 12345678&gt;&gt;&gt; import matplotlib.pyplot as plt&gt;&gt;&gt; ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))&gt;&gt;&gt; ts = ts.cumsum()&gt;&gt;&gt; ts.plot()&lt;matplotlib.axes._subplots.AxesSubplot at 0x108594668&gt;&gt;&gt;&gt; plt.show() 曲线图 画图带图例的图 123456789&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=['A','B'... ,'C', 'D'])&gt;&gt;&gt; df.cumsum()&gt;&gt;&gt; plt.figure();df.plot();plt.legend(loc='best')&lt;matplotlib.legend.Legend at 0x111793f98&gt;&gt;&gt;&gt; plt.show() 带图例的曲线图 数据In/Out CSV 保存到csv文件 1&gt;&gt;&gt; df.to_csv('foo.csv') 从csv文件读取数据 1&gt;&gt;&gt; pd.read_csv('foo.csv') HDF5 保存到HDF5仓库 1&gt;&gt;&gt; df.to_hdf('foo.h5','df') 从仓库读取 1&gt;&gt;&gt; pd.read_hdf('foo.h5','df') Excel 保存到excel 1&gt;&gt;&gt; df.to_excel('foo.xlsx', sheet_name='Sheet1') 从excel文件读取 1&gt;&gt;&gt; pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA']) 扩展阅读 pandas指南]]></content>
      <categories>
        <category>python数据分析</category>
      </categories>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NumPy快速入门指南]]></title>
    <url>%2F2017%2F12%2F19%2Fnumpy-qucikstart%2F</url>
    <content type="text"><![CDATA[参考: https://docs.scipy.org/doc/numpy-dev/user/quickstart.html 准备 安装numpy $ pip install numpy 基础 In NumPy dimensions are called axes. The number of axes is rank. Numpy中，维度被称作axes, 维度数被称作rank。 Numpy的数组类是ndarray, 与标准python库的数组不太一样，它包含的元素必须是相同类型的。 ndarray的常见属性如下: ndarray.ndim数组的轴数(即rank) ndarray.shape数组的维度，返回的是一个元组，元组的长度值刚好是ndim ndarray.size数组元素的个数 ndarray.dtype数组元素的类型 ndarray.itemsize数组元素的字节大小 ndarray.data数组包含的实际数据(一般情况下不会用到这个属性，都是通过索引来访问元素) 数组例子 1234567891011121314151617181920212223242526272829&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; a = np.arange(15).reshape(3,5)&gt;&gt;&gt; aarray([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])&gt;&gt;&gt; type(a)numpy.ndarray&gt;&gt;&gt; a.ndim2&gt;&gt;&gt; a.shape(3, 5)&gt;&gt;&gt; a.size15&gt;&gt;&gt; a.dtypedtype('int64')&gt;&gt;&gt; a.itemsize8&gt;&gt;&gt; a.data&lt;memory at 0x11221b120&gt; 创建数组 创建数组一般有如下几种方法: 通过array函数，可以从普通的python列表或元组来创建 123456789101112&gt;&gt;&gt; a=np.array([2,3,4])&gt;&gt;&gt; aarray([2, 3, 4])&gt;&gt;&gt; a.dtypedtype('int64')&gt;&gt;&gt; b = np.array([1.2, 3.5, 5.1])&gt;&gt;&gt; b.dtypedtype('float64') 一个常见的错误就是在调用array函数时， 传递多个参数，而不是一个列表 12&gt;&gt;&gt; a = np.array(1,2,3,4) # 错误&gt;&gt;&gt; a = np.array([1,2,3,4]) # 正确 array同样可以接受列表序列并将它转换为多维数组 1234&gt;&gt;&gt; c = np.array([(1.5, 2.3), (4,5,6)])&gt;&gt;&gt; carray([(1.5, 2.3), (4, 5, 6)], dtype=object) 同样可以在创建数组的时候，指定数据类型 12345&gt;&gt;&gt; d = np.array([[1,2], [3,4]], dtype=complex)&gt;&gt;&gt; darray([[ 1.+0.j, 2.+0.j], [ 3.+0.j, 4.+0.j]]) 通常情况下， 数组元素的初始数据是不知道的，但是知道数组的大小。因此Numpy提供了一些函数来创建指定大小的数组，并用占位符来填充数组。 zeros函数创建初始值为0的数组 ones创建初始值为1的数组 empty创建未初始化的随机数组 默认情况下，上面三个函数创建数组的元素类型都是float64。 123456789101112131415&gt;&gt;&gt; np.zeros((3,4))array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]])&gt;&gt;&gt; np.ones((3,4))array([[ 1., 1., 1., 1.], [ 1., 1., 1., 1.], [ 1., 1., 1., 1.]])&gt;&gt;&gt; np.empty((2,5))array([[ 2.68156159e+154, 2.68679227e+154, 2.37663529e-312, 2.56761491e-312, 8.48798317e-313], [ 9.33678148e-313, 8.70018275e-313, 2.02566915e-322, 0.00000000e+000, 6.95335581e-309]]) 为了创建序列函数，Numpy也提供了类似range函数的方法 12345&gt;&gt;&gt; np.arange(10, 30, 5)array([10, 15, 20, 25])&gt;&gt;&gt; np.arange(0, 2, 0.3)array([ 0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8]) 当使用arange函数的生成float类型的序列时，生成的序列有时候并不会按照我们预期的步长来生成，要实现这个效果，最好是用linspace函数来代替。例如: 12345678&gt;&gt;&gt; from numpy import pi&gt;&gt;&gt; np.linspace(0, 2, 9)array([ 0. , 0.25, 0.5 , 0.75, 1. , 1.25, 1.5 , 1.75, 2. ])&gt;&gt;&gt; x = np.linspace( 0, 2*pi, 100)&gt;&gt;&gt; f=np.sin(x) 除此之外，还可以使用下面的函数来创建数组 array zeros zeros_like # 创建一个和给定数组相同shape的全是0的数组 ones ones_like empty empty_like arange linspace numpy.random.rand # 从 [0, 1) 中返回一个或多个样本值。 numpy.random.randn # 从标准正态分布中返回一个或多个样本值。 fromfunction fromfile 打印数组 当使用print函数打印数组时，numpy会输出一个嵌套的列表形式 123456789101112131415161718192021# 一维数组&gt;&gt;&gt; a = np.arange(6)&gt;&gt;&gt; print(a)[0 1 2 3 4 5]# 二维数组&gt;&gt;&gt; b = np.arange(12).reshape(4,3)&gt;&gt;&gt; print(b)[[ 0 1 2] [ 3 4 5] [ 6 7 8] [ 9 10 11]]# 三维数组&gt;&gt;&gt; c = np.arange(24).reshape(2,3,4)&gt;&gt;&gt; print(c)[[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] 当数组包含的元素太多时，会省略中间的元素，只打印角落的元素 12345678910&gt;&gt;&gt; print(np.arange(10000))[ 0 1 2 ..., 9997 9998 9999]&gt;&gt;&gt; print(np.arange(10000).reshape(100,100))[[ 0 1 2 ..., 97 98 99] [ 100 101 102 ..., 197 198 199] [ 200 201 202 ..., 297 298 299] ..., [9700 9701 9702 ..., 9797 9798 9799] [9800 9801 9802 ..., 9897 9898 9899] [9900 9901 9902 ..., 9997 9998 9999]] 如果想禁用这个行为，强制打印所有的元素，可以开启set_printoptions选项 123456&gt;&gt;&gt; np.set_printoptions(threshold=np.nan)&gt;&gt;&gt; print(np.arange(100))[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] 还原成省略效果 1&gt;&gt;&gt; np.set_printoptions(threshold=1000) 设置打印浮点数的小数位数: 1&gt;&gt;&gt; np.set_printoptions(precision=4) # 设置打印浮点数的小数位数，默认是8位 基本操作 数组的算术运算会自动作用于每个元素，并返回一个新的数组 1234567891011121314151617&gt;&gt;&gt; a = np.array([20,30,40,50])&gt;&gt;&gt; b = np.arange(4)&gt;&gt;&gt; c = a - b&gt;&gt;&gt; carray([20, 29, 38, 47])&gt;&gt;&gt; b**2array([0, 1, 4, 9])&gt;&gt;&gt; 10 * np.sin(a)array([ 9.12945251, -9.88031624, 7.4511316 , -2.62374854])&gt;&gt;&gt; a &lt; 35array([ True, True, False, False], dtype=bool) *返回的是每个元素相乘的结果，要实现矩阵乘法，需要使用dot函数 12345678910111213141516171819&gt;&gt;&gt; a = np.array([ [1, 1],... [0, 1]])...&gt;&gt;&gt; b = np.array([ [2, 0],... [3, 4]])...&gt;&gt;&gt; a * b # 对应位置的元素相乘array([[2, 0], [0, 4]])&gt;&gt;&gt; a.dot(b) # 矩阵乘法array([[5, 4], [3, 4]])&gt;&gt;&gt; np.dot(a, b) # 另一种形式的矩阵乘法array([[5, 4], [3, 4]]) 一些操作， 如+=和*=是直接修改原有的数组，而不是新建一个 1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; a = np.ones((2,3), dtype=int)&gt;&gt;&gt; aarray([[1, 1, 1], [1, 1, 1]])&gt;&gt;&gt; b = np.random.random((2,3))&gt;&gt;&gt; barray([[ 0.7216234 , 0.5813183 , 0.21175569], [ 0.11697569, 0.89835328, 0.06088455]])&gt;&gt;&gt; a.dtypedtype('int64')&gt;&gt;&gt; b.dtypedtype('float64')&gt;&gt;&gt; b+=a&gt;&gt;&gt; barray([[ 1.7216234 , 1.5813183 , 1.21175569], [ 1.11697569, 1.89835328, 1.06088455]])# 报错的原因是因为a数组原来是保存int64类型，现在没法保存float64类型&gt;&gt;&gt; a+=b---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-11-0a45668e3cc6&gt; in &lt;module&gt;()----&gt; 1 a+=bTypeError: Cannot cast ufunc add output from dtype('float64') to dtype('int64') with casting rule 'same_kind' 当不同类型的数组运算操作时，总是向精度更高的自动转换 1234567891011121314151617181920212223&gt;&gt;&gt; a = np.ones(3, dtype=np.int32)&gt;&gt;&gt; b = np.linspace(0, np.pi, 3)&gt;&gt;&gt; b.dtype.name'float64'&gt;&gt;&gt; c = a + b&gt;&gt;&gt; carray([ 1. , 2.57079633, 4.14159265])&gt;&gt;&gt; c.dtype.name'float64'&gt;&gt;&gt; d = np.exp(c*1j)&gt;&gt;&gt; darray([ 0.54030231+0.84147098j, -0.84147098+0.54030231j, -0.54030231-0.84147098j])&gt;&gt;&gt; d.dtype.name'complex128' ndarray包含了很多一元运算。如求和等 123456789101112131415&gt;&gt;&gt; a = np.arange(15).reshape(3, 5)&gt;&gt;&gt; aarray([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])&gt;&gt;&gt; a.sum()105&gt;&gt;&gt; a.min()0&gt;&gt;&gt; a.max()14 默认情况下，这些操作都是作用于每一个元素，而不管它的维度。但是，我们也可以通过axis参数来限定操作的轴 1234567891011121314151617181920&gt;&gt;&gt; b = np.arange(12).reshape(3, 4)&gt;&gt;&gt; barray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])# 计算每一列的和&gt;&gt;&gt; b.sum(axis=0)array([12, 15, 18, 21])# 计算每一行的最小值&gt;&gt;&gt; b.min(axis=1)array([0, 4, 8])# 每一行累积和&gt;&gt;&gt; b.cumsum(axis=1)array([[ 0, 1, 3, 6], [ 4, 9, 15, 22], [ 8, 17, 27, 38]]) 通用函数 Numpy提供了很多常见的数学上的运算，如sin, cos, exp。在Numpy中，我们称这些为“universal functions”（ufunc） 123456789101112131415161718&gt;&gt;&gt; B = np.arange(3)&gt;&gt;&gt; Barray([0, 1, 2])&gt;&gt;&gt; np.exp(B)array([ 1. , 2.71828183, 7.3890561 ])&gt;&gt;&gt; 2.71828183 * 2.718281837.389056107308149&gt;&gt;&gt; np.sqrt(B)array([ 0. , 1. , 1.41421356])&gt;&gt;&gt; C = np.array([2., -1., 4.])&gt;&gt;&gt; np.add(B, C)array([ 2., 0., 6.]) 索引，切片和迭代 一维数组的索引，切片，迭代跟普通的python列表一样 1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; a = np.arange(10) ** 3&gt;&gt;&gt; aarray([ 0, 1, 8, 27, 64, 125, 216, 343, 512, 729])&gt;&gt;&gt; a[2]8&gt;&gt;&gt; a[2:5]array([ 8, 27, 64])&gt;&gt;&gt; a[:6:2] # 等价于a[0:6:2]array([ 0, 8, 64])&gt;&gt;&gt; a[:6:2] = -1000&gt;&gt;&gt; aarray([-1000, 1, -1000, 27, -1000, 125, 216, 343, 512, 729])&gt;&gt;&gt; a[::-1] # 反转数组aarray([ 729, 512, 343, 216, 125, -1000, 27, -1000, 1, -1000])&gt;&gt;&gt; for i in a:... print(i**(1/3.))...nan1.0nan3.0nan5.06.07.08.09.0 多维数组可以在每个轴上索引，多个索引用,分隔 12345678910111213141516171819202122232425262728&gt;&gt;&gt; def f(x,y):... return 10*x +y...&gt;&gt;&gt; b=np.fromfunction(f, (5,4), dtype=int)&gt;&gt;&gt; barray([[ 0, 1, 2, 3], [10, 11, 12, 13], [20, 21, 22, 23], [30, 31, 32, 33], [40, 41, 42, 43]])&gt;&gt;&gt; help(np.fromfunction)&gt;&gt;&gt; b[2,3]23&gt;&gt;&gt; b[0:5, 1]array([ 1, 11, 21, 31, 41])&gt;&gt;&gt; b[:, 1]array([ 1, 11, 21, 31, 41])&gt;&gt;&gt; b[1:3, :]array([[10, 11, 12, 13], [20, 21, 22, 23]]) 当索引数少于轴数时，缺失的索引认为是全切片: 12&gt;&gt;&gt; b[-1] # 等价于 b[-1,:]array([40, 41, 42, 43]) 同样可以使用...来表示全切片，它代表补全剩下的所有索引。例如数组x, rank是5.那么 x[1,2,...]等价于x[1,2,:,:,:] x[...,3]等价于x[:,:,:,:,3] x[4,...,5,:]等价于x[4,:,:,5,:] 12345678910111213141516&gt;&gt;&gt; c = np.array([[[0,1,2],... [10,12,13]],... [[100,101,102],... [110,112,113]]])...&gt;&gt;&gt; c.shape(2, 2, 3)&gt;&gt;&gt; c[1,...]array([[100, 101, 102], [110, 112, 113]])&gt;&gt;&gt; c[...,2]array([[ 2, 13], [102, 113]]) 多维数组的迭代是根据第一个轴来操作的 123456789101112131415&gt;&gt;&gt; barray([[ 0, 1, 2, 3], [10, 11, 12, 13], [20, 21, 22, 23], [30, 31, 32, 33], [40, 41, 42, 43]])&gt;&gt;&gt; for row in b:... print(row)...[0 1 2 3][10 11 12 13][20 21 22 23][30 31 32 33][40 41 42 43] 如果想遍历每个元素，可以使用flat属性 1234567891011121314151617181920212223&gt;&gt;&gt; for element in b.flat:... print(element)...012310111213202122233031323340414243 shape操作 改变数组的shape 许多函数都可以改变数组的shape，但是它们都是返回一个新的修改后的数组，并不会改变原数组 1234567891011121314151617181920212223242526272829303132333435&gt;&gt;&gt; a = np.floor(10*np.random.random((3,4)))&gt;&gt;&gt; a.shape(3, 4)&gt;&gt;&gt; aarray([[ 7., 8., 0., 9.], [ 8., 4., 9., 8.], [ 4., 3., 7., 0.]])# 返回降维的数组&gt;&gt;&gt; a.ravel()array([ 7., 8., 0., 9., 8., 4., 9., 8., 4., 3., 7., 0.])# 直接修改shape&gt;&gt;&gt; a.reshape(6,2)array([[ 7., 8.], [ 0., 9.], [ 8., 4.], [ 9., 8.], [ 4., 3.], [ 7., 0.]])# 数组转置&gt;&gt;&gt; a.Tarray([[ 7., 8., 4.], [ 8., 4., 3.], [ 0., 9., 7.], [ 9., 8., 0.]])&gt;&gt;&gt; a.T.shape(4, 3)&gt;&gt;&gt; a.shape(3, 4) reshape返回修改后的数组，不改变数组本身，但是resize函数直接修改原数组 12345678910&gt;&gt;&gt; aarray([[ 7., 8., 0., 9.], [ 8., 4., 9., 8.], [ 4., 3., 7., 0.]])&gt;&gt;&gt; a.resize((2,6))&gt;&gt;&gt; aarray([[ 7., 8., 0., 9., 8., 4.], [ 9., 8., 4., 3., 7., 0.]]) 如果一个维度为的是-1, 那么reshape函数会自动计算它的值。 12345678&gt;&gt;&gt; aarray([[ 7., 8., 0., 9., 8., 4.], [ 9., 8., 4., 3., 7., 0.]])&gt;&gt;&gt; a.reshape(3, -1)array([[ 7., 8., 0., 9.], [ 8., 4., 9., 8.], [ 4., 3., 7., 0.]]) 数组合并 多个数组可以根据不同的轴组合在一起 123456789101112131415161718192021&gt;&gt;&gt; a = np.floor(10*np.random.random((2,2)))&gt;&gt;&gt; aarray([[ 1., 1.], [ 4., 4.]])&gt;&gt;&gt; b = np.floor(10*np.random.random((2,2)))&gt;&gt;&gt; barray([[ 2., 9.], [ 0., 3.]])&gt;&gt;&gt; np.vstack((a, b))array([[ 1., 1.], [ 4., 4.], [ 2., 9.], [ 0., 3.]])&gt;&gt;&gt; np.hstack((a,b))array([[ 1., 1., 2., 9.], [ 4., 4., 0., 3.]]) column_stack函数把1维数组当做列来拼成2维数组，如果只是操作2维数组，跟hstack的等效。 123456789101112131415161718192021222324252627282930313233343536&gt;&gt;&gt; aarray([[ 1., 1.], [ 4., 4.]])&gt;&gt;&gt; barray([[ 2., 9.], [ 0., 3.]])# 操作2维数组，等效于hstack&gt;&gt;&gt; np.column_stack((a, b))array([[ 1., 1., 2., 9.], [ 4., 4., 0., 3.]])&gt;&gt;&gt; a = np.array([4., 2.])&gt;&gt;&gt; b = np.array([3., 8.])# 操作1维数组，返回2维数组，a,b分别为2维数组的列&gt;&gt;&gt; np.column_stack((a, b))array([[ 4., 3.], [ 2., 8.]])&gt;&gt;&gt; from numpy import newaxis# 将1维数组变成2维数组&gt;&gt;&gt; a[:,newaxis]array([[ 4.], [ 2.]])# 都是操作二维数组，下面两个操作column_stack和hstack等效&gt;&gt;&gt; np.column_stack((a[:, newaxis], b[:, newaxis]))array([[ 4., 3.], [ 2., 8.]])&gt;&gt;&gt; np.hstack((a[:, newaxis], b[:, newaxis]))array([[ 4., 3.], [ 2., 8.]]) 另外不论什么数组，row_stack函数等效于vstack。通常来说，2维以上的数组，hstack基于第2根轴做运算，vstack基于第1根轴，concatenate函数额外多接受一个参数，可以指定基于哪根轴做数组的合并操作。 另外, r_和c_函数对于在一个轴上组合数据相当哟偶用，他们允许使用范围符号: 12&gt;&gt;&gt; np.r_[1:4, 0, 4]array([1, 2, 3, 0, 4]) 数组切割 使用hsplit函数，你可以在水平方向切割一个数组 123456789101112131415161718192021&gt;&gt;&gt; aarray([[ 9., 0., 2., 0., 0., 4., 1., 6., 4., 8., 3., 9.], [ 5., 3., 0., 5., 5., 8., 0., 5., 6., 3., 8., 7.]])# 切割成3个数组&gt;&gt;&gt; np.hsplit(a, 3)[array([[ 9., 0., 2., 0.], [ 5., 3., 0., 5.]]), array([[ 0., 4., 1., 6.], [ 5., 8., 0., 5.]]), array([[ 4., 8., 3., 9.], [ 6., 3., 8., 7.]])] &gt;&gt;&gt; np.vsplit(a, 2)[array([[ 9., 0., 2., 0., 0., 4., 1., 6., 4., 8., 3., 9.]]), array([[ 5., 3., 0., 5., 5., 8., 0., 5., 6., 3., 8., 7.]])]# 基于第3和第4列切割&gt;&gt;&gt; np.hsplit(a, (3,4))[array([[ 9., 0., 2.], [ 5., 3., 0.]]), array([[ 0.], [ 5.]]), array([[ 0., 4., 1., 6., 4., 8., 3., 9.], [ 5., 8., 0., 5., 6., 3., 8., 7.]])] vsplit可以基于垂直轴切割，array_split可以指定基于哪个轴切割 复制和视图(Views) 当进行数组运算和改变数组时，有时候数据是被复制到一个新的数组，有时候不是。对于初学者来说，对于具体是哪种操作，很容易混淆。 主要分三种情况。 一点也不复制 12345678910111213141516171819&gt;&gt;&gt; a = np.arange(12)&gt;&gt;&gt; b = a # 不会有新对象产生&gt;&gt;&gt; b is a # a和b是同一个数组True&gt;&gt;&gt; b.shape(12,)&gt;&gt;&gt; b.shape = 3, 4 # 改变b的shape, a也同样变化&gt;&gt;&gt; a.shape(3, 4)&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) python中使用可变参数时，可以看做是引用传参，因此函数调用不会复制 123456789&gt;&gt;&gt; def f(x):... print(id(x))...&gt;&gt;&gt; id(a)4361164320&gt;&gt;&gt; f(a)4361164320 视图(View)和浅复制(Shallow Copy) 不同的数组可以共享数据，view函数可以创造一个数据相同的新数组 12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; c = a.view()&gt;&gt;&gt; c is a # c和a不是同一个数组False&gt;&gt;&gt; c.base is a # c是a的数据的视图True&gt;&gt;&gt; c.flags.owndataFalse&gt;&gt;&gt; c.shape = 2, 6 # a的不会改变&gt;&gt;&gt; a.shape(3, 4)&gt;&gt;&gt; c[0, 4] = 1234 # a的数据发生改变&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [1234, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; carray([[ 0, 1, 2, 3, 1234, 5], [ 6, 7, 8, 9, 10, 11]]) 一个数组的切片返回的就是它的视图 123456789101112131415161718&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [1234, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; s = a [:, 1:3]&gt;&gt;&gt; sarray([[ 1, 2], [ 5, 6], [ 9, 10]])&gt;&gt;&gt; s[:] = 10 # s[:]是s的视图&gt;&gt;&gt; sarray([[10, 10], [10, 10], [10, 10]]) 深度复制(Deep Copy) copy方法可以完全复制数组和它的数据 1234567891011121314151617181920212223242526&gt;&gt;&gt; a = np.arange(12).reshape((3,4))&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; d = a.copy()&gt;&gt;&gt; d is aFalse&gt;&gt;&gt; d.base is aFalse&gt;&gt;&gt; d[0, 0] = 9999&gt;&gt;&gt; darray([[9999, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) 函数和方法概览 如下是按照分类整理的常用函数和方法，完整的分类可以参考Routines 数组创建 arange array copy empty empty_like eye # 创建一个对角线全是1的二维数组 fromfile fromfunction identity # 创建一个对角线全是1的方形矩阵，与eye方法差不多，只是可以接受的参数不同 linspace logspace # 创建等比数列 mgrid orgid ones ones_like zeros zeros_like 转换 ndarray.astype # 改变数组的元素格式 atleast_1d # 将输入转换为至少1维数组 atleast_2d alteast_3d mat # 将输入转换为矩阵 处理 array_split column_stack concatenate diagonal dsplit dstack hsplit hstack ndarray.item newaxis ravel repeat reshape resize squeeze swapaxes take transpose vsplit vstack Questions all any nonezero where 排序 argmax # 返回最大值的索引 argmin # 返回最小值的索引 argsort # 返回排序后的索引 max min ptp searchsorted sort 运算 choose compress cumprod cumsum inner ndarray.fill imag prod put putmask real sum 基本统计 cov mean std var 线性代数 cross dot outer linalg svd vdot Less Basic 广播机制 属于广播主要描述于numpy对于不同shape的数组如何进行算术运算。受限于一些特定约束。 一般都是小的数组扩展为大的数组，以便能计算。 通常情况下，numpy操作的数组必须是相同shape的。 12345678&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; a = np.array([1.0, 2.0, 3.0])&gt;&gt;&gt; b = np.array([2.0, 2.0, 2.0])&gt;&gt;&gt; a * barray([ 2., 4., 6.]) 当数组的shape满足某些特定约束时，numpy的广播机制可以使这个约束更宽松。最简单的就是广播例子就是当数组和一个标量操作时。 123456&gt;&gt;&gt; a = np.array([1.0, 2.0, 3.0])&gt;&gt;&gt; b = 2.0&gt;&gt;&gt; a * barray([ 2., 4., 6.]) 上面两个例子的结果是一样的，我们可以认为标量b被扩展为了和a同样shape的数组，b中的新元素就是原来标量的拷贝。这个扩展策略仅仅是概念上的，实际上Numpy足够聪明，能自动使用标量做运算，而不需要复制任何东西。所以广播运算从计算内存上来说更优秀。 要能满足广播，必须符合下面两条规则： 广播之后，输出数组的shape是输入数组shape的各个轴上的最大值，然后沿着较大shape属性的方向复制延伸； 要进行广播机制，要么两个数组的shape属性一样，要么其中有一个数组的shape属性必须有一个等于1； 更多可以参考: https://docs.scipy.org/doc/numpy-dev/user/basics.broadcasting.html http://www.labri.fr/perso/nrougier/from-python-to-numpy/?utm_source=mybridge&amp;utm_medium=blog&amp;utm_campaign=read_more#broadcasting 索引 numpy除了支持普通的python方式的索引和切片之外，还支持整数数组或布尔数组索引 数组索引 1234567891011121314151617181920&gt;&gt;&gt; a = np.arange(12) ** 2&gt;&gt;&gt; i = np.array([1,1,3,8,5])&gt;&gt;&gt; aarray([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121])&gt;&gt;&gt; iarray([1, 1, 3, 8, 5])# 返回a中在索引i的元素&gt;&gt;&gt; a[i]array([ 1, 1, 9, 64, 25])&gt;&gt;&gt; j = np.array([[3,4], [9,7]])# 二维数组索引，返回a中在索引j的元素&gt;&gt;&gt; a[j]array([[ 9, 16], [81, 49]]) 当数组索引作用在多维数组时，是根据数组的第一个维度来索引的。 123456789101112131415161718192021&gt;&gt;&gt; palette = np.array([ [0 ,0, 0],... [255, 0, 0],... [0, 255, 0],... [0, 0, 255],... [255, 255, 255] ])...&gt;&gt;&gt; image = np.array([ [0, 1, 2, 0],... [0, 3, 4, 0] ])...&gt;&gt;&gt; palette[image]array([[[ 0, 0, 0], [255, 0, 0], [ 0, 255, 0], [ 0, 0, 0]], [[ 0, 0, 0], [ 0, 0, 255], [255, 255, 255], [ 0, 0, 0]]]) 索引同样可以是多维的，但是必须是相同的shape 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&gt;&gt;&gt; a = np.arange(12).reshape(3, 4)&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])# indices for the first dim of a&gt;&gt;&gt; i = np.array([[0,1],... [1,2] ])...# indices for the second dim&gt;&gt;&gt; j = np.array([[2, 1],... [3, 3] ])...# i and j must have equal shape# 返回的结果是是[ [a[0,2], a[1,1]# [a[1,3], a[2, 3] ]&gt;&gt;&gt; a[i, j]array([[ 2, 5], [ 7, 11]])# [ a[0,2], a[1, 2],# a[1,2], a[2, 2] ]&gt;&gt;&gt; a[i, 2]array([[ 2, 6], [ 6, 10]])# [[[ a[0,2], a[0,1],# a[0,3], a[0,3]],## a[1,2], a[1,1],# a[1,3], a[1,3]],## a[2,2], a[2,1],# a[2,3], a[2,3]]]&gt;&gt;&gt; a[:, j]array([[[ 2, 1], [ 3, 3]], [[ 6, 5], [ 7, 7]], [[10, 9], [11, 11]]]) 同样，我们可以把i和j放在一个列表里，然后用列表做索引 123456&gt;&gt;&gt; l = [i, j]# 等价于a[i, j]&gt;&gt;&gt; a[l]array([[ 2, 5], [ 7, 11]]) 但是，我们不可以把i和j放在一个数组里，因为数组索引是作用在第一个维度上的。 123456789101112131415161718192021&gt;&gt;&gt; s = np.array([i, j])&gt;&gt;&gt; sarray([[[0, 1], [1, 2]], [[2, 1], [3, 3]]])&gt;&gt;&gt; a[s]---------------------------------------------------------------------------IndexError Traceback (most recent call last)&lt;ipython-input-42-c057dc68e5fe&gt; in &lt;module&gt;()----&gt; 1 a[s]IndexError: index 3 is out of bounds for axis 0 with size 3# 等价于 a[i, j]&gt;&gt;&gt; a[tuple(s)]array([[ 2, 5], [ 7, 11]]) 我们同样可以给数组索引赋值 123456789&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; aarray([0, 1, 2, 3, 4])&gt;&gt;&gt; a[[1,3,4]] = 0&gt;&gt;&gt; aarray([0, 0, 2, 0, 0]) 但是当列表包含相同的索引时，这个位置会被赋值多次，最终只保留最后一次的值 123456&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; a[[0, 0, 2]] = [1,2,3]&gt;&gt;&gt; aarray([2, 1, 3, 3, 4]) 上面看起来很合理，但是当使用+=符号的时候，结果和我们想的可能不太一样 123456&gt;&gt;&gt; a = np.arange(5)&gt;&gt;&gt; a[[0, 0, 2]] += 1&gt;&gt;&gt; aarray([1, 1, 3, 3, 4]) 尽管索引中出现了两次0，但是第0个元素它只加了1次。 布尔数组索引 当使用数字数组索引时，我们提供了哪些元素要被索引的信息。但是当使用布尔数组时，我们是明确哪些元素需要，哪些元素不需要。 12345678910111213141516&gt;&gt;&gt; a = np.arange(12).reshape((3,4))&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])&gt;&gt;&gt; b = a &gt; 4&gt;&gt;&gt; barray([[False, False, False, False], [False, True, True, True], [ True, True, True, True]], dtype=bool)&gt;&gt;&gt; a[b]array([ 5, 6, 7, 8, 9, 10, 11]) 这个特性非常适合用来赋值 1234567# 所有大于4的元素都赋值为0&gt;&gt;&gt; a[b] = 0&gt;&gt;&gt; aarray([[0, 1, 2, 3], [4, 0, 0, 0], [0, 0, 0, 0]]) 一个使用布尔数组索引的例子就是曼德博集合(Mandelbrot set) 12345678910111213141516171819import numpy as npimport matplotlib.pyplot as pltdef mandelbrot( h,w, maxit=20 ): """Returns an image of the Mandelbrot fractal of size (h,w).""" y,x = np.ogrid[ -1.4:1.4:h*1j, -2:0.8:w*1j ] c = x+y*1j z = c divtime = maxit + np.zeros(z.shape, dtype=int) for i in range(maxit): z = z**2 + c diverge = z*np.conj(z) &gt; 2**2 # who is diverging div_now = diverge &amp; (divtime==maxit) # who is diverging now divtime[div_now] = i # note when z[diverge] = 2 # avoid diverging too much return divtimeplt.imshow(mandelbrot(400,400))plt.show() 另一个布尔数组的场景跟数字数组索引类似，对每个维度，我们提供一个1维的数组来选择我们需要的切片 123456789101112131415161718192021222324252627282930&gt;&gt;&gt; a = np.arange(12).reshape(3, 4)&gt;&gt;&gt; b1 = np.array([False, True, True])&gt;&gt;&gt; b2 = np.array([True, False, True, False])&gt;&gt;&gt; aarray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])# 选择行&gt;&gt;&gt; a[b1, :]array([[ 4, 5, 6, 7], [ 8, 9, 10, 11]])# 同上&gt;&gt;&gt; a[b1]array([[ 4, 5, 6, 7], [ 8, 9, 10, 11]])# 选择列&gt;&gt;&gt; a[:, b2]array([[ 0, 2], [ 4, 6], [ 8, 10]])# a weird thing to do&gt;&gt;&gt; a[b1, b2]array([ 4, 10]) ix_()函数 （作用待定） 字符串索引 Numpy提供了创建结构化的数组的能力，可以通过列名来操作数据 123456789101112131415# dtype分别指定每一个的名字和数据类型&gt;&gt;&gt; x = np.array([(1, 2., 'Hello'), (2, 3., 'World')], dtype=[('foo', 'i4'), ('bar', 'f4'), ('baz', 'S10')])&gt;&gt;&gt; x[1](2, 3., b'World')&gt;&gt;&gt; x['foo']array([1, 2], dtype=int32)&gt;&gt;&gt; x['bar']array([ 2., 3.], dtype=float32)&gt;&gt;&gt; x['baz']array([b'Hello', b'World'], dtype='|S10') 更多 Data types Array creation I/O with NumPy Indexing Broadcasting Byte-swapping Structured arrays Subclassing ndarray 扩展阅读 100 numpy exercises 试验性的Numpy教程 From Python to Numpy]]></content>
      <categories>
        <category>python数据分析</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
</search>
